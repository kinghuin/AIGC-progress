| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-06-04 | [axodox/axodox-machinelearning](https://github.com/axodox/axodox-machinelearning) | ![axodox/axodox-machinelearning Stars](https://img.shields.io/github/stars/axodox/axodox-machinelearning.svg?label=&style=flat-square) | ğŸ–¼ï¸ğŸ’¡ | This repository contains a C++ ONNX implementation of StableDiffusion. |
| 2023-04-29 | [mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm) | ![mlc-ai/mlc-llm Stars](https://img.shields.io/github/stars/mlc-ai/mlc-llm.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | Enable everyone to develop, optimize and deploy AI models natively on everyone's devices. |
| 2023-04-19 | [bentoml/OpenLLM](https://github.com/bentoml/OpenLLM) | ![bentoml/OpenLLM Stars](https://img.shields.io/github/stars/bentoml/OpenLLM.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | Operating LLMs in production |
| 2023-04-13 | [mlc-ai/web-llm](https://github.com/mlc-ai/web-llm) | ![mlc-ai/web-llm Stars](https://img.shields.io/github/stars/mlc-ai/web-llm.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | Bringing large-language models and chat to web browsers. Everything runs inside the browser with no server support. |
| 2023-04-02 | [Jittor/JittorLLMs](https://github.com/Jittor/JittorLLMs) | ![Jittor/JittorLLMs Stars](https://img.shields.io/github/stars/Jittor/JittorLLMs.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | è®¡å›¾å¤§æ¨¡å‹æ¨ç†åº“ï¼Œå…·æœ‰é«˜æ€§èƒ½ã€é…ç½®è¦æ±‚ä½ã€ä¸­æ–‡æ”¯æŒå¥½ã€å¯ç§»æ¤ç­‰ç‰¹ç‚¹ |
| 2023-03-30 | [saharNooby/rwkv.cpp](https://github.com/saharNooby/rwkv.cpp) | ![saharNooby/rwkv.cpp Stars](https://img.shields.io/github/stars/saharNooby/rwkv.cpp.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | INT4/INT5/INT8 and FP16 inference on CPU for RWKV language model |
| 2023-03-23 | [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python) | ![abetlen/llama-cpp-python Stars](https://img.shields.io/github/stars/abetlen/llama-cpp-python.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | Python bindings for llama.cpp |
| 2023-03-21 | [CVI-SZU/Linly](https://github.com/CVI-SZU/Linly) | ![CVI-SZU/Linly Stars](https://img.shields.io/github/stars/CVI-SZU/Linly.svg?label=&style=flat-square) | ğŸ” â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£âœ‚ï¸ğŸ’¡âœ…ğŸ€„ | Chinese-LLaMA ã€Chinese-Falcon åŸºç¡€æ¨¡å‹ï¼›ChatFlowä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼›ä¸­æ–‡OpenLLaMAæ¨¡å‹ï¼›NLPé¢„è®­ç»ƒ/æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† |
| 2023-03-19 | [gotzmann/llama.go](https://github.com/gotzmann/llama.go) | ![gotzmann/llama.go Stars](https://img.shields.io/github/stars/gotzmann/llama.go.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | llama.go is like llama.cpp in pure Golang! |
| 2023-03-18 | [go-skynet/LocalAI](https://github.com/go-skynet/LocalAI) | ![go-skynet/LocalAI Stars](https://img.shields.io/github/stars/go-skynet/LocalAI.svg?label=&style=flat-square) | ğŸ” ğŸ–¼ï¸ğŸ’¡ | :robot: Self-hosted, community-driven, local OpenAI-compatible API. Drop-in replacement for OpenAI running LLMs on consumer-grade hardware. Free Open Source OpenAI alternative. No GPU required. LocalAI is an API to run ggml compatible models: llama, gpt4all, rwkv, whisper, vicuna, koala, gpt4all-j, cerebras, falcon, dolly, starcoder, and many other |
| 2023-03-17 | [wangzhaode/ChatGLM-MNN](https://github.com/wangzhaode/ChatGLM-MNN) | ![wangzhaode/ChatGLM-MNN Stars](https://img.shields.io/github/stars/wangzhaode/ChatGLM-MNN.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | Pure C++, Easy Deploy ChatGLM-6B. |
| 2023-03-15 | [kuleshov/minillm](https://github.com/kuleshov/minillm) | ![kuleshov/minillm Stars](https://img.shields.io/github/stars/kuleshov/minillm.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | MiniLLM is a minimal system for running modern LLMs on consumer-grade GPUs |
| 2023-03-15 | [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | ![ymcui/Chinese-LLaMA-Alpaca Stars](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca.svg?label=&style=flat-square) | ğŸ” 1ï¸âƒ£2ï¸âƒ£âœ‚ï¸ğŸ’¡ğŸ€„ | ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°CPU/GPUè®­ç»ƒéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs) |
| 2023-03-13 | [rustformers/llm](https://github.com/rustformers/llm) | ![rustformers/llm Stars](https://img.shields.io/github/stars/rustformers/llm.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | An ecosystem of Rust libraries for working with large language models |
| 2023-03-12 | [NouamaneTazi/bloomz.cpp](https://github.com/NouamaneTazi/bloomz.cpp) | ![NouamaneTazi/bloomz.cpp Stars](https://img.shields.io/github/stars/NouamaneTazi/bloomz.cpp.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ğŸ’¡ | C++ implementation for BLOOM |
| 2023-03-10 | [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) | ![ggerganov/llama.cpp Stars](https://img.shields.io/github/stars/ggerganov/llama.cpp.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ğŸ’¡ | Port of Facebook's LLaMA model in C/C++ |
| 2023-03-08 | [jankais3r/LLaMA_MPS](https://github.com/jankais3r/LLaMA_MPS) | ![jankais3r/LLaMA_MPS Stars](https://img.shields.io/github/stars/jankais3r/LLaMA_MPS.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | Run LLaMA (and Stanford-Alpaca) inference on Apple Silicon GPUs. |
| 2023-03-06 | [mlc-ai/web-stable-diffusion](https://github.com/mlc-ai/web-stable-diffusion) | ![mlc-ai/web-stable-diffusion Stars](https://img.shields.io/github/stars/mlc-ai/web-stable-diffusion.svg?label=&style=flat-square) | ğŸ–¼ï¸ğŸ’¡ | Bringing stable diffusion models to web browsers. Everything runs inside the browser with no server support.  |
| 2023-02-15 | [FMInference/FlexGen](https://github.com/FMInference/FlexGen) | ![FMInference/FlexGen Stars](https://img.shields.io/github/stars/FMInference/FlexGen.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | Running large language models on a single GPU for throughput-oriented scenarios. |
| 2023-02-09 | [vllm-project/vllm](https://github.com/vllm-project/vllm) | ![vllm-project/vllm Stars](https://img.shields.io/github/stars/vllm-project/vllm.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | A high-throughput and memory-efficient inference and serving engine for LLMs |
| 2022-11-16 | [apple/ml-stable-diffusion](https://github.com/apple/ml-stable-diffusion) | ![apple/ml-stable-diffusion Stars](https://img.shields.io/github/stars/apple/ml-stable-diffusion.svg?label=&style=flat-square) | ğŸ–¼ï¸ğŸ’¡ | Stable Diffusion with Core ML on Apple Silicon |
| 2022-10-08 | [huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference) | ![huggingface/text-generation-inference Stars](https://img.shields.io/github/stars/huggingface/text-generation-inference.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ğŸ’¡ | Large Language Model Text Generation Inference |
| 2022-09-18 | [ggerganov/ggml](https://github.com/ggerganov/ggml) | ![ggerganov/ggml Stars](https://img.shields.io/github/stars/ggerganov/ggml.svg?label=&style=flat-square) | ğŸ” ğŸ’¡ | Tensor library for machine learning |
| 2022-07-15 | [facebookincubator/AITemplate](https://github.com/facebookincubator/AITemplate) | ![facebookincubator/AITemplate Stars](https://img.shields.io/github/stars/facebookincubator/AITemplate.svg?label=&style=flat-square) | ğŸ–¼ï¸ğŸ’¡ | AITemplate is a Python framework which renders neural network into high performance CUDA/HIP C++ code. Specialized for FP16 TensorCore (NVIDIA GPU) and MatrixCore (AMD GPU) inference. |
