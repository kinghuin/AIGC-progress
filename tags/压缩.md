| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-07-08 | [taprosoft/llm_finetuning](https://github.com/taprosoft/llm_finetuning) | ![taprosoft/llm_finetuning Stars](https://img.shields.io/github/stars/taprosoft/llm_finetuning.svg?label=&style=flat-square) | ğŸ” 2ï¸âƒ£âœ‚ï¸ | Convenient wrapper for fine-tuning and inference of Large Language Models (LLMs) with several quantization techniques (GTPQ, bitsandbytes) |
| 2023-07-04 | [SpongebBob/Finetune-ChatGLM2-6B](https://github.com/SpongebBob/Finetune-ChatGLM2-6B) | ![SpongebBob/Finetune-ChatGLM2-6B Stars](https://img.shields.io/github/stars/SpongebBob/Finetune-ChatGLM2-6B.svg?label=&style=flat-square) | ğŸ” â›½2ï¸âƒ£âœ‚ï¸ | ChatGLM2-6B å…¨å‚æ•°å¾®è°ƒï¼Œæ”¯æŒå¤šè½®å¯¹è¯çš„é«˜æ•ˆå¾®è°ƒã€‚ |
| 2023-06-24 | [THUDM/ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) | ![THUDM/ChatGLM2-6B Stars](https://img.shields.io/github/stars/THUDM/ChatGLM2-6B.svg?label=&style=flat-square) | ğŸ” ğŸšŒâœ‚ï¸ğŸ€„ | ChatGLM2-6B: An Open Bilingual Chat LLM \| å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ |
| 2023-06-12 | [SqueezeAILab/SqueezeLLM](https://github.com/SqueezeAILab/SqueezeLLM) | ![SqueezeAILab/SqueezeLLM Stars](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ | SqueezeLLM: Dense-and-Sparse Quantization |
| 2023-06-06 | [locuslab/wanda](https://github.com/locuslab/wanda) | ![locuslab/wanda Stars](https://img.shields.io/github/stars/locuslab/wanda.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ | A simple and effective LLM pruning approach. |
| 2023-06-05 | [RUC-GSAI/YuLan-Chat](https://github.com/RUC-GSAI/YuLan-Chat) | ![RUC-GSAI/YuLan-Chat Stars](https://img.shields.io/github/stars/RUC-GSAI/YuLan-Chat.svg?label=&style=flat-square) | ğŸ” ğŸšŒâœ‚ï¸ | YuLan-Chat: An Open-Source Bilingual Chatbot |
| 2023-06-05 | [Vahe1994/SpQR](https://github.com/Vahe1994/SpQR) | ![Vahe1994/SpQR Stars](https://img.shields.io/github/stars/Vahe1994/SpQR.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ | This repository contains quantization algorithm and the model evaluation code for SpQR method for LLM compression |
| 2023-06-01 | [mit-han-lab/llm-awq](https://github.com/mit-han-lab/llm-awq) | ![mit-han-lab/llm-awq Stars](https://img.shields.io/github/stars/mit-han-lab/llm-awq.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ | AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration |
| 2023-05-28 | [rmihaylov/falcontune](https://github.com/rmihaylov/falcontune) | ![rmihaylov/falcontune Stars](https://img.shields.io/github/stars/rmihaylov/falcontune.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ | Tune any FALCON in 4-bit |
| 2023-05-11 | [artidoro/qlora](https://github.com/artidoro/qlora) | ![artidoro/qlora Stars](https://img.shields.io/github/stars/artidoro/qlora.svg?label=&style=flat-square) | ğŸ” 2ï¸âƒ£âœ‚ï¸ | QLoRA: Efficient Finetuning of Quantized LLMs |
| 2023-05-04 | [turboderp/exllama](https://github.com/turboderp/exllama) | ![turboderp/exllama Stars](https://img.shields.io/github/stars/turboderp/exllama.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ | A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights. |
| 2023-04-23 | [THUDM/VisualGLM-6B](https://github.com/THUDM/VisualGLM-6B) | ![THUDM/VisualGLM-6B Stars](https://img.shields.io/github/stars/THUDM/VisualGLM-6B.svg?label=&style=flat-square) | ğŸ” ğŸ–¼ï¸â›½ğŸšŒ2ï¸âƒ£âœ‚ï¸ğŸ€„ | Chinese and English multimodal conversational language model \| å¤šæ¨¡æ€ä¸­è‹±åŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ |
| 2023-04-19 | [RiseInRose/MiniGPT-4-ZH](https://github.com/RiseInRose/MiniGPT-4-ZH) | ![RiseInRose/MiniGPT-4-ZH Stars](https://img.shields.io/github/stars/RiseInRose/MiniGPT-4-ZH.svg?label=&style=flat-square) | ğŸ” ğŸ–¼ï¸ğŸšŒ2ï¸âƒ£âœ‚ï¸ | MiniGPT-4 ä¸­æ–‡éƒ¨ç½²ç¿»è¯‘ å®Œå–„éƒ¨ç½²ç»†èŠ‚ |
| 2023-04-15 | [OpenLMLab/MOSS](https://github.com/OpenLMLab/MOSS) | ![OpenLMLab/MOSS Stars](https://img.shields.io/github/stars/OpenLMLab/MOSS.svg?label=&style=flat-square) | ğŸ” â›½ğŸšŒâœ‚ï¸âœ…ğŸ€„ | An open-source tool-augmented conversational language model from Fudan University |
| 2023-04-13 | [PanQiWei/AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) | ![PanQiWei/AutoGPTQ Stars](https://img.shields.io/github/stars/PanQiWei/AutoGPTQ.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ | An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm. |
| 2023-04-02 | [yangjianxin1/Firefly](https://github.com/yangjianxin1/Firefly) | ![yangjianxin1/Firefly Stars](https://img.shields.io/github/stars/yangjianxin1/Firefly.svg?label=&style=flat-square) | ğŸ” â›½ğŸšŒ2ï¸âƒ£âœ‚ï¸ğŸ€„ | Firefly(æµè¤): ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹(å…¨é‡å¾®è°ƒ+QLoRA) |
| 2023-04-01 | [FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) | ![FreedomIntelligence/LLMZoo Stars](https://img.shields.io/github/stars/FreedomIntelligence/LLMZoo.svg?label=&style=flat-square) | ğŸ” â›½ğŸšŒ2ï¸âƒ£âœ‚ï¸ğŸ“ | âš¡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.âš¡ |
| 2023-03-23 | [Facico/Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna) | ![Facico/Chinese-Vicuna Stars](https://img.shields.io/github/stars/Facico/Chinese-Vicuna.svg?label=&style=flat-square) | ğŸ” ğŸšŒ2ï¸âƒ£âœ‚ï¸ğŸ€„ | Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model â€”â€” ä¸€ä¸ªä¸­æ–‡ä½èµ„æºçš„llama+loraæ–¹æ¡ˆï¼Œç»“æ„å‚è€ƒalpaca |
| 2023-03-22 | [Lightning-AI/lit-llama](https://github.com/Lightning-AI/lit-llama) | ![Lightning-AI/lit-llama Stars](https://img.shields.io/github/stars/Lightning-AI/lit-llama.svg?label=&style=flat-square) | ğŸ” ğŸšŒ1ï¸âƒ£2ï¸âƒ£âœ‚ï¸âœ… | Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed. |
| 2023-03-21 | [CVI-SZU/Linly](https://github.com/CVI-SZU/Linly) | ![CVI-SZU/Linly Stars](https://img.shields.io/github/stars/CVI-SZU/Linly.svg?label=&style=flat-square) | ğŸ” â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£âœ‚ï¸ğŸ’¡âœ…ğŸ€„ | Chinese-LLaMA ã€Chinese-Falcon åŸºç¡€æ¨¡å‹ï¼›ChatFlowä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼›ä¸­æ–‡OpenLLaMAæ¨¡å‹ï¼›NLPé¢„è®­ç»ƒ/æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† |
| 2023-03-17 | [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE) | ![LianjiaTech/BELLE Stars](https://img.shields.io/github/stars/LianjiaTech/BELLE.svg?label=&style=flat-square) | ğŸ” â›½ğŸšŒ2ï¸âƒ£âœ‚ï¸ğŸ€„ | BELLE: Be Everyone's Large Language model Engineï¼ˆå¼€æºä¸­æ–‡å¯¹è¯å¤§æ¨¡å‹ï¼‰ |
| 2023-03-15 | [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | ![ymcui/Chinese-LLaMA-Alpaca Stars](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca.svg?label=&style=flat-square) | ğŸ” 1ï¸âƒ£2ï¸âƒ£âœ‚ï¸ğŸ’¡ğŸ€„ | ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°CPU/GPUè®­ç»ƒéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs) |
| 2023-03-13 | [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) | ![THUDM/ChatGLM-6B Stars](https://img.shields.io/github/stars/THUDM/ChatGLM-6B.svg?label=&style=flat-square) | ğŸ” ğŸšŒ2ï¸âƒ£âœ‚ï¸ğŸ€„ | ChatGLM-6B: An Open Bilingual Dialogue Language Model \| å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ |
| 2023-03-12 | [NouamaneTazi/bloomz.cpp](https://github.com/NouamaneTazi/bloomz.cpp) | ![NouamaneTazi/bloomz.cpp Stars](https://img.shields.io/github/stars/NouamaneTazi/bloomz.cpp.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ğŸ’¡ | C++ implementation for BLOOM |
| 2023-03-10 | [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) | ![ggerganov/llama.cpp Stars](https://img.shields.io/github/stars/ggerganov/llama.cpp.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ğŸ’¡ | Port of Facebook's LLaMA model in C/C++ |
| 2023-03-06 | [qwopqwop200/GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa) | ![qwopqwop200/GPTQ-for-LLaMa Stars](https://img.shields.io/github/stars/qwopqwop200/GPTQ-for-LLaMa.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ | 4 bits quantization of LLaMA using GPTQ |
| 2023-02-28 | [juncongmoo/pyllama](https://github.com/juncongmoo/pyllama) | ![juncongmoo/pyllama Stars](https://img.shields.io/github/stars/juncongmoo/pyllama.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ | LLaMA: Open and Efficient Foundation Language Models |
| 2023-02-26 | [openai/consistency_models](https://github.com/openai/consistency_models) | ![openai/consistency_models Stars](https://img.shields.io/github/stars/openai/consistency_models.svg?label=&style=flat-square) | ğŸ–¼ï¸ğŸšŒ2ï¸âƒ£âœ‚ï¸ | Official repo for consistency models. |
| 2023-01-27 | [lucidrains/musiclm-pytorch](https://github.com/lucidrains/musiclm-pytorch) | ![lucidrains/musiclm-pytorch Stars](https://img.shields.io/github/stars/lucidrains/musiclm-pytorch.svg?label=&style=flat-square) | ğŸµğŸšŒ2ï¸âƒ£âœ‚ï¸ | Implementation of MusicLM, Google's new SOTA model for music generation using attention networks, in Pytorch |
| 2022-10-08 | [huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference) | ![huggingface/text-generation-inference Stars](https://img.shields.io/github/stars/huggingface/text-generation-inference.svg?label=&style=flat-square) | ğŸ” âœ‚ï¸ğŸ’¡ | Large Language Model Text Generation Inference |
| 2022-09-25 | [ggerganov/whisper.cpp](https://github.com/ggerganov/whisper.cpp) | ![ggerganov/whisper.cpp Stars](https://img.shields.io/github/stars/ggerganov/whisper.cpp.svg?label=&style=flat-square) | ğŸµğŸšŒâœ‚ï¸ | Port of OpenAI's Whisper model in C/C++ |
