| Date | Repository | Stars | Description | tags |
|------------|---------|-------|-------------|-------------|
| 2023-05-20 | [YuanGongND/ltu](https://github.com/YuanGongND/ltu) | ![YuanGongND/ltu Stars](https://img.shields.io/github/stars/YuanGongND/ltu.svg?label=&style=flat-square) | Github Repo for Paper "Listen, Think, and Understand". | `文本`,`语音`,`训练(TD)` |
| 2023-05-19 | [OFA-Sys/ONE-PEACE](https://github.com/OFA-Sys/ONE-PEACE) | ![OFA-Sys/ONE-PEACE Stars](https://img.shields.io/github/stars/OFA-Sys/ONE-PEACE.svg?label=&style=flat-square) | A general representation modal across vision, audio, language modalities. | `图像`,`通用模型`,`预训练`,`训练` |
| 2023-05-18 | [OpenGVLab/VisionLLM](https://github.com/OpenGVLab/VisionLLM) | ![OpenGVLab/VisionLLM Stars](https://img.shields.io/github/stars/OpenGVLab/VisionLLM.svg?label=&style=flat-square) | VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks | `图像`,`通用模型(TD)`,`训练(TD)` |
| 2023-05-18 | [ysymyth/tree-of-thought-llm](https://github.com/ysymyth/tree-of-thought-llm) | ![ysymyth/tree-of-thought-llm Stars](https://img.shields.io/github/stars/ysymyth/tree-of-thought-llm.svg?label=&style=flat-square) |  | `文本`,`训练(TD)` |
| 2023-05-17 | [mit-han-lab/fastcomposer](https://github.com/mit-han-lab/fastcomposer) | ![mit-han-lab/fastcomposer Stars](https://img.shields.io/github/stars/mit-han-lab/fastcomposer.svg?label=&style=flat-square) |  | `图像`,`数据(TD)`,`通用模型`,`训练(TD)` |
| 2023-05-15 | [PKU-Alignment/safe-rlhf](https://github.com/PKU-Alignment/safe-rlhf) | ![PKU-Alignment/safe-rlhf Stars](https://img.shields.io/github/stars/PKU-Alignment/safe-rlhf.svg?label=&style=flat-square) | Safe-RLHF: Constrained Value Alignment via Safe Reinforcement Learning from Human Feedback | `文本`,`数据`,`通用模型`,`训练`,`强化` |
| 2023-05-11 | [kuleshov-group/llmtune](https://github.com/kuleshov-group/llmtune) | ![kuleshov-group/llmtune Stars](https://img.shields.io/github/stars/kuleshov-group/llmtune.svg?label=&style=flat-square) | 4-Bit Finetuning of Large Language Models on One Consumer GPU | `文本`,`训练` |
| 2023-05-08 | [conceptofmind/PaLM](https://github.com/conceptofmind/PaLM) | ![conceptofmind/PaLM Stars](https://img.shields.io/github/stars/conceptofmind/PaLM.svg?label=&style=flat-square) | An open-source implementation of Google's PaLM models | `文本`,`通用模型`,`训练` |
| 2023-05-04 | [IBM/Dromedary](https://github.com/IBM/Dromedary) | ![IBM/Dromedary Stars](https://img.shields.io/github/stars/IBM/Dromedary.svg?label=&style=flat-square) | Dromedary: towards helpful, ethical and reliable LLMs. | `文本`,`通用模型`,`训练` |
| 2023-04-29 | [mosaicml/llm-foundry](https://github.com/mosaicml/llm-foundry) | ![mosaicml/llm-foundry Stars](https://img.shields.io/github/stars/mosaicml/llm-foundry.svg?label=&style=flat-square) | LLM training code for MosaicML foundation models | `文本`,`通用模型`,`训练` |
| 2023-04-26 | [mosaicml/diffusion](https://github.com/mosaicml/diffusion) | ![mosaicml/diffusion Stars](https://img.shields.io/github/stars/mosaicml/diffusion.svg?label=&style=flat-square) | This repo contains code used to train your own Stable Diffusion model on your own data. | `图像`,`训练` |
| 2023-04-25 | [X-PLUG/mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl) | ![X-PLUG/mPLUG-Owl Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?label=&style=flat-square) | mPLUG-Owl🦉: Modularization Empowers Large Language Models with Multimodality | `文本`,`图像`,`数据`,`通用模型`,`训练` |
| 2023-04-24 | [bigcode-project/starcoder](https://github.com/bigcode-project/starcoder) | ![bigcode-project/starcoder Stars](https://img.shields.io/github/stars/bigcode-project/starcoder.svg?label=&style=flat-square) | Home of StarCoder: fine-tuning & inference! | `文本`,`数据`,`定制模型`,`训练` |
| 2023-04-24 | [minosvasilias/godot-dodo](https://github.com/minosvasilias/godot-dodo) | ![minosvasilias/godot-dodo Stars](https://img.shields.io/github/stars/minosvasilias/godot-dodo.svg?label=&style=flat-square) | Finetuning large language models for GDScript generation. | `文本`,`定制模型`,`训练` |
| 2023-04-23 | [THUDM/VisualGLM-6B](https://github.com/THUDM/VisualGLM-6B) | ![THUDM/VisualGLM-6B Stars](https://img.shields.io/github/stars/THUDM/VisualGLM-6B.svg?label=&style=flat-square) | Chinese and English multimodal conversational language model \| 多模态中英双语对话语言模型 | `文本`,`图像`,`数据`,`通用模型`,`训练`,`压缩`,`中文` |
| 2023-04-23 | [nlpxucan/WizardLM](https://github.com/nlpxucan/WizardLM) | ![nlpxucan/WizardLM Stars](https://img.shields.io/github/stars/nlpxucan/WizardLM.svg?label=&style=flat-square) | WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions | `文本`,`数据`,`通用模型`,`训练` |
| 2023-04-23 | [StevenGrove/GPT4Tools](https://github.com/StevenGrove/GPT4Tools) | ![StevenGrove/GPT4Tools Stars](https://img.shields.io/github/stars/StevenGrove/GPT4Tools.svg?label=&style=flat-square) | GPT4Tools is an intelligent system that can automatically decide, control, and utilize different visual foundation models, allowing the user to interact with images during a conversation. | `文本`,`图像`,`数据`,`通用模型`,`训练` |
| 2023-04-20 | [pengxiao-song/LaWGPT](https://github.com/pengxiao-song/LaWGPT) | ![pengxiao-song/LaWGPT Stars](https://img.shields.io/github/stars/pengxiao-song/LaWGPT.svg?label=&style=flat-square) |  🎉 Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. 基于中文法律知识的大语言模型 | `文本`,`数据`,`定制模型`,`训练`,`中文` |
| 2023-04-19 | [RiseInRose/MiniGPT-4-ZH](https://github.com/RiseInRose/MiniGPT-4-ZH) | ![RiseInRose/MiniGPT-4-ZH Stars](https://img.shields.io/github/stars/RiseInRose/MiniGPT-4-ZH.svg?label=&style=flat-square) | MiniGPT-4 中文部署翻译 完善部署细节 | `文本`,`图像`,`通用模型`,`训练`,`压缩` |
| 2023-04-18 | [h2oai/h2o-llmstudio](https://github.com/h2oai/h2o-llmstudio) | ![h2oai/h2o-llmstudio Stars](https://img.shields.io/github/stars/h2oai/h2o-llmstudio.svg?label=&style=flat-square) | H2O LLM Studio - a framework and no-code GUI for fine-tuning LLMs | `文本`,`数据`,`通用模型`,`训练`,`可商用` |
| 2023-04-16 | [Vision-CAIR/MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) | ![Vision-CAIR/MiniGPT-4 Stars](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?label=&style=flat-square) | MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models | `文本`,`图像`,`通用模型`,`预训练`,`训练` |
| 2023-04-10 | [declare-lab/tango](https://github.com/declare-lab/tango) | ![declare-lab/tango Stars](https://img.shields.io/github/stars/declare-lab/tango.svg?label=&style=flat-square) | Codes and Model of the paper "Text-to-Audio Generation using Instruction Tuned LLM and Latent Diffusion Model" | `文本`,`语音`,`通用模型`,`训练` |
| 2023-04-08 | [hiyouga/ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning) | ![hiyouga/ChatGLM-Efficient-Tuning Stars](https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning.svg?label=&style=flat-square) | Fine-tuning ChatGLM-6B with PEFT \| 基于 PEFT 的高效 ChatGLM 微调 | `文本`,`数据`,`训练`,`强化`,`中文` |
| 2023-04-06 | [Instruction-Tuning-with-GPT-4/GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | ![Instruction-Tuning-with-GPT-4/GPT-4-LLM Stars](https://img.shields.io/github/stars/Instruction-Tuning-with-GPT-4/GPT-4-LLM.svg?label=&style=flat-square) | Instruction Tuning with GPT-4 | `文本`,`数据`,`训练` |
| 2023-04-06 | [liucongg/ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning) | ![liucongg/ChatGLM-Finetuning Stars](https://img.shields.io/github/stars/liucongg/ChatGLM-Finetuning.svg?label=&style=flat-square) | 基于ChatGLM-6B模型，进行下游具体任务微调，涉及Freeze、Lora、P-tuning等 | `文本`,`训练`,`中文` |
| 2023-04-01 | [project-baize/baize-chatbot](https://github.com/project-baize/baize-chatbot) | ![project-baize/baize-chatbot Stars](https://img.shields.io/github/stars/project-baize/baize-chatbot.svg?label=&style=flat-square) | Let ChatGPT teach your own chatbot in hours with a single GPU! | `文本`,`数据`,`通用模型`,`训练` |
| 2023-04-01 | [FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) | ![FreedomIntelligence/LLMZoo Stars](https://img.shields.io/github/stars/FreedomIntelligence/LLMZoo.svg?label=&style=flat-square) | ⚡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.⚡ | `文本`,`数据`,`通用模型`,`训练`,`压缩`,`文档` |
| 2023-03-31 | [SCIR-HI/Huatuo-Llama-Med-Chinese](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) | ![SCIR-HI/Huatuo-Llama-Med-Chinese Stars](https://img.shields.io/github/stars/SCIR-HI/Huatuo-Llama-Med-Chinese.svg?label=&style=flat-square) | Repo for BenTsao [original name: HuaTuo (华驼)], Llama-7B tuned with Chinese medical knowledge. 本草（原名：华驼）模型仓库，基于中文医学知识的LLaMA模型指令微调 | `文本`,`数据`,`定制模型`,`训练`,`中文` |
| 2023-03-29 | [AGI-Edgerunners/LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters) | ![AGI-Edgerunners/LLM-Adapters Stars](https://img.shields.io/github/stars/AGI-Edgerunners/LLM-Adapters.svg?label=&style=flat-square) | LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models | `文本`,`数据`,`通用模型`,`训练` |
| 2023-03-28 | [WangRongsheng/ChatGenTitle](https://github.com/WangRongsheng/ChatGenTitle) | ![WangRongsheng/ChatGenTitle Stars](https://img.shields.io/github/stars/WangRongsheng/ChatGenTitle.svg?label=&style=flat-square) | 🌟 ChatGenTitle：使用百万arXiv论文信息在LLaMA模型上进行微调的论文题目生成模型 | `文本`,`定制模型`,`训练` |
| 2023-03-27 | [OptimalScale/LMFlow](https://github.com/OptimalScale/LMFlow) | ![OptimalScale/LMFlow Stars](https://img.shields.io/github/stars/OptimalScale/LMFlow.svg?label=&style=flat-square) | An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Model for All. | `文本`,`数据`,`通用模型`,`训练`,`强化` |
| 2023-03-25 | [h2oai/h2ogpt](https://github.com/h2oai/h2ogpt) | ![h2oai/h2ogpt Stars](https://img.shields.io/github/stars/h2oai/h2ogpt.svg?label=&style=flat-square) | Come join the movement to make the world's best open source GPT led by H2O.ai - 100% private chat and document search, no data leaks, Apache 2.0 | `文本`,`通用模型`,`训练`,`可商用` |
| 2023-03-24 | [PhoebusSi/Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT) | ![PhoebusSi/Alpaca-CoT Stars](https://img.shields.io/github/stars/PhoebusSi/Alpaca-CoT.svg?label=&style=flat-square) | We unified the interfaces of instruction-tuning data (e.g., CoT data), multiple LLMs and parameter-efficient methods (e.g., lora, p-tuning) together for easy use. Meanwhile, we created a new branch to build a Tabular LLM.（我们分别统一了丰富的IFT数据（如CoT数据，目前仍不断扩充）、多种训练效率方法（如lora，p-tuning）以及多种LLMs，三个层面上的接口，打造方便研究人员上手的LLM-IFT研究平台。同时tabular_llm分支构建了面向表格智能任务的LLM。 | `文本`,`数据`,`训练` |
| 2023-03-23 | [Facico/Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna) | ![Facico/Chinese-Vicuna Stars](https://img.shields.io/github/stars/Facico/Chinese-Vicuna.svg?label=&style=flat-square) | Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model —— 一个中文低资源的llama+lora方案，结构参考alpaca | `文本`,`通用模型`,`训练`,`压缩`,`中文` |
| 2023-03-22 | [Kent0n-Li/ChatDoctor](https://github.com/Kent0n-Li/ChatDoctor) | ![Kent0n-Li/ChatDoctor Stars](https://img.shields.io/github/stars/Kent0n-Li/ChatDoctor.svg?label=&style=flat-square) |  | `文本`,`数据`,`定制模型`,`训练` |
| 2023-03-22 | [Lightning-AI/lit-llama](https://github.com/Lightning-AI/lit-llama) | ![Lightning-AI/lit-llama Stars](https://img.shields.io/github/stars/Lightning-AI/lit-llama.svg?label=&style=flat-square) | Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed. | `文本`,`通用模型`,`预训练`,`训练`,`压缩`,`可商用` |
| 2023-03-21 | [CVI-SZU/Linly](https://github.com/CVI-SZU/Linly) | ![CVI-SZU/Linly Stars](https://img.shields.io/github/stars/CVI-SZU/Linly.svg?label=&style=flat-square) | Chinese-LLaMA基础模型；ChatFlow中文对话模型；中文OpenLLaMA模型；NLP预训练/指令微调数据集 | `文本`,`数据`,`通用模型`,`预训练`,`训练`,`强化(TD)`,`压缩`,`推理`,`可商用`,`中文` |
| 2023-03-19 | [lm-sys/FastChat](https://github.com/lm-sys/FastChat) | ![lm-sys/FastChat Stars](https://img.shields.io/github/stars/lm-sys/FastChat.svg?label=&style=flat-square) | An open platform for training, serving, and evaluating large languages. Release repo for Vicuna and FastChat-T5. | `文本`,`通用模型`,`训练`,`可商用` |
| 2023-03-19 | [stochasticai/xturing](https://github.com/stochasticai/xturing) | ![stochasticai/xturing Stars](https://img.shields.io/github/stars/stochasticai/xturing.svg?label=&style=flat-square) | Easily build, customize and control your own LLMs | `文本`,`训练` |
| 2023-03-17 | [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE) | ![LianjiaTech/BELLE Stars](https://img.shields.io/github/stars/LianjiaTech/BELLE.svg?label=&style=flat-square) | BELLE: Be Everyone's Large Language model Engine（开源中文对话大模型） | `文本`,`数据`,`通用模型`,`训练`,`压缩`,`中文` |
| 2023-03-17 | [hikariming/alpaca_chinese_dataset](https://github.com/hikariming/alpaca_chinese_dataset) | ![hikariming/alpaca_chinese_dataset Stars](https://img.shields.io/github/stars/hikariming/alpaca_chinese_dataset.svg?label=&style=flat-square) | 人工精调的中文对话数据集和一段chatglm的微调代码 | `文本`,`数据`,`训练`,`中文` |
| 2023-03-16 | [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) | ![mymusise/ChatGLM-Tuning Stars](https://img.shields.io/github/stars/mymusise/ChatGLM-Tuning.svg?label=&style=flat-square) | 一种平价的chatgpt实现方案,  基于ChatGLM-6B + LoRA | `文本`,`训练`,`中文` |
| 2023-03-15 | [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | ![ymcui/Chinese-LLaMA-Alpaca Stars](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca.svg?label=&style=flat-square) | 中文LLaMA&Alpaca大语言模型+本地CPU/GPU部署 (Chinese LLaMA & Alpaca LLMs) | `文本`,`预训练`,`训练`,`压缩`,`推理`,`中文` |
| 2023-03-14 | [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora) | ![tloen/alpaca-lora Stars](https://img.shields.io/github/stars/tloen/alpaca-lora.svg?label=&style=flat-square) | Instruct-tune LLaMA on consumer hardware | `文本`,`通用模型`,`训练` |
| 2023-03-14 | [ssbuild/chatglm_finetuning](https://github.com/ssbuild/chatglm_finetuning) | ![ssbuild/chatglm_finetuning Stars](https://img.shields.io/github/stars/ssbuild/chatglm_finetuning.svg?label=&style=flat-square) | chatglm 6b finetuning and alpaca finetuning | `文本`,`训练`,`中文` |
| 2023-03-13 | [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) | ![THUDM/ChatGLM-6B Stars](https://img.shields.io/github/stars/THUDM/ChatGLM-6B.svg?label=&style=flat-square) | ChatGLM-6B: An Open Bilingual Dialogue Language Model \| 开源双语对话语言模型 | `文本`,`通用模型`,`训练`,`压缩`,`中文` |
| 2023-03-10 | [svc-develop-team/so-vits-svc](https://github.com/svc-develop-team/so-vits-svc) | ![svc-develop-team/so-vits-svc Stars](https://img.shields.io/github/stars/svc-develop-team/so-vits-svc.svg?label=&style=flat-square) | SoftVC VITS Singing Voice Conversion | `语音`,`通用模型`,`训练` |
