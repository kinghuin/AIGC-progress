| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-07-05 | [OpenLMLab/MOSS-RLHF](https://github.com/OpenLMLab/MOSS-RLHF) | ![OpenLMLab/MOSS-RLHF Stars](https://img.shields.io/github/stars/OpenLMLab/MOSS-RLHF.svg?label=&style=flat-square) | 🔠🚌3️⃣🀄 | MOSS-RLHF |
| 2023-06-22 | [eric-mitchell/direct-preference-optimization](https://github.com/eric-mitchell/direct-preference-optimization) | ![eric-mitchell/direct-preference-optimization Stars](https://img.shields.io/github/stars/eric-mitchell/direct-preference-optimization.svg?label=&style=flat-square) | 🔠2️⃣3️⃣ | Reference implementation for DPO (Direct Preference Optimization) |
| 2023-06-02 | [shibing624/MedicalGPT](https://github.com/shibing624/MedicalGPT) | ![shibing624/MedicalGPT Stars](https://img.shields.io/github/stars/shibing624/MedicalGPT.svg?label=&style=flat-square) | 🔠⛽🚕1️⃣2️⃣3️⃣🀄 | MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. 训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。 |
| 2023-05-28 | [hiyouga/LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) | ![hiyouga/LLaMA-Efficient-Tuning Stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Efficient-Tuning.svg?label=&style=flat-square) | 🔠1️⃣2️⃣3️⃣ | Easy-to-use fine-tuning framework using PEFT (PT+SFT+RLHF with QLoRA) |
| 2023-05-15 | [PKU-Alignment/safe-rlhf](https://github.com/PKU-Alignment/safe-rlhf) | ![PKU-Alignment/safe-rlhf Stars](https://img.shields.io/github/stars/PKU-Alignment/safe-rlhf.svg?label=&style=flat-square) | 🔠⛽🚌2️⃣3️⃣ | Safe-RLHF: Constrained Value Alignment via Safe Reinforcement Learning from Human Feedback |
| 2023-04-10 | [GanjinZero/RRHF](https://github.com/GanjinZero/RRHF) | ![GanjinZero/RRHF Stars](https://img.shields.io/github/stars/GanjinZero/RRHF.svg?label=&style=flat-square) | 🔠⛽🚌3️⃣ | RRHF & Wombat |
| 2023-04-08 | [hiyouga/ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning) | ![hiyouga/ChatGLM-Efficient-Tuning Stars](https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning.svg?label=&style=flat-square) | 🔠⛽2️⃣3️⃣🀄 | Fine-tuning ChatGLM-6B with PEFT \| 基于 PEFT 的高效 ChatGLM 微调 |
| 2023-04-01 | [THUDM/ImageReward](https://github.com/THUDM/ImageReward) | ![THUDM/ImageReward Stars](https://img.shields.io/github/stars/THUDM/ImageReward.svg?label=&style=flat-square) | 🔠🖼️3️⃣🔌 | ImageReward: Learning and Evaluating Human Preferences for Text-to-image Generation |
| 2023-03-27 | [OptimalScale/LMFlow](https://github.com/OptimalScale/LMFlow) | ![OptimalScale/LMFlow Stars](https://img.shields.io/github/stars/OptimalScale/LMFlow.svg?label=&style=flat-square) | 🔠⛽🚌2️⃣3️⃣ | An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All. |
| 2023-03-21 | [CVI-SZU/Linly](https://github.com/CVI-SZU/Linly) | ![CVI-SZU/Linly Stars](https://img.shields.io/github/stars/CVI-SZU/Linly.svg?label=&style=flat-square) | 🔠⛽🚌1️⃣2️⃣3️⃣✂️💡✅🀄 | Chinese-LLaMA 、Chinese-Falcon 基础模型；ChatFlow中文对话模型；中文OpenLLaMA模型；NLP预训练/指令微调数据集 |
| 2023-02-27 | [juncongmoo/chatllama](https://github.com/juncongmoo/chatllama) | ![juncongmoo/chatllama Stars](https://img.shields.io/github/stars/juncongmoo/chatllama.svg?label=&style=flat-square) | 🔠3️⃣ | ChatLLaMA 📢 Open source implementation for LLaMA-based ChatGPT runnable in a single GPU. 15x faster training process than ChatGPT |
| 2022-12-09 | [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch) | ![lucidrains/PaLM-rlhf-pytorch Stars](https://img.shields.io/github/stars/lucidrains/PaLM-rlhf-pytorch.svg?label=&style=flat-square) | 🔠3️⃣ | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM |
| 2022-10-03 | [CarperAI/trlx](https://github.com/CarperAI/trlx) | ![CarperAI/trlx Stars](https://img.shields.io/github/stars/CarperAI/trlx.svg?label=&style=flat-square) | 🔠3️⃣ | A repo for distributed training of language models with Reinforcement Learning via Human Feedback (RLHF) |
