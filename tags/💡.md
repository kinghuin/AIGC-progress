| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-08-24 | [llama2.c-to-ncnn](https://github.com/lrw04/llama2.c-to-ncnn) | 28 | ğŸ”  ğŸ’¡  | A converter for llama2.c legacy models to ncnn models. |
| 2023-07-31 | [Jlama](https://github.com/tjake/Jlama) | 120 | ğŸ”  ğŸ’¡  | Jlama is a pure Java implementation of a LLM inference engine. |
| 2023-07-30 | [LanguageModels.jl](https://github.com/rai-llc/LanguageModels.jl) | 48 | ğŸ”  ğŸ’¡  | Load nanoGPT-style transformers in Julia. Code ported from @karpathy's llama2.c |
| 2023-07-29 | [llama2.c-for-dummies](https://github.com/RahulSChand/llama2.c-for-dummies) | 153 | ğŸ”  ğŸ’¡ ğŸ“  | Step by step explanation/tutorial of llama2.c |
| 2023-07-25 | [llama2.go](https://github.com/nikolaydubina/llama2.go) | 133 | ğŸ”  ğŸ’¡  | LLaMA-2 in native Go |
| 2023-07-23 | [whisper-burn](https://github.com/Gadersd/whisper-burn) | 118 | ğŸµ ğŸ’¡  | A Rust implementation of OpenAI's Whisper model using the burn framework |
| 2023-07-23 | [llama2.c](https://github.com/karpathy/llama2.c) | 11474 | ğŸ”  ğŸ’¡  | Inference Llama 2 in one file of pure C |
| 2023-07-22 | [lightllm](https://github.com/ModelTC/lightllm) | 632 | ğŸ”  ğŸ’¡  | LightLLM is a Python-based LLM (Large Language Model) inference and serving framework, notable for its lightweight design, easy scalability, and high-speed performance. |
| 2023-07-17 | [Llama-2-Onnx](https://github.com/microsoft/Llama-2-Onnx) | 774 | ğŸ”  ğŸ’¡  | This is an optimized version of the Llama 2 model, available from Meta under the Llama Community License Agreement found on this repository. |
| 2023-07-15 | [minigpt4.cpp](https://github.com/Maknee/minigpt4.cpp) | 452 | ğŸ”  ğŸ’¡  | Port of MiniGPT4 in C++ (4bit, 5bit, 6bit, 8bit, 16bit CPU inference with GGML) |
| 2023-07-14 | [OnnxStream](https://github.com/vitoplantamura/OnnxStream) | 721 | ğŸ–¼ï¸ ğŸ’¡  | Running Stable Diffusion on a RPI Zero 2 (or in 260MB of RAM) |
| 2023-07-12 | [SD-inference](https://github.com/yangyuke001/SD-inference) | 181 | ğŸ”  ğŸ–¼ï¸ ğŸ’¡  | Stable Diffusion inference |
| 2023-07-10 | [ai00_rwkv_server](https://github.com/cgisky1980/ai00_rwkv_server) | 153 | ğŸ”  ğŸ’¡  | A localized open-source AI server that is better than ChatGPT. |
| 2023-07-06 | [Llama-2-Open-Source-LLM-CPU-Inference](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference) | 747 | ğŸ”  ğŸ’¡ ğŸ”¨ `Python`  | Running Llama 2 and other Open-Source LLMs on CPU Inference Locally for DocumentÂ Q&A |
| 2023-07-01 | [a1gpt](https://github.com/a1k0n/a1gpt) | 131 | ğŸ”  ğŸ’¡  | throwaway GPT inference |
| 2023-06-15 | [lmdeploy](https://github.com/InternLM/lmdeploy) | 684 | ğŸ”  âœ‚ï¸ ğŸ’¡  | LMDeploy is a toolkit for compressing, deploying, and serving LLMs. |
| 2023-06-14 | [inference](https://github.com/xorbitsai/inference) | 873 | ğŸ”  ğŸ’¡  | Replace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you're empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop. |
| 2023-06-04 | [axodox-machinelearning](https://github.com/axodox/axodox-machinelearning) | 527 | ğŸ–¼ï¸ ğŸ’¡  | This repository contains a C++ ONNX implementation of StableDiffusion. |
| 2023-05-23 | [chatglm.cpp](https://github.com/li-plus/chatglm.cpp) | 999 | ğŸ”  ğŸ’¡  | C++ implementation of ChatGLM-6B & ChatGLM2-6B & more LLMs |
| 2023-05-13 | [fastllm](https://github.com/ztxz16/fastllm) | 2009 | ğŸ”  ğŸ’¡  | çº¯c++çš„å…¨å¹³å°llmåŠ é€Ÿåº“ï¼Œæ”¯æŒpythonè°ƒç”¨ï¼Œchatglm-6Bçº§æ¨¡å‹å•å¡å¯è¾¾10000+token / sï¼Œæ”¯æŒglm, llama, mossåŸºåº§ï¼Œæ‰‹æœºç«¯æµç•…è¿è¡Œ |
| 2023-04-29 | [mlc-llm](https://github.com/mlc-ai/mlc-llm) | 12427 | ğŸ”  ğŸ’¡  | Enable everyone to develop, optimize and deploy AI models natively on everyone's devices. |
| 2023-04-20 | [WebGPT](https://github.com/0hq/WebGPT) | 3186 | ğŸ”  ğŸ’¡  | Run GPT model on the browser with WebGPU. An implementation of GPT inference in less than ~1500 lines of vanilla Javascript.  |
| 2023-04-19 | [OpenLLM](https://github.com/bentoml/OpenLLM) | 5817 | ğŸ”  ğŸ’¡  | Operating LLMs in production |
| 2023-04-13 | [web-llm](https://github.com/mlc-ai/web-llm) | 7196 | ğŸ”  ğŸ’¡  | Bringing large-language models and chat to web browsers. Everything runs inside the browser with no server support. |
| 2023-04-02 | [JittorLLMs](https://github.com/Jittor/JittorLLMs) | 1832 | ğŸ”  ğŸ’¡  | è®¡å›¾å¤§æ¨¡å‹æ¨ç†åº“ï¼Œå…·æœ‰é«˜æ€§èƒ½ã€é…ç½®è¦æ±‚ä½ã€ä¸­æ–‡æ”¯æŒå¥½ã€å¯ç§»æ¤ç­‰ç‰¹ç‚¹ |
| 2023-03-23 | [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) | 2985 | ğŸ”  ğŸ’¡  | Python bindings for llama.cpp |
| 2023-03-21 | [Linly](https://github.com/CVI-SZU/Linly) | 2678 | ğŸ”  â›½ ğŸšŒ 1ï¸âƒ£ 2ï¸âƒ£ 3ï¸âƒ£ âœ‚ï¸ ğŸ’¡ ğŸ’° ğŸ€„  | Chinese-LLaMA 1&2ã€Chinese-Falcon åŸºç¡€æ¨¡å‹ï¼›ChatFlowä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼›ä¸­æ–‡OpenLLaMAæ¨¡å‹ï¼›NLPé¢„è®­ç»ƒ/æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† |
| 2023-03-18 | [LocalAI](https://github.com/go-skynet/LocalAI) | 10621 | ğŸ”  ğŸ–¼ï¸ ğŸ’¡  | :robot: Self-hosted, community-driven, local OpenAI compatible API. Drop-in replacement for OpenAI running LLMs on consumer-grade hardware. Free Open Source OpenAI alternative. No GPU required. Runs ggml, gguf, GPTQ, onnx, TF compatible models: llama, llama2, gpt4all, rwkv, whisper, vicuna, koala, cerebras, falcon, dolly, starcoder, and many others |
| 2023-03-17 | [ChatGLM-MNN](https://github.com/wangzhaode/ChatGLM-MNN) | 897 | ğŸ”  ğŸ’¡  | Pure C++, Easy Deploy ChatGLM-6B. |
| 2023-03-15 | [minillm](https://github.com/kuleshov/minillm) | 617 | ğŸ”  ğŸ’¡  | MiniLLM is a minimal system for running modern LLMs on consumer-grade GPUs |
| 2023-03-15 | [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | 14301 | ğŸ”  1ï¸âƒ£ 2ï¸âƒ£ âœ‚ï¸ ğŸ’¡ ğŸ€„  | ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°CPU/GPUè®­ç»ƒéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs) |
| 2023-03-13 | [llm](https://github.com/rustformers/llm) | 4727 | ğŸ”  ğŸ’¡  | An ecosystem of Rust libraries for working with large language models |
| 2023-03-12 | [bloomz.cpp](https://github.com/NouamaneTazi/bloomz.cpp) | 790 | ğŸ”  âœ‚ï¸ ğŸ’¡  | C++ implementation for BLOOM |
| 2023-03-10 | [llama.cpp](https://github.com/ggerganov/llama.cpp) | 39709 | ğŸ”  âœ‚ï¸ ğŸ’¡  | Port of Facebook's LLaMA model in C/C++ |
| 2023-03-08 | [LLaMA_MPS](https://github.com/jankais3r/LLaMA_MPS) | 569 | ğŸ”  ğŸ’¡  | Run LLaMA (and Stanford-Alpaca) inference on Apple Silicon GPUs. |
| 2023-03-06 | [web-stable-diffusion](https://github.com/mlc-ai/web-stable-diffusion) | 3068 | ğŸ–¼ï¸ ğŸ’¡  | Bringing stable diffusion models to web browsers. Everything runs inside the browser with no server support.  |
| 2023-02-15 | [FlexGen](https://github.com/FMInference/FlexGen) | 8481 | ğŸ”  ğŸ’¡  | Running large language models on a single GPU for throughput-oriented scenarios. |
| 2023-02-11 | [faster-whisper](https://github.com/guillaumekln/faster-whisper) | 4474 | ğŸµ ğŸ’¡  | Faster Whisper transcription with CTranslate2 |
| 2023-02-09 | [vllm](https://github.com/vllm-project/vllm) | 5970 | ğŸ”  ğŸ’¡  | A high-throughput and memory-efficient inference and serving engine for LLMs |
| 2022-11-16 | [ml-stable-diffusion](https://github.com/apple/ml-stable-diffusion) | 14345 | ğŸ–¼ï¸ ğŸ’¡  | Stable Diffusion with Core ML on Apple Silicon |
| 2022-10-08 | [text-generation-inference](https://github.com/huggingface/text-generation-inference) | 4719 | ğŸ”  âœ‚ï¸ ğŸ’¡  | Large Language Model Text Generation Inference |
| 2022-09-18 | [ggml](https://github.com/ggerganov/ggml) | 6632 | ğŸ”  ğŸ’¡  | Tensor library for machine learning |
| 2022-07-15 | [AITemplate](https://github.com/facebookincubator/AITemplate) | 4201 | ğŸ–¼ï¸ ğŸ’¡  | AITemplate is a Python framework which renders neural network into high performance CUDA/HIP C++ code. Specialized for FP16 TensorCore (NVIDIA GPU) and MatrixCore (AMD GPU) inference. |
