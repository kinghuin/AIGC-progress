| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-07-20 | [LinkSoul-AI/Chinese-Llama-2-7b](https://github.com/LinkSoul-AI/Chinese-Llama-2-7b) | 1043 | ğŸ” â›½ğŸšŒ2ï¸âƒ£ğŸ’°ğŸ€„ | å¼€æºç¤¾åŒºç¬¬ä¸€ä¸ªèƒ½ä¸‹è½½ã€èƒ½è¿è¡Œçš„ä¸­æ–‡ LLaMA2 æ¨¡å‹ï¼ |
| 2023-07-19 | [FlagAlpha/Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese) | 2180 | ğŸ” â›½ğŸšŒ2ï¸âƒ£ğŸ’°ğŸ€„ | Llamaä¸­æ–‡ç¤¾åŒºï¼Œæœ€å¥½çš„ä¸­æ–‡Llamaå¤§æ¨¡å‹ï¼Œå®Œå…¨å¼€æºå¯å•†ç”¨ |
| 2023-07-19 | [michael-wzhu/Chinese-LlaMA2](https://github.com/michael-wzhu/Chinese-LlaMA2) | 585 | ğŸ” `ğŸšŒ``ğŸš•``1ï¸âƒ£``2ï¸âƒ£`ğŸ’°ğŸ€„ | Repo for adapting Meta LlaMA2 in Chinese! METAæœ€æ–°å‘å¸ƒçš„LlaMA2çš„æ±‰åŒ–ç‰ˆï¼ ï¼ˆå®Œå…¨å¼€æºå¯å•†ç”¨ï¼‰ |
| 2023-07-19 | [yangjianxin1/Firefly-LLaMA2-Chinese](https://github.com/yangjianxin1/Firefly-LLaMA2-Chinese) | 82 | ğŸ” `ğŸšŒ``ğŸš•``1ï¸âƒ£``2ï¸âƒ£`ğŸ€„ | ä¸­æ–‡LLaMA-2å¤§æ¨¡å‹ï¼Œå…¼å®¹å¯¹ä¸­æ–‡å¤§æ¨¡å‹è¿›è¡Œå¢é‡é¢„è®­ç»ƒ |
| 2023-07-18 | [ymcui/Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) | 722 | ğŸ” `ğŸšŒ``1ï¸âƒ£``2ï¸âƒ£`ğŸ’°ğŸ€„ | ä¸­æ–‡LLaMA-2 & Alpaca-2å¤§è¯­è¨€æ¨¡å‹ (Chinese LLaMA-2 & Alpaca-2 LLMs) |
| 2023-07-17 | [DUOMO/TransGPT](https://github.com/DUOMO/TransGPT) | 320 | ğŸ” â›½ğŸš•2ï¸âƒ£ğŸ’°ğŸ€„ | TransGPTæ˜¯å›½å†…é¦–æ¬¾å¼€æºäº¤é€šå¤§æ¨¡å‹ï¼Œä¸»è¦è‡´åŠ›äºåœ¨çœŸå®äº¤é€šè¡Œä¸šä¸­å‘æŒ¥å®é™…ä»·å€¼ã€‚ |
| 2023-07-13 | [X-PLUG/CValues](https://github.com/X-PLUG/CValues) | 213 | ğŸ” â›½ğŸ€„â“ | é¢å‘ä¸­æ–‡å¤§æ¨¡å‹ä»·å€¼è§‚çš„è¯„ä¼°ä¸å¯¹é½ç ”ç©¶ |
| 2023-07-06 | [InternLM/InternLM](https://github.com/InternLM/InternLM) | 2300 | ğŸ” ğŸšŒ2ï¸âƒ£ğŸ€„ | InternLM has open-sourced a 7 billion parameter base model, a chat model tailored for practical scenarios and the training system. |
| 2023-07-05 | [OpenLMLab/MOSS-RLHF](https://github.com/OpenLMLab/MOSS-RLHF) | 659 | ğŸ” ğŸšŒ3ï¸âƒ£ğŸ€„ | MOSS-RLHF |
| 2023-06-30 | [OpenBMB/VisCPM](https://github.com/OpenBMB/VisCPM) | 704 | ğŸ” ğŸ–¼ï¸ğŸšŒ`2ï¸âƒ£`ğŸ€„ | Chinese and English Multimodal Large Model Series (Chat and Paint) \| åŸºäºCPMåŸºç¡€æ¨¡å‹çš„ä¸­è‹±åŒè¯­å¤šæ¨¡æ€å¤§æ¨¡å‹ç³»åˆ— |
| 2023-06-29 | [IMOSR/MediaGPT](https://github.com/IMOSR/MediaGPT) | 419 | ğŸ” â›½ğŸš•2ï¸âƒ£ğŸ€„ | ä¸­æ–‡çš„è‡ªåª’ä½“å¤§è¯­è¨€æ¨¡å‹MediaGPT(æ›¾ç”¨åMedia LLaMA) |
| 2023-06-24 | [THUDM/ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) | 10461 | ğŸ” ğŸšŒâœ‚ï¸ğŸ€„ | ChatGLM2-6B: An Open Bilingual Chat LLM \| å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ |
| 2023-06-16 | [haonan-li/CMMLU](https://github.com/haonan-li/CMMLU) | 157 | ğŸ” â›½ğŸ€„â“ | CMMLUæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„ğŸ€„è¯„ä¼°åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ğŸ€„è¯­å¢ƒä¸‹çš„çŸ¥è¯†å’ŒğŸ’¡èƒ½åŠ›ã€‚ |
| 2023-06-14 | [baichuan-inc/Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B) | 4719 | ğŸ” â›½ğŸšŒğŸ€„ | A large-scale 7B pretraining language model developed by BaiChuan-Inc. |
| 2023-06-12 | [lyogavin/Anima](https://github.com/lyogavin/Anima) | 1068 | ğŸ” â›½ğŸšŒ2ï¸âƒ£ğŸ€„ | ç¬¬ä¸€ä¸ªå¼€æºçš„åŸºäºQLoRAçš„33Bä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹First QLoRA based open source 33B Chinese LLM |
| 2023-06-02 | [shibing624/MedicalGPT](https://github.com/shibing624/MedicalGPT) | 1071 | ğŸ” â›½ğŸš•1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£ğŸ€„ | MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. è®­ç»ƒåŒ»ç–—å¤§æ¨¡å‹ï¼Œå®ç°åŒ…æ‹¬äºŒæ¬¡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚ |
| 2023-05-28 | [THUDM/WebGLM](https://github.com/THUDM/WebGLM) | 1198 | ğŸ” â›½ğŸšŒ2ï¸âƒ£ğŸ“±ğŸ€„ | WebGLM: An Efficient Web-enhanced Question Answering System (KDD 2023) |
| 2023-05-10 | [Neutralzz/BiLLa](https://github.com/Neutralzz/BiLLa) | 394 | ğŸ” â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£ğŸ€„ | BiLLa: A Bilingual LLaMA with Enhanced Reasoning Ability |
| 2023-04-28 | [dandelionsllm/pandallm](https://github.com/dandelionsllm/pandallm) | 842 | ğŸ” â›½ğŸšŒğŸ€„ | Panda: æµ·å¤–ä¸­æ–‡å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº Llama-7B, -13B, -33B, -65B è¿›è¡Œä¸­æ–‡é¢†åŸŸä¸Šçš„æŒç»­é¢„è®­ç»ƒã€‚ |
| 2023-04-26 | [OpenBuddy/OpenBuddy](https://github.com/OpenBuddy/OpenBuddy) | 785 | ğŸ” ğŸšŒğŸ€„ | Open Multilingual Chatbot for Everyone |
| 2023-04-23 | [THUDM/VisualGLM-6B](https://github.com/THUDM/VisualGLM-6B) | 3100 | ğŸ” ğŸ–¼ï¸â›½ğŸšŒ2ï¸âƒ£âœ‚ï¸ğŸ€„ | Chinese and English multimodal conversational language model \| å¤šæ¨¡æ€ä¸­è‹±åŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ |
| 2023-04-21 | [lvwzhen/law-cn-ai](https://github.com/lvwzhen/law-cn-ai) | 4361 | ğŸ” â›½ğŸ“±ğŸ€„ | âš–ï¸ AI æ³•å¾‹åŠ©æ‰‹ |
| 2023-04-20 | [pengxiao-song/LaWGPT](https://github.com/pengxiao-song/LaWGPT) | 5019 | ğŸ” â›½ğŸš•2ï¸âƒ£ğŸ€„ |  ğŸ‰ Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. åŸºäºä¸­æ–‡æ³•å¾‹çŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹ |
| 2023-04-18 | [kaqijiang/Auto-GPT-ZH](https://github.com/kaqijiang/Auto-GPT-ZH) | 2226 | ğŸ” ğŸ“±ğŸ€„ | Auto-GPTä¸­æ–‡ç‰ˆæœ¬åŠçˆ±å¥½è€…ç»„ç»‡ åŒæ­¥æ›´æ–°åŸé¡¹ç›® AIé¢†åŸŸåˆ›ä¸š è‡ªåª’ä½“ç»„ç»‡ ç”¨AIå·¥ä½œå­¦ä¹ åˆ›ä½œå˜ç° |
| 2023-04-15 | [OpenLMLab/MOSS](https://github.com/OpenLMLab/MOSS) | 11288 | ğŸ” â›½ğŸšŒâœ‚ï¸ğŸ’°ğŸ€„ | An open-source tool-augmented conversational language model from Fudan University |
| 2023-04-08 | [hiyouga/ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning) | 2603 | ğŸ” â›½2ï¸âƒ£3ï¸âƒ£ğŸ€„ | Fine-tuning ChatGLM-6B with PEFT \| åŸºäº PEFT çš„é«˜æ•ˆ ChatGLM å¾®è°ƒ |
| 2023-04-06 | [liucongg/ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning) | 1132 | ğŸ” 2ï¸âƒ£ğŸ€„ | åŸºäºChatGLM-6Bæ¨¡å‹ï¼Œè¿›è¡Œä¸‹æ¸¸å…·ä½“ä»»åŠ¡å¾®è°ƒï¼Œæ¶‰åŠFreezeã€Loraã€P-tuningç­‰ |
| 2023-04-02 | [yangjianxin1/Firefly](https://github.com/yangjianxin1/Firefly) | 1467 | ğŸ” â›½ğŸšŒ2ï¸âƒ£âœ‚ï¸ğŸ€„ | Firefly(æµè¤): ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹(å…¨é‡å¾®è°ƒ+QLoRA)ï¼Œæ”¯æŒå¾®è°ƒLlma2ã€Llamaã€Baichuanã€InternLMã€Ziyaã€Bloomç­‰å¤§æ¨¡å‹ |
| 2023-03-31 | [LC1332/Chinese-alpaca-lora](https://github.com/LC1332/Chinese-alpaca-lora) | 627 | ğŸ” â›½ğŸšŒğŸ€„ | éª†é©¼:A Chinese finetuned instruction LLaMA. Developed by é™ˆå¯æº @ åä¸­å¸ˆèŒƒå¤§å­¦ & æé²é² @ å•†æ±¤ç§‘æŠ€ & å†·å­æ˜‚ @ å•†æ±¤ç§‘æŠ€ |
| 2023-03-31 | [SCIR-HI/Huatuo-Llama-Med-Chinese](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) | 3440 | ğŸ” â›½ğŸš•2ï¸âƒ£ğŸ€„ | Repo for BenTsao [original name: HuaTuo (åé©¼)], Llama-7B tuned with Chinese medical knowledge. æœ¬è‰ï¼ˆåŸåï¼šåé©¼ï¼‰æ¨¡å‹ä»“åº“ï¼ŒåŸºäºä¸­æ–‡åŒ»å­¦çŸ¥è¯†çš„LLaMAæ¨¡å‹æŒ‡ä»¤å¾®è°ƒ |
| 2023-03-31 | [chatchat-space/langchain-ChatGLM](https://github.com/chatchat-space/langchain-ChatGLM) | 12819 | ğŸ” ğŸ“±ğŸ€„ | langchain-ChatGLM, local knowledge based ChatGLM with langchain ï½œ åŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„ ChatGLM é—®ç­” |
| 2023-03-23 | [Facico/Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna) | 3892 | ğŸ” ğŸšŒ2ï¸âƒ£âœ‚ï¸ğŸ€„ | Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model â€”â€” ä¸€ä¸ªä¸­æ–‡ä½èµ„æºçš„llama+loraæ–¹æ¡ˆï¼Œç»“æ„å‚è€ƒalpaca |
| 2023-03-21 | [CVI-SZU/Linly](https://github.com/CVI-SZU/Linly) | 2469 | ğŸ” â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£`3ï¸âƒ£`âœ‚ï¸ğŸ’¡ğŸ’°ğŸ€„ | Chinese-LLaMA 1&2ã€Chinese-Falcon åŸºç¡€æ¨¡å‹ï¼›ChatFlowä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼›ä¸­æ–‡OpenLLaMAæ¨¡å‹ï¼›NLPé¢„è®­ç»ƒ/æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† |
| 2023-03-21 | [LC1332/Luotuo-Chinese-LLM](https://github.com/LC1332/Luotuo-Chinese-LLM) | 3160 | ğŸ” â›½ğŸšŒğŸ€„ | éª†é©¼(Luotuo): Open Sourced Chinese Language Models. Developed by é™ˆå¯æº @ åä¸­å¸ˆèŒƒå¤§å­¦ & æé²é² @ å•†æ±¤ç§‘æŠ€ & å†·å­æ˜‚ @ å•†æ±¤ç§‘æŠ€ |
| 2023-03-21 | [yzfly/awesome-chatgpt-zh](https://github.com/yzfly/awesome-chatgpt-zh) | 7787 | ğŸ” ğŸ“ğŸ€„ | ChatGPT ä¸­æ–‡æŒ‡å—ğŸ”¥ï¼ŒChatGPT ä¸­æ–‡è°ƒæ•™æŒ‡å—ï¼ŒæŒ‡ä»¤æŒ‡å—ï¼Œåº”ç”¨å¼€å‘æŒ‡å—ï¼Œç²¾é€‰èµ„æºæ¸…å•ï¼Œæ›´å¥½çš„ä½¿ç”¨ chatGPT è®©ä½ çš„ç”Ÿäº§åŠ› up up up! ğŸš€ |
| 2023-03-17 | [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE) | 6344 | ğŸ” â›½ğŸšŒ2ï¸âƒ£âœ‚ï¸ğŸ€„ | BELLE: Be Everyone's Large Language model Engineï¼ˆå¼€æºä¸­æ–‡å¯¹è¯å¤§æ¨¡å‹ï¼‰ |
| 2023-03-17 | [hikariming/alpaca_chinese_dataset](https://github.com/hikariming/alpaca_chinese_dataset) | 898 | ğŸ” â›½2ï¸âƒ£ğŸ€„ | äººå·¥ç²¾è°ƒçš„ä¸­æ–‡å¯¹è¯æ•°æ®é›†å’Œä¸€æ®µchatglmçš„å¾®è°ƒä»£ç  |
| 2023-03-16 | [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) | 3195 | ğŸ” 2ï¸âƒ£ğŸ€„ | ä¸€ç§å¹³ä»·çš„chatgptå®ç°æ–¹æ¡ˆ,  åŸºäºChatGLM-6B + LoRA |
| 2023-03-15 | [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | 13451 | ğŸ” 1ï¸âƒ£2ï¸âƒ£âœ‚ï¸ğŸ’¡ğŸ€„ | ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°CPU/GPUè®­ç»ƒéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs) |
| 2023-03-14 | [ssbuild/chatglm_finetuning](https://github.com/ssbuild/chatglm_finetuning) | 1401 | ğŸ” 2ï¸âƒ£ğŸ€„ | chatglm 6b finetuning and alpaca finetuning |
| 2023-03-13 | [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) | 32748 | ğŸ” ğŸšŒ2ï¸âƒ£âœ‚ï¸ğŸ€„ | ChatGLM-6B: An Open Bilingual Dialogue Language Model \| å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ |
| 2023-02-11 | [AI4Finance-Foundation/FinGPT](https://github.com/AI4Finance-Foundation/FinGPT) | 7514 | ğŸ” â›½ğŸš•ğŸ€„ | Data-Centric FinGPT.  Open-source for open finance!  Revolutionize ğŸ”¥    We'll soon release the trained model. |
