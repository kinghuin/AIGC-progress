| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-06-02 | [shibing624/MedicalGPT](https://github.com/shibing624/MedicalGPT) | ![shibing624/MedicalGPT Stars](https://img.shields.io/github/stars/shibing624/MedicalGPT.svg?label=&style=flat-square) | ğŸ” â›½ğŸš•1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£ğŸ€„ | MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. è®­ç»ƒåŒ»ç–—å¤§æ¨¡å‹ï¼Œå®ç°åŒ…æ‹¬äºŒæ¬¡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚ |
| 2023-05-28 | [hiyouga/LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) | ![hiyouga/LLaMA-Efficient-Tuning Stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Efficient-Tuning.svg?label=&style=flat-square) | ğŸ” 1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£ | Fine-tuning LLaMA with PEFT (PT+SFT+RLHF with QLoRA) |
| 2023-05-18 | [OFA-Sys/ONE-PEACE](https://github.com/OFA-Sys/ONE-PEACE) | ![OFA-Sys/ONE-PEACE Stars](https://img.shields.io/github/stars/OFA-Sys/ONE-PEACE.svg?label=&style=flat-square) | ğŸ–¼ï¸ğŸšŒ1ï¸âƒ£2ï¸âƒ£ | A general representation modal across vision, audio, language modalities. Paper: ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities |
| 2023-05-12 | [TigerResearch/TigerBot](https://github.com/TigerResearch/TigerBot) | ![TigerResearch/TigerBot Stars](https://img.shields.io/github/stars/TigerResearch/TigerBot.svg?label=&style=flat-square) | ğŸ” â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£ | TigerBot: A multi-language multi-task LLM |
| 2023-05-10 | [Neutralzz/BiLLa](https://github.com/Neutralzz/BiLLa) | ![Neutralzz/BiLLa Stars](https://img.shields.io/github/stars/Neutralzz/BiLLa.svg?label=&style=flat-square) | ğŸ” â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£ğŸ€„ | BiLLa: A Bilingual LLaMA with Enhanced Reasoning Ability |
| 2023-05-06 | [DAMO-NLP-SG/Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA) | ![DAMO-NLP-SG/Video-LLaMA Stars](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?label=&style=flat-square) | ğŸ” ğŸ¥ğŸšŒ1ï¸âƒ£2ï¸âƒ£ | Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding |
| 2023-05-04 | [Lightning-AI/lit-gpt](https://github.com/Lightning-AI/lit-gpt) | ![Lightning-AI/lit-gpt Stars](https://img.shields.io/github/stars/Lightning-AI/lit-gpt.svg?label=&style=flat-square) | ğŸ” ğŸ–¼ï¸ğŸšŒ1ï¸âƒ£2ï¸âƒ£ | Implementation of Falcon, StableLM, Pythia, INCITE language models based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed. |
| 2023-04-17 | [haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA) | ![haotian-liu/LLaVA Stars](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?label=&style=flat-square) | ğŸ” ğŸ–¼ï¸â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£ | Large Language-and-Vision Assistant built towards multimodal GPT-4 level capabilities. |
| 2023-04-15 | [Vision-CAIR/MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) | ![Vision-CAIR/MiniGPT-4 Stars](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?label=&style=flat-square) | ğŸ” ğŸ–¼ï¸ğŸšŒ1ï¸âƒ£2ï¸âƒ£ | MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models |
| 2023-03-22 | [Lightning-AI/lit-llama](https://github.com/Lightning-AI/lit-llama) | ![Lightning-AI/lit-llama Stars](https://img.shields.io/github/stars/Lightning-AI/lit-llama.svg?label=&style=flat-square) | ğŸ” ğŸšŒ1ï¸âƒ£2ï¸âƒ£âœ‚ï¸âœ… | Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed. |
| 2023-03-21 | [CVI-SZU/Linly](https://github.com/CVI-SZU/Linly) | ![CVI-SZU/Linly Stars](https://img.shields.io/github/stars/CVI-SZU/Linly.svg?label=&style=flat-square) | ğŸ” â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£âœ‚ï¸ğŸ’¡âœ…ğŸ€„ | Chinese-LLaMA ã€Chinese-Falcon åŸºç¡€æ¨¡å‹ï¼›ChatFlowä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼›ä¸­æ–‡OpenLLaMAæ¨¡å‹ï¼›NLPé¢„è®­ç»ƒ/æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† |
| 2023-03-15 | [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | ![ymcui/Chinese-LLaMA-Alpaca Stars](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca.svg?label=&style=flat-square) | ğŸ” 1ï¸âƒ£2ï¸âƒ£âœ‚ï¸ğŸ’¡ğŸ€„ | ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°CPU/GPUè®­ç»ƒéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs) |
| 2023-03-03 | [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit) | ![togethercomputer/OpenChatKit Stars](https://img.shields.io/github/stars/togethercomputer/OpenChatKit.svg?label=&style=flat-square) | ğŸ” â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£ |  |
| 2022-12-28 | [karpathy/nanoGPT](https://github.com/karpathy/nanoGPT) | ![karpathy/nanoGPT Stars](https://img.shields.io/github/stars/karpathy/nanoGPT.svg?label=&style=flat-square) | ğŸ” â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£ | The simplest, fastest repository for training/finetuning medium-sized GPTs. |
| 2022-08-24 | [salesforce/LAVIS](https://github.com/salesforce/LAVIS) | ![salesforce/LAVIS Stars](https://img.shields.io/github/stars/salesforce/LAVIS.svg?label=&style=flat-square) | ğŸ” ğŸ–¼ï¸â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£ | LAVIS - A One-stop Library for Language-Vision Intelligence |
