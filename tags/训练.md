| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-05-19 | [YuanGongND/ltu](https://github.com/YuanGongND/ltu) | ![YuanGongND/ltu Stars](https://img.shields.io/github/stars/YuanGongND/ltu.svg?label=&style=flat-square) | `æ–‡æœ¬`,`è¯­éŸ³`,`è®­ç»ƒ(TD)` | Github Repo for Paper "Listen, Think, and Understand". |
| 2023-05-18 | [OpenGVLab/VisionLLM](https://github.com/OpenGVLab/VisionLLM) | ![OpenGVLab/VisionLLM Stars](https://img.shields.io/github/stars/OpenGVLab/VisionLLM.svg?label=&style=flat-square) | `å›¾åƒ`,`é€šç”¨æ¨¡å‹(TD)`,`è®­ç»ƒ(TD)` | VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks |
| 2023-05-18 | [OFA-Sys/ONE-PEACE](https://github.com/OFA-Sys/ONE-PEACE) | ![OFA-Sys/ONE-PEACE Stars](https://img.shields.io/github/stars/OFA-Sys/ONE-PEACE.svg?label=&style=flat-square) | `å›¾åƒ`,`é€šç”¨æ¨¡å‹`,`é¢„è®­ç»ƒ`,`è®­ç»ƒ` | A general representation modal across vision, audio, language modalities. |
| 2023-05-17 | [mit-han-lab/fastcomposer](https://github.com/mit-han-lab/fastcomposer) | ![mit-han-lab/fastcomposer Stars](https://img.shields.io/github/stars/mit-han-lab/fastcomposer.svg?label=&style=flat-square) | `å›¾åƒ`,`æ•°æ®(TD)`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ(TD)` |  |
| 2023-05-17 | [ysymyth/tree-of-thought-llm](https://github.com/ysymyth/tree-of-thought-llm) | ![ysymyth/tree-of-thought-llm Stars](https://img.shields.io/github/stars/ysymyth/tree-of-thought-llm.svg?label=&style=flat-square) | `æ–‡æœ¬`,`è®­ç»ƒ(TD)` |  |
| 2023-05-15 | [PKU-Alignment/safe-rlhf](https://github.com/PKU-Alignment/safe-rlhf) | ![PKU-Alignment/safe-rlhf Stars](https://img.shields.io/github/stars/PKU-Alignment/safe-rlhf.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ`,`å¼ºåŒ–` | Safe-RLHF: Constrained Value Alignment via Safe Reinforcement Learning from Human Feedback |
| 2023-05-11 | [kuleshov-group/llmtune](https://github.com/kuleshov-group/llmtune) | ![kuleshov-group/llmtune Stars](https://img.shields.io/github/stars/kuleshov-group/llmtune.svg?label=&style=flat-square) | `æ–‡æœ¬`,`è®­ç»ƒ` | 4-Bit Finetuning of Large Language Models on One Consumer GPU |
| 2023-05-07 | [conceptofmind/PaLM](https://github.com/conceptofmind/PaLM) | ![conceptofmind/PaLM Stars](https://img.shields.io/github/stars/conceptofmind/PaLM.svg?label=&style=flat-square) | `æ–‡æœ¬`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ` | An open-source implementation of Google's PaLM models |
| 2023-05-03 | [IBM/Dromedary](https://github.com/IBM/Dromedary) | ![IBM/Dromedary Stars](https://img.shields.io/github/stars/IBM/Dromedary.svg?label=&style=flat-square) | `æ–‡æœ¬`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ` | Dromedary: towards helpful, ethical and reliable LLMs. |
| 2023-04-28 | [mosaicml/llm-foundry](https://github.com/mosaicml/llm-foundry) | ![mosaicml/llm-foundry Stars](https://img.shields.io/github/stars/mosaicml/llm-foundry.svg?label=&style=flat-square) | `æ–‡æœ¬`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ` | LLM training code for MosaicML foundation models |
| 2023-04-26 | [mosaicml/diffusion](https://github.com/mosaicml/diffusion) | ![mosaicml/diffusion Stars](https://img.shields.io/github/stars/mosaicml/diffusion.svg?label=&style=flat-square) | `å›¾åƒ`,`è®­ç»ƒ` | This repo contains code used to train your own Stable Diffusion model on your own data. |
| 2023-04-25 | [X-PLUG/mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl) | ![X-PLUG/mPLUG-Owl Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?label=&style=flat-square) | `æ–‡æœ¬`,`å›¾åƒ`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ` | mPLUG-OwlğŸ¦‰: Modularization Empowers Large Language Models with Multimodality |
| 2023-04-24 | [bigcode-project/starcoder](https://github.com/bigcode-project/starcoder) | ![bigcode-project/starcoder Stars](https://img.shields.io/github/stars/bigcode-project/starcoder.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`å®šåˆ¶æ¨¡å‹`,`è®­ç»ƒ` | Home of StarCoder: fine-tuning & inference! |
| 2023-04-23 | [THUDM/VisualGLM-6B](https://github.com/THUDM/VisualGLM-6B) | ![THUDM/VisualGLM-6B Stars](https://img.shields.io/github/stars/THUDM/VisualGLM-6B.svg?label=&style=flat-square) | `æ–‡æœ¬`,`å›¾åƒ`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ`,`å‹ç¼©`,`ä¸­æ–‡` | Chinese and English multimodal conversational language model \| å¤šæ¨¡æ€ä¸­è‹±åŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ |
| 2023-04-23 | [nlpxucan/WizardLM](https://github.com/nlpxucan/WizardLM) | ![nlpxucan/WizardLM Stars](https://img.shields.io/github/stars/nlpxucan/WizardLM.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ` | WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions |
| 2023-04-23 | [minosvasilias/godot-dodo](https://github.com/minosvasilias/godot-dodo) | ![minosvasilias/godot-dodo Stars](https://img.shields.io/github/stars/minosvasilias/godot-dodo.svg?label=&style=flat-square) | `æ–‡æœ¬`,`å®šåˆ¶æ¨¡å‹`,`è®­ç»ƒ` | Finetuning large language models for GDScript generation. |
| 2023-04-23 | [StevenGrove/GPT4Tools](https://github.com/StevenGrove/GPT4Tools) | ![StevenGrove/GPT4Tools Stars](https://img.shields.io/github/stars/StevenGrove/GPT4Tools.svg?label=&style=flat-square) | `æ–‡æœ¬`,`å›¾åƒ`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ` | GPT4Tools is an intelligent system that can automatically decide, control, and utilize different visual foundation models, allowing the user to interact with images during a conversation. |
| 2023-04-20 | [pengxiao-song/LaWGPT](https://github.com/pengxiao-song/LaWGPT) | ![pengxiao-song/LaWGPT Stars](https://img.shields.io/github/stars/pengxiao-song/LaWGPT.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`å®šåˆ¶æ¨¡å‹`,`è®­ç»ƒ`,`ä¸­æ–‡` |  ğŸ‰ Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. åŸºäºä¸­æ–‡æ³•å¾‹çŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹ |
| 2023-04-19 | [RiseInRose/MiniGPT-4-ZH](https://github.com/RiseInRose/MiniGPT-4-ZH) | ![RiseInRose/MiniGPT-4-ZH Stars](https://img.shields.io/github/stars/RiseInRose/MiniGPT-4-ZH.svg?label=&style=flat-square) | `æ–‡æœ¬`,`å›¾åƒ`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ`,`å‹ç¼©` | MiniGPT-4 ä¸­æ–‡éƒ¨ç½²ç¿»è¯‘ å®Œå–„éƒ¨ç½²ç»†èŠ‚ |
| 2023-04-17 | [h2oai/h2o-llmstudio](https://github.com/h2oai/h2o-llmstudio) | ![h2oai/h2o-llmstudio Stars](https://img.shields.io/github/stars/h2oai/h2o-llmstudio.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ`,`å¯å•†ç”¨` | H2O LLM Studio - a framework and no-code GUI for fine-tuning LLMs |
| 2023-04-15 | [Vision-CAIR/MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) | ![Vision-CAIR/MiniGPT-4 Stars](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?label=&style=flat-square) | `æ–‡æœ¬`,`å›¾åƒ`,`é€šç”¨æ¨¡å‹`,`é¢„è®­ç»ƒ`,`è®­ç»ƒ` | MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models |
| 2023-04-10 | [declare-lab/tango](https://github.com/declare-lab/tango) | ![declare-lab/tango Stars](https://img.shields.io/github/stars/declare-lab/tango.svg?label=&style=flat-square) | `æ–‡æœ¬`,`è¯­éŸ³`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ` | Codes and Model of the paper "Text-to-Audio Generation using Instruction Tuned LLM and Latent Diffusion Model" |
| 2023-04-08 | [hiyouga/ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning) | ![hiyouga/ChatGLM-Efficient-Tuning Stars](https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`è®­ç»ƒ`,`å¼ºåŒ–`,`ä¸­æ–‡` | Fine-tuning ChatGLM-6B with PEFT \| åŸºäº PEFT çš„é«˜æ•ˆ ChatGLM å¾®è°ƒ |
| 2023-04-06 | [Instruction-Tuning-with-GPT-4/GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | ![Instruction-Tuning-with-GPT-4/GPT-4-LLM Stars](https://img.shields.io/github/stars/Instruction-Tuning-with-GPT-4/GPT-4-LLM.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`è®­ç»ƒ` | Instruction Tuning with GPT-4 |
| 2023-04-06 | [liucongg/ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning) | ![liucongg/ChatGLM-Finetuning Stars](https://img.shields.io/github/stars/liucongg/ChatGLM-Finetuning.svg?label=&style=flat-square) | `æ–‡æœ¬`,`è®­ç»ƒ`,`ä¸­æ–‡` | åŸºäºChatGLM-6Bæ¨¡å‹ï¼Œè¿›è¡Œä¸‹æ¸¸å…·ä½“ä»»åŠ¡å¾®è°ƒï¼Œæ¶‰åŠFreezeã€Loraã€P-tuningç­‰ |
| 2023-04-01 | [FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) | ![FreedomIntelligence/LLMZoo Stars](https://img.shields.io/github/stars/FreedomIntelligence/LLMZoo.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ`,`å‹ç¼©`,`æ–‡æ¡£` | âš¡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.âš¡ |
| 2023-03-31 | [SCIR-HI/Huatuo-Llama-Med-Chinese](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) | ![SCIR-HI/Huatuo-Llama-Med-Chinese Stars](https://img.shields.io/github/stars/SCIR-HI/Huatuo-Llama-Med-Chinese.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`å®šåˆ¶æ¨¡å‹`,`è®­ç»ƒ`,`ä¸­æ–‡` | Repo for BenTsao [original name: HuaTuo (åé©¼)], Llama-7B tuned with Chinese medical knowledge. æœ¬è‰ï¼ˆåŸåï¼šåé©¼ï¼‰æ¨¡å‹ä»“åº“ï¼ŒåŸºäºä¸­æ–‡åŒ»å­¦çŸ¥è¯†çš„LLaMAæ¨¡å‹æŒ‡ä»¤å¾®è°ƒ |
| 2023-03-31 | [project-baize/baize-chatbot](https://github.com/project-baize/baize-chatbot) | ![project-baize/baize-chatbot Stars](https://img.shields.io/github/stars/project-baize/baize-chatbot.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ` | Let ChatGPT teach your own chatbot in hours with a single GPU! |
| 2023-03-29 | [AGI-Edgerunners/LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters) | ![AGI-Edgerunners/LLM-Adapters Stars](https://img.shields.io/github/stars/AGI-Edgerunners/LLM-Adapters.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ` | LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models |
| 2023-03-28 | [WangRongsheng/ChatGenTitle](https://github.com/WangRongsheng/ChatGenTitle) | ![WangRongsheng/ChatGenTitle Stars](https://img.shields.io/github/stars/WangRongsheng/ChatGenTitle.svg?label=&style=flat-square) | `æ–‡æœ¬`,`å®šåˆ¶æ¨¡å‹`,`è®­ç»ƒ` | ğŸŒŸ ChatGenTitleï¼šä½¿ç”¨ç™¾ä¸‡arXivè®ºæ–‡ä¿¡æ¯åœ¨LLaMAæ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒçš„è®ºæ–‡é¢˜ç›®ç”Ÿæˆæ¨¡å‹ |
| 2023-03-27 | [OptimalScale/LMFlow](https://github.com/OptimalScale/LMFlow) | ![OptimalScale/LMFlow Stars](https://img.shields.io/github/stars/OptimalScale/LMFlow.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ`,`å¼ºåŒ–` | An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Model for All. |
| 2023-03-24 | [PhoebusSi/Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT) | ![PhoebusSi/Alpaca-CoT Stars](https://img.shields.io/github/stars/PhoebusSi/Alpaca-CoT.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`è®­ç»ƒ` | We unified the interfaces of instruction-tuning data (e.g., CoT data), multiple LLMs and parameter-efficient methods (e.g., lora, p-tuning) together for easy use. Meanwhile, we created a new branch to build a Tabular LLM.ï¼ˆæˆ‘ä»¬åˆ†åˆ«ç»Ÿä¸€äº†ä¸°å¯Œçš„IFTæ•°æ®ï¼ˆå¦‚CoTæ•°æ®ï¼Œç›®å‰ä»ä¸æ–­æ‰©å……ï¼‰ã€å¤šç§è®­ç»ƒæ•ˆç‡æ–¹æ³•ï¼ˆå¦‚loraï¼Œp-tuningï¼‰ä»¥åŠå¤šç§LLMsï¼Œä¸‰ä¸ªå±‚é¢ä¸Šçš„æ¥å£ï¼Œæ‰“é€ æ–¹ä¾¿ç ”ç©¶äººå‘˜ä¸Šæ‰‹çš„LLM-IFTç ”ç©¶å¹³å°ã€‚åŒæ—¶tabular_llmåˆ†æ”¯æ„å»ºäº†é¢å‘è¡¨æ ¼æ™ºèƒ½ä»»åŠ¡çš„LLMã€‚ |
| 2023-03-24 | [h2oai/h2ogpt](https://github.com/h2oai/h2ogpt) | ![h2oai/h2ogpt Stars](https://img.shields.io/github/stars/h2oai/h2ogpt.svg?label=&style=flat-square) | `æ–‡æœ¬`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ`,`å¯å•†ç”¨` | Come join the movement to make the world's best open source GPT led by H2O.ai - 100% private chat and document search, no data leaks, Apache 2.0 |
| 2023-03-23 | [Facico/Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna) | ![Facico/Chinese-Vicuna Stars](https://img.shields.io/github/stars/Facico/Chinese-Vicuna.svg?label=&style=flat-square) | `æ–‡æœ¬`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ`,`å‹ç¼©`,`ä¸­æ–‡` | Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model â€”â€” ä¸€ä¸ªä¸­æ–‡ä½èµ„æºçš„llama+loraæ–¹æ¡ˆï¼Œç»“æ„å‚è€ƒalpaca |
| 2023-03-22 | [Lightning-AI/lit-llama](https://github.com/Lightning-AI/lit-llama) | ![Lightning-AI/lit-llama Stars](https://img.shields.io/github/stars/Lightning-AI/lit-llama.svg?label=&style=flat-square) | `æ–‡æœ¬`,`é€šç”¨æ¨¡å‹`,`é¢„è®­ç»ƒ`,`è®­ç»ƒ`,`å‹ç¼©`,`å¯å•†ç”¨` | Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed. |
| 2023-03-21 | [Kent0n-Li/ChatDoctor](https://github.com/Kent0n-Li/ChatDoctor) | ![Kent0n-Li/ChatDoctor Stars](https://img.shields.io/github/stars/Kent0n-Li/ChatDoctor.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`å®šåˆ¶æ¨¡å‹`,`è®­ç»ƒ` |  |
| 2023-03-21 | [CVI-SZU/Linly](https://github.com/CVI-SZU/Linly) | ![CVI-SZU/Linly Stars](https://img.shields.io/github/stars/CVI-SZU/Linly.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`é¢„è®­ç»ƒ`,`è®­ç»ƒ`,`å¼ºåŒ–(TD)`,`å‹ç¼©`,`æ¨ç†`,`å¯å•†ç”¨`,`ä¸­æ–‡` | Chinese-LLaMAåŸºç¡€æ¨¡å‹ï¼›ChatFlowä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼›ä¸­æ–‡OpenLLaMAæ¨¡å‹ï¼›NLPé¢„è®­ç»ƒ/æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† |
| 2023-03-19 | [lm-sys/FastChat](https://github.com/lm-sys/FastChat) | ![lm-sys/FastChat Stars](https://img.shields.io/github/stars/lm-sys/FastChat.svg?label=&style=flat-square) | `æ–‡æœ¬`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ`,`å¯å•†ç”¨` | An open platform for training, serving, and evaluating large languages. Release repo for Vicuna and FastChat-T5. |
| 2023-03-19 | [stochasticai/xturing](https://github.com/stochasticai/xturing) | ![stochasticai/xturing Stars](https://img.shields.io/github/stars/stochasticai/xturing.svg?label=&style=flat-square) | `æ–‡æœ¬`,`è®­ç»ƒ` | Easily build, customize and control your own LLMs |
| 2023-03-17 | [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE) | ![LianjiaTech/BELLE Stars](https://img.shields.io/github/stars/LianjiaTech/BELLE.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ`,`å‹ç¼©`,`ä¸­æ–‡` | BELLE: Be Everyone's Large Language model Engineï¼ˆå¼€æºä¸­æ–‡å¯¹è¯å¤§æ¨¡å‹ï¼‰ |
| 2023-03-17 | [hikariming/alpaca_chinese_dataset](https://github.com/hikariming/alpaca_chinese_dataset) | ![hikariming/alpaca_chinese_dataset Stars](https://img.shields.io/github/stars/hikariming/alpaca_chinese_dataset.svg?label=&style=flat-square) | `æ–‡æœ¬`,`æ•°æ®`,`è®­ç»ƒ`,`ä¸­æ–‡` | äººå·¥ç²¾è°ƒçš„ä¸­æ–‡å¯¹è¯æ•°æ®é›†å’Œä¸€æ®µchatglmçš„å¾®è°ƒä»£ç  |
| 2023-03-16 | [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) | ![mymusise/ChatGLM-Tuning Stars](https://img.shields.io/github/stars/mymusise/ChatGLM-Tuning.svg?label=&style=flat-square) | `æ–‡æœ¬`,`è®­ç»ƒ`,`ä¸­æ–‡` | ä¸€ç§å¹³ä»·çš„chatgptå®ç°æ–¹æ¡ˆ,  åŸºäºChatGLM-6B + LoRA |
| 2023-03-15 | [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | ![ymcui/Chinese-LLaMA-Alpaca Stars](https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca.svg?label=&style=flat-square) | `æ–‡æœ¬`,`é¢„è®­ç»ƒ`,`è®­ç»ƒ`,`å‹ç¼©`,`æ¨ç†`,`ä¸­æ–‡` | ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°CPU/GPUéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs) |
| 2023-03-14 | [ssbuild/chatglm_finetuning](https://github.com/ssbuild/chatglm_finetuning) | ![ssbuild/chatglm_finetuning Stars](https://img.shields.io/github/stars/ssbuild/chatglm_finetuning.svg?label=&style=flat-square) | `æ–‡æœ¬`,`è®­ç»ƒ`,`ä¸­æ–‡` | chatglm 6b finetuning and alpaca finetuning |
| 2023-03-13 | [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) | ![THUDM/ChatGLM-6B Stars](https://img.shields.io/github/stars/THUDM/ChatGLM-6B.svg?label=&style=flat-square) | `æ–‡æœ¬`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ`,`å‹ç¼©`,`ä¸­æ–‡` | ChatGLM-6B: An Open Bilingual Dialogue Language Model \| å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ |
| 2023-03-13 | [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora) | ![tloen/alpaca-lora Stars](https://img.shields.io/github/stars/tloen/alpaca-lora.svg?label=&style=flat-square) | `æ–‡æœ¬`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ` | Instruct-tune LLaMA on consumer hardware |
| 2023-03-10 | [svc-develop-team/so-vits-svc](https://github.com/svc-develop-team/so-vits-svc) | ![svc-develop-team/so-vits-svc Stars](https://img.shields.io/github/stars/svc-develop-team/so-vits-svc.svg?label=&style=flat-square) | `è¯­éŸ³`,`é€šç”¨æ¨¡å‹`,`è®­ç»ƒ` | SoftVC VITS Singing Voice Conversion |
