| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-08-29 | [AnomalyGPT](https://github.com/CASIA-IVA-Lab/AnomalyGPT) | 65 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  |  |
| 2023-08-29 | [CoVR](https://github.com/lucas-ventura/CoVR) | 23 | ğŸ¥ â›½ ğŸšŒ 2ï¸âƒ£  | Official PyTorch implementation of the paper "CoVR: Learning Composed Video Retrieval from Web Video Captions". |
| 2023-08-28 | [DiffBIR](https://github.com/XPixelGroup/DiffBIR) | 24 | ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  |  |
| 2023-08-19 | [DiffusionTrack](https://github.com/RainBowLuoCS/DiffusionTrack) | 107 | ğŸ–¼ï¸ â›½ 2ï¸âƒ£  |  |
| 2023-08-18 | [meru](https://github.com/facebookresearch/meru) | 70 | ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | Code for the paper "Hyperbolic Image-Text Representations", Desai et al, ICML 2023 |
| 2023-08-18 | [Dataset_Quantization](https://github.com/magic-research/Dataset_Quantization) | 174 | ğŸ”  ğŸ–¼ï¸ â›½ â“  | [ICCV2023] Dataset Quantization |
| 2023-08-15 | [CoDeF](https://github.com/qiuyu96/CoDeF) | 3581 | ğŸ¥ â›½ ğŸšŒ 2ï¸âƒ£  | Official PyTorch implementation of CoDeF: Content Deformation Fields for Temporally Consistent Video Processing |
| 2023-08-14 | [WanJuan1.0](https://github.com/opendatalab/WanJuan1.0) | 259 | ğŸ”  ğŸ–¼ï¸ ğŸ¥ â›½  | ä¸‡å·1.0å¤šæ¨¡æ€è¯­æ–™ |
| 2023-08-10 | [Taiwan-LLaMa](https://github.com/MiuLab/Taiwan-LLaMa) | 513 | ğŸ”  â›½ ğŸšŒ ğŸ€„  | Traditional Mandarin LLMs for Taiwan |
| 2023-08-01 | [QiaoBan](https://github.com/HIT-SCIR-SC/QiaoBan) | 75 | ğŸ”  â›½ ğŸš• 2ï¸âƒ£  | è¿™æ˜¯ä¸€ä¸ªå·§æ¿å¤§æ¨¡å‹çš„ä»“åº“ï¼Œæ—¨åœ¨æ„å»ºä¸€ä¸ªé¢å‘å„¿ç«¥æƒ…æ„Ÿé™ªä¼´çš„å¤§æ¨¡å‹ |
| 2023-08-01 | [LISA](https://github.com/dvlab-research/LISA) | 866 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ  | Project Page for "LISA: Reasoning Segmentation via Large Language Model" |
| 2023-07-21 | [LLaMA2-Accessory](https://github.com/Alpha-VLLM/LLaMA2-Accessory) | 1440 | ğŸ”  ğŸ–¼ï¸ â›½ 1ï¸âƒ£ 2ï¸âƒ£ ğŸ”¨ `Python`  | An Open-source Toolkit for LLM Development |
| 2023-07-21 | [Subject-Diffusion](https://github.com/OPPO-Mente-Lab/Subject-Diffusion) | 158 | ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning |
| 2023-07-20 | [Chinese-Llama-2-7b](https://github.com/LinkSoul-AI/Chinese-Llama-2-7b) | 1765 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ ğŸ’° ğŸ€„  | å¼€æºç¤¾åŒºç¬¬ä¸€ä¸ªèƒ½ä¸‹è½½ã€èƒ½è¿è¡Œçš„ä¸­æ–‡ LLaMA2 æ¨¡å‹ï¼ |
| 2023-07-19 | [Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese) | 4631 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ ğŸ’° ğŸ€„  | Llamaä¸­æ–‡ç¤¾åŒºï¼Œæœ€å¥½çš„ä¸­æ–‡Llamaå¤§æ¨¡å‹ï¼Œå®Œå…¨å¼€æºå¯å•†ç”¨ |
| 2023-07-17 | [TransGPT](https://github.com/DUOMO/TransGPT) | 441 | ğŸ”  â›½ ğŸš• 2ï¸âƒ£ ğŸ’° ğŸ€„  | TransGPTæ˜¯å›½å†…é¦–æ¬¾å¼€æºäº¤é€šå¤§æ¨¡å‹ï¼Œä¸»è¦è‡´åŠ›äºåœ¨çœŸå®äº¤é€šè¡Œä¸šä¸­å‘æŒ¥å®é™…ä»·å€¼ã€‚ |
| 2023-07-17 | [ER-NeRF](https://github.com/Fictionarry/ER-NeRF) | 150 | ğŸ¥ â›½ ğŸšŒ 2ï¸âƒ£  | [ICCV'23] Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis |
| 2023-07-16 | [bubogpt](https://github.com/magic-research/bubogpt) | 358 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ  | BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs |
| 2023-07-13 | [CValues](https://github.com/X-PLUG/CValues) | 254 | ğŸ”  â›½ ğŸ€„ â“  | é¢å‘ä¸­æ–‡å¤§æ¨¡å‹ä»·å€¼è§‚çš„è¯„ä¼°ä¸å¯¹é½ç ”ç©¶ |
| 2023-07-06 | [GPT4RoI](https://github.com/jshilong/GPT4RoI) | 313 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest |
| 2023-07-04 | [Finetune-ChatGLM2-6B](https://github.com/SpongebBob/Finetune-ChatGLM2-6B) | 265 | ğŸ”  â›½ 2ï¸âƒ£ âœ‚ï¸  | ChatGLM2-6B å…¨å‚æ•°å¾®è°ƒï¼Œæ”¯æŒå¤šè½®å¯¹è¯çš„é«˜æ•ˆå¾®è°ƒã€‚ |
| 2023-07-04 | [StyleDrop-PyTorch](https://github.com/zideliu/StyleDrop-PyTorch) | 441 | ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | Unoffical implement for [StyleDrop](https://arxiv.org/abs/2306.00983) |
| 2023-06-29 | [MediaGPT](https://github.com/IMOSR/MediaGPT) | 463 | ğŸ”  â›½ ğŸš• 2ï¸âƒ£ ğŸ€„  | ä¸­æ–‡çš„è‡ªåª’ä½“å¤§è¯­è¨€æ¨¡å‹MediaGPT(æ›¾ç”¨åMedia LLaMA) |
| 2023-06-27 | [LLaVAR](https://github.com/SALT-NLP/LLaVAR) | 129 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | Code/Data for the paper: "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding" |
| 2023-06-26 | [EduChat](https://github.com/icalk-nlp/EduChat) | 257 | ğŸ”  â›½ ğŸš•  | An open-source educational chat model from ICALK, East China Normal University. å¼€æºä¸­è‹±æ•™è‚²å¯¹è¯å¤§æ¨¡å‹ã€‚(é€šç”¨åŸºåº§æ¨¡å‹ï¼ŒGPUéƒ¨ç½²ï¼Œæ•°æ®æ¸…ç†) è‡´æ•¬: LLaMA, MOSS, BELLE, Ziya, vLLM |
| 2023-06-22 | [generative-models](https://github.com/Stability-AI/generative-models) | 8851 | ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | Generative Models by Stability AI |
| 2023-06-16 | [CMMLU](https://github.com/haonan-li/CMMLU) | 267 | ğŸ”  â›½ ğŸ€„ â“  | CMMLUæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„ğŸ€„è¯„ä¼°åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ğŸ€„è¯­å¢ƒä¸‹çš„çŸ¥è¯†å’ŒğŸ’¡èƒ½åŠ›ã€‚ |
| 2023-06-14 | [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B) | 5022 | ğŸ”  â›½ ğŸšŒ ğŸ€„  | A large-scale 7B pretraining language model developed by BaiChuan-Inc. |
| 2023-06-12 | [Anima](https://github.com/lyogavin/Anima) | 1118 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ ğŸ€„  | ç¬¬ä¸€ä¸ªå¼€æºçš„åŸºäºQLoRAçš„33Bä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹First QLoRA based open source 33B Chinese LLM |
| 2023-06-09 | [open-instruct](https://github.com/allenai/open-instruct) | 407 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | We explore instruction-tuning popular base models on publicly available datasets. |
| 2023-06-07 | [ChatLaw](https://github.com/PKU-YuanGroup/ChatLaw) | 4796 | ğŸ”  â›½ ğŸš•  | ä¸­æ–‡æ³•å¾‹å¤§æ¨¡å‹ |
| 2023-06-02 | [MedicalGPT](https://github.com/shibing624/MedicalGPT) | 1381 | ğŸ”  â›½ ğŸš• 1ï¸âƒ£ 2ï¸âƒ£ 3ï¸âƒ£ ğŸ€„  | MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. è®­ç»ƒåŒ»ç–—å¤§æ¨¡å‹ï¼Œå®ç°äº†åŒ…æ‹¬å¢é‡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€RLHF(å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒ)å’ŒDPO(ç›´æ¥åå¥½ä¼˜åŒ–)ã€‚ |
| 2023-05-31 | [Chat-Haruhi-Suzumiya](https://github.com/LC1332/Chat-Haruhi-Suzumiya) | 674 | ğŸ”  ğŸµ â›½ ğŸš•  | Chatå‡‰å®«æ˜¥æ—¥, ç”±æé²é², å†·å­æ˜‚ç­‰åŒå­¦å¼€å‘çš„æ¨¡ä»¿äºŒæ¬¡å…ƒå¯¹è¯çš„èŠå¤©æœºå™¨äººã€‚ |
| 2023-05-31 | [LLM-Blender](https://github.com/yuchenlin/LLM-Blender) | 504 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | [ACL2023] We introduce LLM-Blender, an innovative ensembling framework to attain consistently superior performance by leveraging the diverse strengths of multiple open-source LLMs. LLM-Blender cut the weaknesses through ranking and integrate the strengths through fusing generation to enhance the capability of LLMs. |
| 2023-05-28 | [WebGLM](https://github.com/THUDM/WebGLM) | 1287 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ ğŸ”¨ ğŸ€„ `Python`  | WebGLM: An Efficient Web-enhanced Question Answering System (KDD 2023) |
| 2023-05-25 | [openchat](https://github.com/imoneoi/openchat) | 1579 | ğŸ”  â›½ ğŸšŒ ğŸš• 2ï¸âƒ£  | OpenChat: Advancing Open-source Language Models with Imperfect Data |
| 2023-05-24 | [SAIL](https://github.com/luohongyin/SAIL) | 138 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | SAIL: Search Augmented Instruction Learning |
| 2023-05-24 | [YaYi](https://github.com/wenge-research/YaYi) | 1550 | ğŸ”  â›½ ğŸš• 2ï¸âƒ£  | é›…æ„å¤§æ¨¡å‹ï¼šä¸ºå®¢æˆ·æ‰“é€ å®‰å…¨å¯é çš„ä¸“å±å¤§æ¨¡å‹ï¼ŒåŸºäºå¤§è§„æ¨¡ä¸­è‹±æ–‡å¤šé¢†åŸŸæŒ‡ä»¤æ•°æ®è®­ç»ƒçš„ LlaMA 2 & BLOOM ç³»åˆ—æ¨¡å‹ï¼Œç”±ä¸­ç§‘é—»æ­Œç®—æ³•å›¢é˜Ÿç ”å‘ã€‚(Repo for YaYi Chinese LLMs based on LlaMA2 & BLOOM) |
| 2023-05-23 | [ExpertLLaMA](https://github.com/OFA-Sys/ExpertLLaMA) | 270 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | An opensource ChatBot built with ExpertPrompting which achieves 96% of ChatGPT's capability. |
| 2023-05-23 | [XrayGLM](https://github.com/WangRongsheng/XrayGLM) | 563 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ  | ğŸ©º é¦–ä¸ªä¼šçœ‹èƒ¸éƒ¨Xå…‰ç‰‡çš„ä¸­æ–‡å¤šæ¨¡æ€åŒ»å­¦å¤§æ¨¡å‹ \| The first Chinese Medical Multimodal Model that Chest Radiographs Summarization. |
| 2023-05-23 | [Macaw-LLM](https://github.com/lyuchenyang/Macaw-LLM) | 1145 | ğŸ”  ğŸ–¼ï¸ ğŸµ â›½ ğŸšŒ 2ï¸âƒ£  | Macaw-LLM: Multi-Modal Language Modeling with Image, Video, Audio, and Text Integration |
| 2023-05-22 | [seahorse](https://github.com/google-research-datasets/seahorse) | 62 | ğŸ”  â›½  | Seahorse is a dataset for multilingual, multi-faceted summarization evaluation. It consists of 96K summaries with human ratings along 6 quality dimensions: comprehensibility, repetition, grammar, attribution, main idea(s), and conciseness, covering 6 languages, 9 systems and 4 datasets. |
| 2023-05-19 | [gorilla](https://github.com/ShishirPatil/gorilla) | 7995 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ ğŸ”¨ ğŸ’° `Python`  | Gorilla: An API store for LLMs |
| 2023-05-18 | [Video-ChatGPT](https://github.com/mbzuai-oryx/Video-ChatGPT) | 481 | ğŸ”  ğŸ¥ â›½ ğŸšŒ 2ï¸âƒ£  | "Video-ChatGPT" is a video conversation model capable of generating meaningful conversation about videos. It combines the capabilities of LLMs with a pretrained visual encoder adapted for spatiotemporal video representation. We also introduce a rigorous 'Quantitative Evaluation Benchmarking' for video-based conversational models. |
| 2023-05-18 | [XrayGPT](https://github.com/mbzuai-oryx/XrayGPT) | 341 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸš• 2ï¸âƒ£  | XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models. |
| 2023-05-17 | [fastcomposer](https://github.com/mit-han-lab/fastcomposer) | 462 | ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention |
| 2023-05-16 | [SpeechGPT](https://github.com/0nutation/SpeechGPT) | 561 | ğŸ”  ğŸµ â›½ ğŸšŒ 2ï¸âƒ£  | SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities.  |
| 2023-05-16 | [bloomchat](https://github.com/sambanova/bloomchat) | 569 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | This repo contains the data preparation, tokenization, training and inference code for BLOOMChat. BLOOMChat is a 176 billion parameter multilingual chat model based on BLOOM. |
| 2023-05-15 | [safe-rlhf](https://github.com/PKU-Alignment/safe-rlhf) | 789 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ 3ï¸âƒ£  | Safe-RLHF: Constrained Value Alignment via Safe Reinforcement Learning from Human Feedback |
| 2023-05-12 | [ceval](https://github.com/SJTU-LIT/ceval) | 1000 | ğŸ”  â›½  | Official github repo for C-Eval, a Chinese evaluation suite for foundation models |
| 2023-05-12 | [TigerBot](https://github.com/TigerResearch/TigerBot) | 1729 | ğŸ”  â›½ ğŸšŒ 1ï¸âƒ£ 2ï¸âƒ£  | TigerBot: A multi-language multi-task LLM |
| 2023-05-10 | [BiLLa](https://github.com/Neutralzz/BiLLa) | 402 | ğŸ”  â›½ ğŸšŒ 1ï¸âƒ£ 2ï¸âƒ£ ğŸ€„  | BiLLa: A Bilingual LLaMA with Enhanced Reasoning Ability |
| 2023-05-04 | [Personalize-SAM](https://github.com/ZrrSkywalker/Personalize-SAM) | 1173 | ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | Personalize Segment Anything Model (SAM) with 1 shot in 10 seconds |
| 2023-05-04 | [WebCPM](https://github.com/thunlp/WebCPM) | 869 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | Official codes for ACL 2023 paper "WebCPM: Interactive Web Search for Chinese Long-form Question Answering" |
| 2023-05-03 | [alpaca_farm](https://github.com/tatsu-lab/alpaca_farm) | 520 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | A simulation framework for RLHF and alternatives. Develop your RLHF method without collecting human data.  |
| 2023-05-02 | [docta](https://github.com/Docta-ai/docta) | 1397 | ğŸ”  ğŸ–¼ï¸ â›½  | A Doctor for your data |
| 2023-05-02 | [CodeTF](https://github.com/salesforce/CodeTF) | 1318 | ğŸ”  â›½ ğŸš• 2ï¸âƒ£  | CodeTF: One-stop Transformer Library for State-of-the-art Code LLM |
| 2023-04-28 | [pandallm](https://github.com/dandelionsllm/pandallm) | 867 | ğŸ”  â›½ ğŸšŒ ğŸ€„  | Pandaé¡¹ç›®æ˜¯äº2023å¹´5æœˆå¯åŠ¨çš„å¼€æºæµ·å¤–ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹é¡¹ç›®ï¼Œè‡´åŠ›äºå¤§æ¨¡å‹æ—¶ä»£æ¢ç´¢æ•´ä¸ªæŠ€æœ¯æ ˆï¼Œæ—¨åœ¨æ¨åŠ¨ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„åˆ›æ–°å’Œåˆä½œã€‚ |
| 2023-04-28 | [datacomp](https://github.com/mlfoundations/datacomp) | 390 | ğŸ”  ğŸ–¼ï¸ â›½  | DataComp: In search of the next generation of multimodal datasets |
| 2023-04-28 | [ReplitLM](https://github.com/replit/ReplitLM) | 853 | ğŸ”  â›½ ğŸšŒ  | Inference code and configs for the ReplitLM model family |
| 2023-04-27 | [Prompt-Diffusion](https://github.com/Zhendong-Wang/Prompt-Diffusion) | 275 | ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | Official PyTorch implementation of the paper "In-Context Learning Unlocked for Diffusion Models" |
| 2023-04-26 | [Multimodal-GPT](https://github.com/open-mmlab/Multimodal-GPT) | 1184 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ  | Multimodal-GPT |
| 2023-04-25 | [mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl) | 1377 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | mPLUG-OwlğŸ¦‰: Modularization Empowers Large Language Models with Multimodality |
| 2023-04-24 | [starcoder](https://github.com/bigcode-project/starcoder) | 6210 | ğŸ”  â›½ ğŸš• 2ï¸âƒ£  | Home of StarCoder: fine-tuning & inference! |
| 2023-04-23 | [GPT4Tools](https://github.com/StevenGrove/GPT4Tools) | 579 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | GPT4Tools is an intelligent system that can automatically decide, control, and utilize different visual foundation models, allowing the user to interact with images during a conversation. |
| 2023-04-23 | [LaMini-LM](https://github.com/mbzuai-nlp/LaMini-LM) | 715 | ğŸ”  â›½ ğŸšŒ  | LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions |
| 2023-04-23 | [WizardLM](https://github.com/nlpxucan/WizardLM) | 6494 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | Family of instruction-following LLMs powered by Evol-Instruct: WizardLM, WizardCoder and WizardMath |
| 2023-04-21 | [law-cn-ai](https://github.com/lvwzhen/law-cn-ai) | 4447 | ğŸ”  â›½ ğŸ”¨ ğŸ€„ `MDX`  | âš–ï¸ AI æ³•å¾‹åŠ©æ‰‹ |
| 2023-04-21 | [awesome-chatgpt-dataset](https://github.com/voidful/awesome-chatgpt-dataset) | 524 | ğŸ”  â›½ ğŸ“  | Unlock the Power of LLM: Explore These Datasets to Train Your Own ChatGPT! |
| 2023-04-20 | [lamini](https://github.com/lamini-ai/lamini) | 2156 | ğŸ”  â›½  |  |
| 2023-04-20 | [LaWGPT](https://github.com/pengxiao-song/LaWGPT) | 5126 | ğŸ”  â›½ ğŸš• 1ï¸âƒ£ 2ï¸âƒ£  |  ğŸ‰ Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. åŸºäºä¸­æ–‡æ³•å¾‹çŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹ |
| 2023-04-17 | [h2o-llmstudio](https://github.com/h2oai/h2o-llmstudio) | 2588 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ ğŸ’°  | H2O LLM Studio - a framework and no-code GUI for fine-tuning LLMs. Documentation: https://h2oai.github.io/h2o-llmstudio/ |
| 2023-04-17 | [LLaVA](https://github.com/haotian-liu/LLaVA) | 4812 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ 1ï¸âƒ£ 2ï¸âƒ£  | Visual Instruction Tuning: Large Language-and-Vision Assistant built towards multimodal GPT-4 level capabilities. |
| 2023-04-15 | [MOSS](https://github.com/OpenLMLab/MOSS) | 11417 | ğŸ”  â›½ ğŸšŒ âœ‚ï¸ ğŸ’° ğŸ€„  | An open-source tool-augmented conversational language model from Fudan University |
| 2023-04-14 | [RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data) | 3428 | ğŸ”  â›½  | The RedPajama-Data repository contains code for preparing large datasets for training large language models. |
| 2023-04-13 | [HuatuoGPT](https://github.com/FreedomIntelligence/HuatuoGPT) | 632 | ğŸ”  â›½ ğŸš• 2ï¸âƒ£  | HuatuoGPT, Towards Taming Language Models To Be a Doctor. (An Open Medical GPT) |
| 2023-04-13 | [prm800k](https://github.com/openai/prm800k) | 953 | ğŸ”  â›½  | 800,000 step-level correctness labels on LLM solutions to MATH problems |
| 2023-04-12 | [dify](https://github.com/langgenius/dify) | 7962 | ğŸ”  â›½ 2ï¸âƒ£ ğŸ”¨ `Python`  | One API for plugins and datasets, one interface for prompt engineering and visual operation, all for creating powerful AI applications. |
| 2023-04-08 | [ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning) | 2956 | ğŸ”  â›½ 2ï¸âƒ£ 3ï¸âƒ£ ğŸ€„  | Fine-tuning ChatGLM-6B with PEFT \| åŸºäº PEFT çš„é«˜æ•ˆ ChatGLM å¾®è°ƒ |
| 2023-04-06 | [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | 3320 | ğŸ”  â›½ 2ï¸âƒ£  | Instruction Tuning with GPT-4 |
| 2023-04-02 | [Firefly](https://github.com/yangjianxin1/Firefly) | 2275 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ âœ‚ï¸ ğŸ€„  | Firefly(æµè¤): ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹(å…¨é‡å¾®è°ƒ+QLoRA)ï¼Œæ”¯æŒå¾®è°ƒLlma2ã€Llamaã€Qwenã€Baichuanã€ChatGLM2ã€InternLMã€Ziyaã€Bloomç­‰å¤§æ¨¡å‹ |
| 2023-04-01 | [LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) | 2607 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ âœ‚ï¸ ğŸ“  | âš¡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.âš¡ |
| 2023-04-01 | [Otter](https://github.com/Luodian/Otter) | 2360 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | ğŸ¦¦ Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following and in-context learning ability. |
| 2023-03-31 | [Chinese-alpaca-lora](https://github.com/LC1332/Chinese-alpaca-lora) | 643 | ğŸ”  â›½ ğŸšŒ ğŸ€„  | éª†é©¼:A Chinese finetuned instruction LLaMA. Developed by é™ˆå¯æº @ åä¸­å¸ˆèŒƒå¤§å­¦ & æé²é² @ å•†æ±¤ç§‘æŠ€ & å†·å­æ˜‚ @ å•†æ±¤ç§‘æŠ€ |
| 2023-03-31 | [Huatuo-Llama-Med-Chinese](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) | 3602 | ğŸ”  â›½ ğŸš• 2ï¸âƒ£ ğŸ€„  | Repo for BenTsao [original name: HuaTuo (åé©¼)], Instruction-tuning Large Language Models with Chinese Medical Knowledge. æœ¬è‰ï¼ˆåŸåï¼šåé©¼ï¼‰æ¨¡å‹ä»“åº“ï¼ŒåŸºäºä¸­æ–‡åŒ»å­¦çŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹æŒ‡ä»¤å¾®è°ƒ |
| 2023-03-31 | [Med-ChatGLM](https://github.com/SCIR-HI/Med-ChatGLM) | 759 | ğŸ”  â›½ ğŸš• 2ï¸âƒ£  | Repo for Chinese Medical ChatGLM åŸºäºä¸­æ–‡åŒ»å­¦çŸ¥è¯†çš„ChatGLMæŒ‡ä»¤å¾®è°ƒ |
| 2023-03-31 | [baize-chatbot](https://github.com/project-baize/baize-chatbot) | 3015 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | Let ChatGPT teach your own chatbot in hours with a single GPU! |
| 2023-03-29 | [LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters) | 704 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models |
| 2023-03-29 | [awesome-instruction-dataset](https://github.com/yaodongC/awesome-instruction-dataset) | 754 | ğŸ”  â›½  | A collection of open-source dataset to train instruction-following LLMs (ChatGPT,LLaMA,Alpaca) |
| 2023-03-27 | [LMFlow](https://github.com/OptimalScale/LMFlow) | 7143 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ 3ï¸âƒ£  | An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All. |
| 2023-03-24 | [Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT) | 2037 | ğŸ”  â›½ 2ï¸âƒ£  | We unified the interfaces of instruction-tuning data (e.g., CoT data), multiple LLMs and parameter-efficient methods (e.g., lora, p-tuning) together for easy use. Meanwhile, we created a new branch to build a Tabular LLM.ï¼ˆæˆ‘ä»¬åˆ†åˆ«ç»Ÿä¸€äº†ä¸°å¯Œçš„IFTæ•°æ®ï¼ˆå¦‚CoTæ•°æ®ï¼Œç›®å‰ä»ä¸æ–­æ‰©å……ï¼‰ã€å¤šç§è®­ç»ƒæ•ˆç‡æ–¹æ³•ï¼ˆå¦‚loraï¼Œp-tuningï¼‰ä»¥åŠå¤šç§LLMsï¼Œä¸‰ä¸ªå±‚é¢ä¸Šçš„æ¥å£ï¼Œæ‰“é€ æ–¹ä¾¿ç ”ç©¶äººå‘˜ä¸Šæ‰‹çš„LLM-IFTç ”ç©¶å¹³å°ã€‚åŒæ—¶tabular_llmåˆ†æ”¯æ„å»ºäº†é¢å‘è¡¨æ ¼æ™ºèƒ½ä»»åŠ¡çš„LLMã€‚ |
| 2023-03-24 | [dolly](https://github.com/databrickslabs/dolly) | 10556 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ ğŸ’°  | Databricksâ€™ Dolly, a large language model trained on the Databricks Machine Learning Platform |
| 2023-03-23 | [InstructGLM](https://github.com/yanqiangmiffy/InstructGLM) | 605 | ğŸ”  â›½ 2ï¸âƒ£  | ChatGLM-6B æŒ‡ä»¤å­¦ä¹ \|æŒ‡ä»¤æ•°æ®\|Instruct |
| 2023-03-22 | [codealpaca](https://github.com/sahil280114/codealpaca) | 1229 | ğŸ”  â›½ 2ï¸âƒ£  | This is the repo for the Code Alpaca project, which aims to build and share an instruction-following LLaMA model for code generation. This repo is fully based on Stanford Alpaca ,and only changes the data used for training. Training approach is the same. |
| 2023-03-21 | [Linly](https://github.com/CVI-SZU/Linly) | 2677 | ğŸ”  â›½ ğŸšŒ 1ï¸âƒ£ 2ï¸âƒ£ 3ï¸âƒ£ âœ‚ï¸ ğŸ’¡ ğŸ’° ğŸ€„  | Chinese-LLaMA 1&2ã€Chinese-Falcon åŸºç¡€æ¨¡å‹ï¼›ChatFlowä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼›ä¸­æ–‡OpenLLaMAæ¨¡å‹ï¼›NLPé¢„è®­ç»ƒ/æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† |
| 2023-03-21 | [ChatDoctor](https://github.com/Kent0n-Li/ChatDoctor) | 3079 | ğŸ”  â›½ ğŸš• 2ï¸âƒ£  |  |
| 2023-03-21 | [AlpacaDataCleaned](https://github.com/gururise/AlpacaDataCleaned) | 1222 | ğŸ”  â›½  | Alpaca dataset from Stanford, cleaned and curated |
| 2023-03-21 | [DreamPose](https://github.com/johannakarras/DreamPose) | 668 | ğŸ¥ â›½ ğŸšŒ 2ï¸âƒ£  | Official implementation of "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion" |
| 2023-03-19 | [LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter) | 4720 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters |
| 2023-03-18 | [KoAlpaca](https://github.com/Beomi/KoAlpaca) | 1287 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | KoAlpaca: í•œêµ­ì–´ ëª…ë ¹ì–´ë¥¼ ì´í•´í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ì–¸ì–´ëª¨ë¸ |
| 2023-03-17 | [BELLE](https://github.com/LianjiaTech/BELLE) | 6629 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£ âœ‚ï¸ ğŸ€„  | BELLE: Be Everyone's Large Language model Engineï¼ˆå¼€æºä¸­æ–‡å¯¹è¯å¤§æ¨¡å‹ï¼‰ |
| 2023-03-17 | [alpaca_chinese_dataset](https://github.com/hikariming/alpaca_chinese_dataset) | 937 | ğŸ”  â›½ 2ï¸âƒ£ ğŸ€„  | äººå·¥ç²¾è°ƒçš„ä¸­æ–‡å¯¹è¯æ•°æ®é›†å’Œä¸€æ®µchatglmçš„å¾®è°ƒä»£ç  |
| 2023-03-16 | [FateZero](https://github.com/ChenyangQiQi/FateZero) | 844 | ğŸ”  ğŸ¥ â›½ ğŸšŒ 2ï¸âƒ£  | [ICCV 2023 Oral] "FateZero: Fusing Attentions for Zero-shot Text-based Video Editing" |
| 2023-03-10 | [stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca) | 26541 | ğŸ”  â›½ ğŸšŒ 2ï¸âƒ£  | Code and documentation to train Stanford's Alpaca models, and generate the data. |
| 2023-03-04 | [DDSP-SVC](https://github.com/yxlllc/DDSP-SVC) | 1065 | ğŸµ â›½ ğŸšŒ 2ï¸âƒ£  | Real-time end-to-end singing voice conversion system based on DDSP (Differentiable Digital Signal Processing) |
| 2023-03-03 | [OpenChatKit](https://github.com/togethercomputer/OpenChatKit) | 8859 | ğŸ”  â›½ ğŸšŒ 1ï¸âƒ£ 2ï¸âƒ£  |  |
| 2023-02-11 | [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT) | 8146 | ğŸ”  â›½ ğŸš• ğŸ€„  | Data-Centric FinGPT.  Open-source for open finance!  Revolutionize ğŸ”¥    We release the trained model on HuggingFace. |
| 2023-01-23 | [evals](https://github.com/openai/evals) | 11629 | ğŸ”  â›½  | Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks. |
| 2023-01-09 | [instruct-pix2pix](https://github.com/timothybrooks/instruct-pix2pix) | 5157 | ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  | PyTorch implementation of InstructPix2Pix, an instruction-based image editing model |
| 2022-12-28 | [nanoGPT](https://github.com/karpathy/nanoGPT) | 24287 | ğŸ”  â›½ ğŸšŒ 1ï¸âƒ£ 2ï¸âƒ£  | The simplest, fastest repository for training/finetuning medium-sized GPTs. |
| 2022-12-20 | [self-instruct](https://github.com/yizhongw/self-instruct) | 2995 | ğŸ”  â›½  | Aligning pretrained language models with instruction data generated by themselves. |
| 2022-11-23 | [stablediffusion](https://github.com/Stability-AI/stablediffusion) | 29342 | ğŸ–¼ï¸ â›½ ğŸšŒ  | High-Resolution Image Synthesis with Latent Diffusion Models |
| 2022-10-20 | [open_flamingo](https://github.com/mlfoundations/open_flamingo) | 2804 | ğŸ”  ğŸ–¼ï¸ ğŸ¥ â›½ ğŸšŒ 2ï¸âƒ£  | An open-source framework for training large multimodal models. |
| 2022-09-29 | [motion-diffusion-model](https://github.com/GuyTevet/motion-diffusion-model) | 2370 | ğŸ¥ â›½ ğŸšŒ 2ï¸âƒ£  | The official PyTorch implementation of the paper "Human Motion Diffusion Model" |
| 2022-08-24 | [LAVIS](https://github.com/salesforce/LAVIS) | 6494 | ğŸ”  ğŸ–¼ï¸ â›½ ğŸšŒ 1ï¸âƒ£ 2ï¸âƒ£  | LAVIS - A One-stop Library for Language-Vision Intelligence |
| 2022-08-10 | [stable-diffusion](https://github.com/CompVis/stable-diffusion) | 59499 | ğŸ–¼ï¸ â›½ ğŸšŒ  | A latent text-to-image diffusion model |
| 2022-08-02 | [textual_inversion](https://github.com/rinongal/textual_inversion) | 2569 | ğŸ–¼ï¸ â›½ ğŸšŒ 2ï¸âƒ£  |  |
