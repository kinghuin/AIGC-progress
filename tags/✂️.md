| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-07-20 | [tiny-stable-diffusion](https://github.com/ThisisBillhe/tiny-stable-diffusion) | 95 | 🔠 🖼️ ✂️  | Tiny optimized Stable-diffusion that can run on GPUs with just 1GB of VRAM. (Beta) |
| 2023-07-18 | [LLM-QAT](https://github.com/facebookresearch/LLM-QAT) | 96 | 🔠 ✂️  | Code repo for the paper "LLM-QAT Data-Free Quantization Aware Training for Large Language Models" |
| 2023-07-10 | [Baichuan-13B](https://github.com/baichuan-inc/Baichuan-13B) | 2478 | 🔠 🚌 ✂️  | A 13B large language model developed by Baichuan Intelligent Technology |
| 2023-07-04 | [Finetune-ChatGLM2-6B](https://github.com/SpongebBob/Finetune-ChatGLM2-6B) | 277 | 🔠 ⛽ 2️⃣ ✂️  | ChatGLM2-6B 全参数微调，支持多轮对话的高效微调。 |
| 2023-06-24 | [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) | 12135 | 🔠 🚌 ✂️ 🀄  | ChatGLM2-6B: An Open Bilingual Chat LLM \| 开源双语对话语言模型 |
| 2023-06-15 | [lmdeploy](https://github.com/InternLM/lmdeploy) | 703 | 🔠 ✂️ 💡  | LMDeploy is a toolkit for compressing, deploying, and serving LLMs. |
| 2023-06-12 | [SqueezeLLM](https://github.com/SqueezeAILab/SqueezeLLM) | 375 | 🔠 ✂️  | SqueezeLLM: Dense-and-Sparse Quantization |
| 2023-06-06 | [wanda](https://github.com/locuslab/wanda) | 310 | 🔠 ✂️  | A simple and effective LLM pruning approach. |
| 2023-06-05 | [YuLan-Chat](https://github.com/RUC-GSAI/YuLan-Chat) | 280 | 🔠 🚌 ✂️  | YuLan-Chat: An Open-Source Bilingual Chatbot |
| 2023-06-05 | [SpQR](https://github.com/Vahe1994/SpQR) | 439 | 🔠 ✂️  | This repository contains quantization algorithm and the model evaluation code for SpQR method for LLM compression |
| 2023-06-01 | [llm-awq](https://github.com/mit-han-lab/llm-awq) | 696 | 🔠 ✂️  | AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration |
| 2023-05-28 | [falcontune](https://github.com/rmihaylov/falcontune) | 456 | 🔠 ✂️  | Tune any FALCON in 4-bit |
| 2023-05-11 | [qlora](https://github.com/artidoro/qlora) | 7361 | 🔠 2️⃣ ✂️  | QLoRA: Efficient Finetuning of Quantized LLMs |
| 2023-05-04 | [exllama](https://github.com/turboderp/exllama) | 1853 | 🔠 ✂️  | A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights. |
| 2023-04-23 | [VisualGLM-6B](https://github.com/THUDM/VisualGLM-6B) | 3295 | 🔠 🖼️ 🚌 2️⃣ ✂️  | Chinese and English multimodal conversational language model \| 多模态中英双语对话语言模型 |
| 2023-04-19 | [MiniGPT-4-ZH](https://github.com/RiseInRose/MiniGPT-4-ZH) | 752 | 🔠 🖼️ 🚌 2️⃣ ✂️  | MiniGPT-4 中文部署翻译 完善部署细节 |
| 2023-04-15 | [MOSS](https://github.com/OpenLMLab/MOSS) | 11428 | 🔠 ⛽ 🚌 ✂️ 💰 🀄  | An open-source tool-augmented conversational language model from Fudan University |
| 2023-04-13 | [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) | 1965 | 🔠 ✂️  | An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm. |
| 2023-04-02 | [Firefly](https://github.com/yangjianxin1/Firefly) | 2306 | 🔠 ⛽ 🚌 2️⃣ ✂️ 🀄  | Firefly(流萤): 中文对话式大语言模型(全量微调+QLoRA)，支持微调Llma2、Llama、Qwen、Baichuan、ChatGLM2、InternLM、Ziya、Bloom等大模型 |
| 2023-04-01 | [LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) | 2610 | 🔠 ⛽ 🚌 2️⃣ ✂️ 📝  | ⚡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.⚡ |
| 2023-03-23 | [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna) | 3976 | 🔠 🚌 2️⃣ ✂️ 🀄  | Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model —— 一个中文低资源的llama+lora方案，结构参考alpaca |
| 2023-03-22 | [lit-llama](https://github.com/Lightning-AI/lit-llama) | 5132 | 🔠 🚌 1️⃣ 2️⃣ ✂️ 💰  | Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed. |
| 2023-03-21 | [Linly](https://github.com/CVI-SZU/Linly) | 2681 | 🔠 ⛽ 🚌 1️⃣ 2️⃣ 3️⃣ ✂️ 💡 💰 🀄  | Chinese-LLaMA 1&2、Chinese-Falcon 基础模型；ChatFlow中文对话模型；中文OpenLLaMA模型；NLP预训练/指令微调数据集 |
| 2023-03-17 | [BELLE](https://github.com/LianjiaTech/BELLE) | 6638 | 🔠 ⛽ 🚌 2️⃣ ✂️ 🀄  | BELLE: Be Everyone's Large Language model Engine（开源中文对话大模型） |
| 2023-03-15 | [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | 14317 | 🔠 1️⃣ 2️⃣ ✂️ 💡 🀄  | 中文LLaMA&Alpaca大语言模型+本地CPU/GPU训练部署 (Chinese LLaMA & Alpaca LLMs) |
| 2023-03-13 | [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) | 33818 | 🔠 🚌 2️⃣ ✂️ 🀄  | ChatGLM-6B: An Open Bilingual Dialogue Language Model \| 开源双语对话语言模型 |
| 2023-03-12 | [bloomz.cpp](https://github.com/NouamaneTazi/bloomz.cpp) | 791 | 🔠 ✂️ 💡  | C++ implementation for BLOOM |
| 2023-03-10 | [llama.cpp](https://github.com/ggerganov/llama.cpp) | 39796 | 🔠 ✂️ 💡  | Port of Facebook's LLaMA model in C/C++ |
| 2023-03-06 | [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa) | 2560 | 🔠 ✂️  | 4 bits quantization of LLaMA using GPTQ |
| 2023-02-28 | [pyllama](https://github.com/juncongmoo/pyllama) | 2591 | 🔠 ✂️  | LLaMA: Open and Efficient Foundation Language Models |
| 2023-02-26 | [consistency_models](https://github.com/openai/consistency_models) | 5395 | 🖼️ 🚌 2️⃣ ✂️  | Official repo for consistency models. |
| 2023-01-27 | [musiclm-pytorch](https://github.com/lucidrains/musiclm-pytorch) | 2680 | 🎵 🚌 2️⃣ ✂️  | Implementation of MusicLM, Google's new SOTA model for music generation using attention networks, in Pytorch |
| 2022-10-08 | [text-generation-inference](https://github.com/huggingface/text-generation-inference) | 4732 | 🔠 ✂️ 💡  | Large Language Model Text Generation Inference |
| 2022-09-25 | [whisper.cpp](https://github.com/ggerganov/whisper.cpp) | 22609 | 🎵 🚌 ✂️  | Port of OpenAI's Whisper model in C/C++ |
