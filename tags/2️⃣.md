| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-08-30 | [facechain-colab](https://github.com/camenduru/facechain-colab) | 31 | 🖼️ 2️⃣  |  |
| 2023-08-29 | [AnomalyGPT](https://github.com/CASIA-IVA-Lab/AnomalyGPT) | 149 | 🔠 🖼️ ⛽ 🚌 2️⃣  | The first LVLM based IAD method! |
| 2023-08-29 | [CoVR](https://github.com/lucas-ventura/CoVR) | 25 | 🎥 ⛽ 🚌 2️⃣  | Official PyTorch implementation of the paper "CoVR: Learning Composed Video Retrieval from Web Video Captions". |
| 2023-08-28 | [DiffBIR](https://github.com/XPixelGroup/DiffBIR) | 59 | 🖼️ ⛽ 🚌 2️⃣  |  |
| 2023-08-24 | [codellama](https://github.com/facebookresearch/codellama) | 8033 | 🔠 🚕 2️⃣  | Inference code for CodeLlama models |
| 2023-08-19 | [DiffusionTrack](https://github.com/RainBowLuoCS/DiffusionTrack) | 112 | 🖼️ ⛽ 2️⃣  |  |
| 2023-08-18 | [meru](https://github.com/facebookresearch/meru) | 73 | 🖼️ ⛽ 🚌 2️⃣  | Code for the paper "Hyperbolic Image-Text Representations", Desai et al, ICML 2023 |
| 2023-08-17 | [wisdomInterrogatory](https://github.com/zhihaiLLM/wisdomInterrogatory) | 254 | 🔠 🚕 2️⃣  |  |
| 2023-08-16 | [gpt-oracle-trainer](https://github.com/mshumer/gpt-oracle-trainer) | 296 | 🔠 2️⃣  | Creating a chatbot that can accurately answer questions about a product or service's documentation |
| 2023-08-16 | [modal_finetune_sql](https://github.com/run-llama/modal_finetune_sql) | 151 | 🔠 2️⃣  |  |
| 2023-08-15 | [CoDeF](https://github.com/qiuyu96/CoDeF) | 3669 | 🎥 ⛽ 🚌 2️⃣  | Official PyTorch implementation of CoDeF: Content Deformation Fields for Temporally Consistent Video Processing |
| 2023-08-12 | [baby-llama2-chinese](https://github.com/DLLXW/baby-llama2-chinese) | 572 | 🔠 2️⃣  | 用于从头预训练+SFT一个小参数量的中文LLaMa2的仓库；24G单卡即可运行得到一个具备简单中文问答能力的chat-llama2. |
| 2023-08-09 | [gpt-llm-trainer](https://github.com/mshumer/gpt-llm-trainer) | 2227 | 🔠 2️⃣  |  |
| 2023-08-01 | [QiaoBan](https://github.com/HIT-SCIR-SC/QiaoBan) | 78 | 🔠 ⛽ 🚕 2️⃣  | 这是一个巧板大模型的仓库，旨在构建一个面向儿童情感陪伴的大模型 |
| 2023-07-30 | [OpenLLaMA2](https://github.com/OpenLLMAI/OpenLLaMA2) | 97 | 🔠 2️⃣ 3️⃣  | DeepSpeed+Ray based LLaMA2 RLHF/RS training framework |
| 2023-07-28 | [Megatron-LLM](https://github.com/epfLLM/Megatron-LLM) | 172 | 🔠 2️⃣  | distributed trainer for LLMs |
| 2023-07-26 | [DWPose](https://github.com/IDEA-Research/DWPose) | 1063 | 🎥 🚌 2️⃣  | "Effective Whole-body Pose Estimation with Two-stages Distillation" (ICCV 2023, CV4Metaverse Workshop) |
| 2023-07-21 | [LLaMA2-Accessory](https://github.com/Alpha-VLLM/LLaMA2-Accessory) | 1449 | 🔠 🖼️ ⛽ 1️⃣ 2️⃣ 🔨 `Python`  | An Open-source Toolkit for LLM Development |
| 2023-07-21 | [Subject-Diffusion](https://github.com/OPPO-Mente-Lab/Subject-Diffusion) | 159 | 🖼️ ⛽ 🚌 2️⃣  | Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning |
| 2023-07-21 | [distill-sd](https://github.com/segmind/distill-sd) | 303 | 🖼️ 🚌 2️⃣  | Segmind Distilled diffusion |
| 2023-07-20 | [Chinese-Llama-2-7b](https://github.com/LinkSoul-AI/Chinese-Llama-2-7b) | 1778 | 🔠 ⛽ 🚌 2️⃣ 💰 🀄  | 开源社区第一个能下载、能运行的中文 LLaMA2 模型！ |
| 2023-07-20 | [TokenFlow](https://github.com/omerbt/TokenFlow) | 597 | 🎥 2️⃣  | Official Pytorch Implementation for "TokenFlow: Consistent Diffusion Features for Consistent Video Editing" presenting "TokenFlow" |
| 2023-07-19 | [Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese) | 4757 | 🔠 ⛽ 🚌 2️⃣ 💰 🀄  | Llama中文社区，最好的中文Llama大模型，完全开源可商用 |
| 2023-07-19 | [Chinese-LlaMA2](https://github.com/michael-wzhu/Chinese-LlaMA2) | 698 | 🔠 🚌 🚕 1️⃣ 2️⃣ 💰 🀄  | Repo for adapting Meta LlaMA2 in Chinese! META最新发布的LlaMA2的汉化版！ （完全开源可商用） |
| 2023-07-19 | [Firefly-LLaMA2-Chinese](https://github.com/yangjianxin1/Firefly-LLaMA2-Chinese) | 115 | 🔠 🚌 🚕 1️⃣ 2️⃣ 🀄  | 中文LLaMA-2大模型，兼容对中文大模型进行增量预训练 |
| 2023-07-18 | [Chinese-Llama-2](https://github.com/longyuewangdcu/Chinese-Llama-2) | 275 | 🔠 🚌 1️⃣ 2️⃣  | Chinese-Llama-2 is a project that aims to expand the impressive capabilities of the Llama-2 language model to the Chinese language. |
| 2023-07-18 | [Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) | 2971 | 🔠 🚌 1️⃣ 2️⃣ 💰 🀄  | 中文LLaMA-2 & Alpaca-2大模型二期项目 + 16K超长上下文模型 (Chinese LLaMA-2 & Alpaca-2 LLMs, including 16K long context models) |
| 2023-07-17 | [TransGPT](https://github.com/DUOMO/TransGPT) | 445 | 🔠 ⛽ 🚕 2️⃣ 💰 🀄  | TransGPT是国内首款开源交通大模型，主要致力于在真实交通行业中发挥实际价值。 |
| 2023-07-17 | [ER-NeRF](https://github.com/Fictionarry/ER-NeRF) | 151 | 🎥 ⛽ 🚌 2️⃣  | [ICCV'23] Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis |
| 2023-07-17 | [llama-recipes](https://github.com/facebookresearch/llama-recipes) | 3515 | 🔠 🚌 2️⃣  | Examples and recipes for Llama 2 model |
| 2023-07-13 | [lynx-llm](https://github.com/bytedance/lynx-llm) | 176 | 🔠 🖼️ 2️⃣ ❓  | paper: https://arxiv.org/abs/2307.02469 page: https://lynx-llm.github.io/ |
| 2023-07-13 | [llm-engine](https://github.com/scaleapi/llm-engine) | 548 | 🔠 2️⃣  | Scale LLM Engine public repository |
| 2023-07-08 | [DragDiffusion](https://github.com/Yujun-Shi/DragDiffusion) | 597 | 🖼️ 🚌 2️⃣  | Official code for DragDiffusion |
| 2023-07-06 | [InternLM](https://github.com/InternLM/InternLM) | 2612 | 🔠 🚌 2️⃣  | InternLM has open-sourced a 7 billion parameter base model, a chat model tailored for practical scenarios and the training system. |
| 2023-07-06 | [GPT4RoI](https://github.com/jshilong/GPT4RoI) | 316 | 🔠 🖼️ ⛽ 🚌 2️⃣  | GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest |
| 2023-07-04 | [Finetune-ChatGLM2-6B](https://github.com/SpongebBob/Finetune-ChatGLM2-6B) | 277 | 🔠 ⛽ 2️⃣ ✂️  | ChatGLM2-6B 全参数微调，支持多轮对话的高效微调。 |
| 2023-07-04 | [text2cinemagraph](https://github.com/text2cinemagraph/text2cinemagraph) | 267 | 🔠 🎥 🚌 2️⃣  | Official Pytorch implementation of Text2Cinemagraph: Synthesizing Artistic Cinemagraphs from Text |
| 2023-07-04 | [StyleDrop-PyTorch](https://github.com/zideliu/StyleDrop-PyTorch) | 443 | 🖼️ ⛽ 🚌 2️⃣  | Unoffical implement for [StyleDrop](https://arxiv.org/abs/2306.00983) |
| 2023-06-30 | [VisCPM](https://github.com/OpenBMB/VisCPM) | 792 | 🔠 🖼️ 🚌 2️⃣ 🀄  | Chinese and English Multimodal Large Model Series (Chat and Paint) \| 基于CPM基础模型的中英双语多模态大模型系列 |
| 2023-06-29 | [MediaGPT](https://github.com/IMOSR/MediaGPT) | 467 | 🔠 ⛽ 🚕 2️⃣ 🀄  | 中文的自媒体大语言模型MediaGPT(曾用名Media LLaMA) |
| 2023-06-28 | [UnitSpeech](https://github.com/gmltmd789/UnitSpeech) | 88 | 🎵 🚌 2️⃣  | An official implementation of "UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data" |
| 2023-06-28 | [cog-llama-template](https://github.com/replicate/cog-llama-template) | 238 | 🔠 2️⃣ ❓  | LLaMA Cog template |
| 2023-06-27 | [LLaVAR](https://github.com/SALT-NLP/LLaVAR) | 131 | 🔠 🖼️ ⛽ 🚌 2️⃣  | Code/Data for the paper: "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding" |
| 2023-06-24 | [train_custom_LLM](https://github.com/EvilPsyCHo/train_custom_LLM) | 170 | 🔠 2️⃣  | Train your custom LLMs like Llama, baichuan-7b, GPT |
| 2023-06-22 | [generative-models](https://github.com/Stability-AI/generative-models) | 8883 | 🖼️ ⛽ 🚌 2️⃣  | Generative Models by Stability AI |
| 2023-06-22 | [direct-preference-optimization](https://github.com/eric-mitchell/direct-preference-optimization) | 422 | 🔠 2️⃣ 3️⃣  | Reference implementation for DPO (Direct Preference Optimization) |
| 2023-06-20 | [OpenPipe](https://github.com/OpenPipe/OpenPipe) | 547 | 🔠 2️⃣  | Turn expensive prompts into cheap fine-tuned models |
| 2023-06-19 | [LLM-Tuning](https://github.com/beyondguo/LLM-Tuning) | 668 | 🔠 2️⃣  | Tuning LLMs with no tears💦, sharing LLM-tools with love❤️. |
| 2023-06-16 | [Meta-voicebox](https://github.com/SpeechifyInc/Meta-voicebox) | 447 | 🎵 2️⃣  | Implementation of Meta-Voicebox : The first generative AI model for speech to generalize across tasks with state-of-the-art performance. |
| 2023-06-13 | [PanoHead](https://github.com/SizheAn/PanoHead) | 1530 | 🖼️ 🚌 2️⃣ 🧊  | Code Repository for CVPR 2023 Paper "PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360 degree" |
| 2023-06-12 | [Anima](https://github.com/lyogavin/Anima) | 1119 | 🔠 ⛽ 🚌 2️⃣ 🀄  | 第一个开源的基于QLoRA的33B中文大语言模型First QLoRA based open source 33B Chinese LLM |
| 2023-06-09 | [open-instruct](https://github.com/allenai/open-instruct) | 407 | 🔠 ⛽ 🚌 2️⃣  | We explore instruction-tuning popular base models on publicly available datasets. |
| 2023-06-08 | [audiocraft](https://github.com/facebookresearch/audiocraft) | 15755 | 🎵 🚌 2️⃣  | Audiocraft is a library for audio processing and generation with deep learning. It features the state-of-the-art EnCodec audio compressor / tokenizer, along with MusicGen, a simple and controllable music generation LM with textual and melodic conditioning. |
| 2023-06-05 | [HeadSculpt](https://github.com/BrandonHanx/HeadSculpt) | 91 | 🔠 2️⃣ 🧊  | [arXiv 2023 WIP] HeadSculpt: Crafting 3D Head Avatars with Text |
| 2023-06-03 | [DisCo](https://github.com/Wangt-CN/DisCo) | 569 | 🎥 🚌 2️⃣  | DisCo: Referring Human Dance Generation in Real World |
| 2023-06-02 | [MedicalGPT](https://github.com/shibing624/MedicalGPT) | 1397 | 🔠 ⛽ 🚕 1️⃣ 2️⃣ 3️⃣ 🀄  | MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. 训练医疗大模型，实现了包括增量预训练、有监督微调、RLHF(奖励建模、强化学习训练)和DPO(直接偏好优化)。 |
| 2023-05-31 | [LLM-Blender](https://github.com/yuchenlin/LLM-Blender) | 505 | 🔠 ⛽ 🚌 2️⃣  | [ACL2023] We introduce LLM-Blender, an innovative ensembling framework to attain consistently superior performance by leveraging the diverse strengths of multiple open-source LLMs. LLM-Blender cut the weaknesses through ranking and integrate the strengths through fusing generation to enhance the capability of LLMs. |
| 2023-05-28 | [WebGLM](https://github.com/THUDM/WebGLM) | 1288 | 🔠 ⛽ 🚌 2️⃣ 🔨 🀄 `Python`  | WebGLM: An Efficient Web-enhanced Question Answering System (KDD 2023) |
| 2023-05-28 | [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) | 3516 | 🔠 1️⃣ 2️⃣ 3️⃣  | Easy-to-use LLM fine-tuning framework (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, ChatGLM2) |
| 2023-05-25 | [openchat](https://github.com/imoneoi/openchat) | 1590 | 🔠 ⛽ 🚌 🚕 2️⃣  | OpenChat: Advancing Open-source Language Models with Imperfect Data |
| 2023-05-24 | [SAIL](https://github.com/luohongyin/SAIL) | 138 | 🔠 ⛽ 🚌 2️⃣  | SAIL: Search Augmented Instruction Learning |
| 2023-05-24 | [YaYi](https://github.com/wenge-research/YaYi) | 1601 | 🔠 ⛽ 🚕 2️⃣  | 雅意大模型：为客户打造安全可靠的专属大模型，基于大规模中英文多领域指令数据训练的 LlaMA 2 & BLOOM 系列模型，由中科闻歌算法团队研发。(Repo for YaYi Chinese LLMs based on LlaMA2 & BLOOM) |
| 2023-05-23 | [ExpertLLaMA](https://github.com/OFA-Sys/ExpertLLaMA) | 270 | 🔠 ⛽ 🚌 2️⃣  | An opensource ChatBot built with ExpertPrompting which achieves 96% of ChatGPT's capability. |
| 2023-05-23 | [Macaw-LLM](https://github.com/lyuchenyang/Macaw-LLM) | 1145 | 🔠 🖼️ 🎵 ⛽ 🚌 2️⃣  | Macaw-LLM: Multi-Modal Language Modeling with Image, Video, Audio, and Text Integration |
| 2023-05-19 | [gorilla](https://github.com/ShishirPatil/gorilla) | 8028 | 🔠 ⛽ 🚌 2️⃣ 🔨 💰 `Python`  | Gorilla: An API store for LLMs |
| 2023-05-19 | [ltu](https://github.com/YuanGongND/ltu) | 99 | 🔠 🎵 2️⃣  | Github Repo for Paper "Listen, Think, and Understand". |
| 2023-05-18 | [ONE-PEACE](https://github.com/OFA-Sys/ONE-PEACE) | 583 | 🖼️ 🚌 1️⃣ 2️⃣  | A general representation model across vision, audio, language modalities. Paper: ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities |
| 2023-05-18 | [VisionLLM](https://github.com/OpenGVLab/VisionLLM) | 436 | 🖼️ 🚌 2️⃣  | VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks |
| 2023-05-18 | [Video-ChatGPT](https://github.com/mbzuai-oryx/Video-ChatGPT) | 483 | 🔠 🎥 ⛽ 🚌 2️⃣  | "Video-ChatGPT" is a video conversation model capable of generating meaningful conversation about videos. It combines the capabilities of LLMs with a pretrained visual encoder adapted for spatiotemporal video representation. We also introduce a rigorous 'Quantitative Evaluation Benchmarking' for video-based conversational models. |
| 2023-05-18 | [XrayGPT](https://github.com/mbzuai-oryx/XrayGPT) | 341 | 🔠 🖼️ ⛽ 🚕 2️⃣  | XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models. |
| 2023-05-18 | [PandaGPT](https://github.com/yxuansu/PandaGPT) | 601 | 🔠 🖼️ 🚌 2️⃣  | PandaGPT: One Model To Instruction-Follow Them All |
| 2023-05-17 | [soundstorm-pytorch](https://github.com/lucidrains/soundstorm-pytorch) | 997 | 🎵 2️⃣  | Implementation of SoundStorm, Efficient Parallel Audio Generation from Google Deepmind, in Pytorch |
| 2023-05-17 | [fastcomposer](https://github.com/mit-han-lab/fastcomposer) | 465 | 🖼️ ⛽ 🚌 2️⃣  | FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention |
| 2023-05-17 | [tree-of-thought-llm](https://github.com/princeton-nlp/tree-of-thought-llm) | 3095 | 🔠 2️⃣  | Official Implementation of "Tree of Thoughts: Deliberate Problem Solving with Large Language Models" |
| 2023-05-16 | [SpeechGPT](https://github.com/0nutation/SpeechGPT) | 562 | 🔠 🎵 ⛽ 🚌 2️⃣  | SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities.  |
| 2023-05-16 | [bloomchat](https://github.com/sambanova/bloomchat) | 569 | 🔠 ⛽ 🚌 2️⃣  | This repo contains the data preparation, tokenization, training and inference code for BLOOMChat. BLOOMChat is a 176 billion parameter multilingual chat model based on BLOOM. |
| 2023-05-15 | [safe-rlhf](https://github.com/PKU-Alignment/safe-rlhf) | 793 | 🔠 ⛽ 🚌 2️⃣ 3️⃣  | Safe-RLHF: Constrained Value Alignment via Safe Reinforcement Learning from Human Feedback |
| 2023-05-13 | [Make-A-Protagonist](https://github.com/HeliosZhao/Make-A-Protagonist) | 270 | 🎥 🚌 2️⃣  | Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts |
| 2023-05-12 | [TigerBot](https://github.com/TigerResearch/TigerBot) | 1738 | 🔠 ⛽ 🚌 1️⃣ 2️⃣  | TigerBot: A multi-language multi-task LLM |
| 2023-05-11 | [qlora](https://github.com/artidoro/qlora) | 7361 | 🔠 2️⃣ ✂️  | QLoRA: Efficient Finetuning of Quantized LLMs |
| 2023-05-11 | [llmtune](https://github.com/kuleshov-group/llmtune) | 528 | 🔠 2️⃣  | 4-Bit Finetuning of Large Language Models on One Consumer GPU |
| 2023-05-10 | [BiLLa](https://github.com/Neutralzz/BiLLa) | 402 | 🔠 ⛽ 🚌 1️⃣ 2️⃣ 🀄  | BiLLa: A Bilingual LLaMA with Enhanced Reasoning Ability |
| 2023-05-07 | [PaLM](https://github.com/conceptofmind/PaLM) | 688 | 🔠 🚌 2️⃣  | An open-source implementation of Google's PaLM models |
| 2023-05-06 | [Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA) | 1514 | 🔠 🎥 🚌 1️⃣ 2️⃣  | Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding |
| 2023-05-04 | [lit-gpt](https://github.com/Lightning-AI/lit-gpt) | 2574 | 🔠 🖼️ 🚌 1️⃣ 2️⃣  | Hackable implementation of state-of-the-art open-source LLMs based on nanoGPT. Supports flash attention, 4-bit and 8-bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed. |
| 2023-05-04 | [Personalize-SAM](https://github.com/ZrrSkywalker/Personalize-SAM) | 1175 | 🖼️ ⛽ 🚌 2️⃣  | Personalize Segment Anything Model (SAM) with 1 shot in 10 seconds |
| 2023-05-04 | [WebCPM](https://github.com/thunlp/WebCPM) | 871 | 🔠 ⛽ 🚌 2️⃣  | Official codes for ACL 2023 paper "WebCPM: Interactive Web Search for Chinese Long-form Question Answering" |
| 2023-05-03 | [Dromedary](https://github.com/IBM/Dromedary) | 969 | 🔠 🚌 2️⃣  | Dromedary: towards helpful, ethical and reliable LLMs. |
| 2023-05-03 | [alpaca_farm](https://github.com/tatsu-lab/alpaca_farm) | 521 | 🔠 ⛽ 🚌 2️⃣  | A simulation framework for RLHF and alternatives. Develop your RLHF method without collecting human data.  |
| 2023-05-02 | [CodeTF](https://github.com/salesforce/CodeTF) | 1321 | 🔠 ⛽ 🚕 2️⃣  | CodeTF: One-stop Transformer Library for State-of-the-art Code LLM |
| 2023-04-28 | [llm-foundry](https://github.com/mosaicml/llm-foundry) | 2984 | 🔠 🚌 2️⃣  | LLM training code for MosaicML foundation models |
| 2023-04-27 | [Prompt-Diffusion](https://github.com/Zhendong-Wang/Prompt-Diffusion) | 277 | 🖼️ ⛽ 🚌 2️⃣  | Official PyTorch implementation of the paper "In-Context Learning Unlocked for Diffusion Models" |
| 2023-04-26 | [diffusion](https://github.com/mosaicml/diffusion) | 449 | 🖼️ 2️⃣  | This repo contains code used to train your own Stable Diffusion model on your own data. |
| 2023-04-25 | [HugNLP](https://github.com/HugAILab/HugNLP) | 323 | 🔠 2️⃣  | HugNLP is a unified and comprehensive NLP library based on HuggingFace Transformer. Please hugging for NLP now!😊 |
| 2023-04-25 | [mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl) | 1388 | 🔠 🖼️ ⛽ 🚌 2️⃣  | mPLUG-Owl🦉: Modularization Empowers Large Language Models with Multimodality |
| 2023-04-24 | [starcoder](https://github.com/bigcode-project/starcoder) | 6218 | 🔠 ⛽ 🚕 2️⃣  | Home of StarCoder: fine-tuning & inference! |
| 2023-04-23 | [GPT4Tools](https://github.com/StevenGrove/GPT4Tools) | 580 | 🔠 🖼️ ⛽ 🚌 2️⃣  | GPT4Tools is an intelligent system that can automatically decide, control, and utilize different visual foundation models, allowing the user to interact with images during a conversation. |
| 2023-04-23 | [VisualGLM-6B](https://github.com/THUDM/VisualGLM-6B) | 3295 | 🔠 🖼️ 🚌 2️⃣ ✂️  | Chinese and English multimodal conversational language model \| 多模态中英双语对话语言模型 |
| 2023-04-23 | [godot-dodo](https://github.com/minosvasilias/godot-dodo) | 449 | 🔠 🚕 2️⃣  | Finetuning large language models for GDScript generation. |
| 2023-04-23 | [WizardLM](https://github.com/nlpxucan/WizardLM) | 6699 | 🔠 ⛽ 🚌 2️⃣  | Family of instruction-following LLMs powered by Evol-Instruct: WizardLM, WizardCoder and WizardMath |
| 2023-04-20 | [LaWGPT](https://github.com/pengxiao-song/LaWGPT) | 5133 | 🔠 ⛽ 🚕 1️⃣ 2️⃣  |  🎉 Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. 基于中文法律知识的大语言模型 |
| 2023-04-19 | [MiniGPT-4-ZH](https://github.com/RiseInRose/MiniGPT-4-ZH) | 752 | 🔠 🖼️ 🚌 2️⃣ ✂️  | MiniGPT-4 中文部署翻译 完善部署细节 |
| 2023-04-19 | [naturalspeech2-pytorch](https://github.com/lucidrains/naturalspeech2-pytorch) | 915 | 🎵 2️⃣  | Implementation of Natural Speech 2, Zero-shot Speech and Singing Synthesizer, in Pytorch |
| 2023-04-17 | [h2o-llmstudio](https://github.com/h2oai/h2o-llmstudio) | 2596 | 🔠 ⛽ 🚌 2️⃣ 💰  | H2O LLM Studio - a framework and no-code GUI for fine-tuning LLMs. Documentation: https://h2oai.github.io/h2o-llmstudio/ |
| 2023-04-17 | [LLaVA](https://github.com/haotian-liu/LLaVA) | 4859 | 🔠 🖼️ ⛽ 🚌 1️⃣ 2️⃣  | Visual Instruction Tuning: Large Language-and-Vision Assistant built towards multimodal GPT-4 level capabilities. |
| 2023-04-15 | [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) | 22290 | 🔠 🖼️ 🚌 1️⃣ 2️⃣  | MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models |
| 2023-04-13 | [HuatuoGPT](https://github.com/FreedomIntelligence/HuatuoGPT) | 639 | 🔠 ⛽ 🚕 2️⃣  | HuatuoGPT, Towards Taming Language Models To Be a Doctor. (An Open Medical GPT) |
| 2023-04-12 | [dify](https://github.com/langgenius/dify) | 8038 | 🔠 ⛽ 2️⃣ 🔨 `Python`  | One API for plugins and datasets, one interface for prompt engineering and visual operation, all for creating powerful AI applications. |
| 2023-04-08 | [ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning) | 2964 | 🔠 ⛽ 2️⃣ 3️⃣ 🀄  | Fine-tuning ChatGLM-6B with PEFT \| 基于 PEFT 的高效 ChatGLM 微调 |
| 2023-04-06 | [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | 3334 | 🔠 ⛽ 2️⃣  | Instruction Tuning with GPT-4 |
| 2023-04-06 | [ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning) | 1465 | 🔠 2️⃣ 🀄  | 基于ChatGLM-6B、ChatGLM2-6B模型，进行下游具体任务微调，涉及Freeze、Lora、P-tuning、全参微调等 |
| 2023-04-06 | [threestudio](https://github.com/threestudio-project/threestudio) | 2613 | 🖼️ 🚌 2️⃣ 🧊  | A unified framework for 3D content generation. |
| 2023-04-03 | [VideoCrafter](https://github.com/VideoCrafter/VideoCrafter) | 2000 | 🔠 🎥 🚌 2️⃣  | A Toolkit for Text-to-Video Generation and Editing |
| 2023-04-02 | [Firefly](https://github.com/yangjianxin1/Firefly) | 2306 | 🔠 ⛽ 🚌 2️⃣ ✂️ 🀄  | Firefly(流萤): 中文对话式大语言模型(全量微调+QLoRA)，支持微调Llma2、Llama、Qwen、Baichuan、ChatGLM2、InternLM、Ziya、Bloom等大模型 |
| 2023-04-01 | [LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) | 2610 | 🔠 ⛽ 🚌 2️⃣ ✂️ 📝  | ⚡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.⚡ |
| 2023-04-01 | [Otter](https://github.com/Luodian/Otter) | 2373 | 🔠 🖼️ ⛽ 🚌 2️⃣  | 🦦 Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following and in-context learning ability. |
| 2023-03-31 | [Huatuo-Llama-Med-Chinese](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) | 3607 | 🔠 ⛽ 🚕 2️⃣ 🀄  | Repo for BenTsao [original name: HuaTuo (华驼)], Instruction-tuning Large Language Models with Chinese Medical Knowledge. 本草（原名：华驼）模型仓库，基于中文医学知识的大语言模型指令微调 |
| 2023-03-31 | [Med-ChatGLM](https://github.com/SCIR-HI/Med-ChatGLM) | 759 | 🔠 ⛽ 🚕 2️⃣  | Repo for Chinese Medical ChatGLM 基于中文医学知识的ChatGLM指令微调 |
| 2023-03-31 | [baize-chatbot](https://github.com/project-baize/baize-chatbot) | 3016 | 🔠 ⛽ 🚌 2️⃣  | Let ChatGPT teach your own chatbot in hours with a single GPU! |
| 2023-03-30 | [Llama-X](https://github.com/AetherCortex/Llama-X) | 1420 | 🔠 🚌 2️⃣  | Open Academic Research on Improving LLaMA to SOTA LLM |
| 2023-03-30 | [FollowYourPose](https://github.com/mayuelala/FollowYourPose) | 658 | 🎥 🚌 2️⃣  | Follow-Your-Pose: This repo is the official implementation of "Follow-Your-Pose : Pose-Guided Text-to-Video Generation using Pose-Free Videos"    |
| 2023-03-29 | [LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters) | 708 | 🔠 ⛽ 🚌 2️⃣  | LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models |
| 2023-03-28 | [ChatGenTitle](https://github.com/WangRongsheng/ChatGenTitle) | 750 | 🔠 🚕 2️⃣  | 🌟 ChatGenTitle：使用百万arXiv论文信息在LLaMA模型上进行微调的论文题目生成模型 |
| 2023-03-27 | [LMFlow](https://github.com/OptimalScale/LMFlow) | 7157 | 🔠 ⛽ 🚌 2️⃣ 3️⃣  | An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All. |
| 2023-03-24 | [Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT) | 2041 | 🔠 ⛽ 2️⃣  | We unified the interfaces of instruction-tuning data (e.g., CoT data), multiple LLMs and parameter-efficient methods (e.g., lora, p-tuning) together for easy use. Meanwhile, we created a new branch to build a Tabular LLM.（我们分别统一了丰富的IFT数据（如CoT数据，目前仍不断扩充）、多种训练效率方法（如lora，p-tuning）以及多种LLMs，三个层面上的接口，打造方便研究人员上手的LLM-IFT研究平台。同时tabular_llm分支构建了面向表格智能任务的LLM。 |
| 2023-03-24 | [dolly](https://github.com/databrickslabs/dolly) | 10558 | 🔠 ⛽ 🚌 2️⃣ 💰  | Databricks’ Dolly, a large language model trained on the Databricks Machine Learning Platform |
| 2023-03-24 | [h2ogpt](https://github.com/h2oai/h2ogpt) | 7184 | 🔠 🚌 2️⃣ 💰  | Private Q&A and summarization of documents+images or chat with local GPT, 100% private, Apache 2.0. Supports LLaMa2, llama.cpp, and more. Demo: https://gpt.h2o.ai/ https://codellama.h2o.ai/ |
| 2023-03-23 | [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna) | 3976 | 🔠 🚌 2️⃣ ✂️ 🀄  | Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model —— 一个中文低资源的llama+lora方案，结构参考alpaca |
| 2023-03-23 | [Make-It-3D](https://github.com/junshutang/Make-It-3D) | 1328 | 🖼️ 🚌 2️⃣ 🧊  | [ICCV 2023] Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior |
| 2023-03-23 | [InstructGLM](https://github.com/yanqiangmiffy/InstructGLM) | 607 | 🔠 ⛽ 2️⃣  | ChatGLM-6B 指令学习\|指令数据\|Instruct |
| 2023-03-22 | [lit-llama](https://github.com/Lightning-AI/lit-llama) | 5132 | 🔠 🚌 1️⃣ 2️⃣ ✂️ 💰  | Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed. |
| 2023-03-22 | [codealpaca](https://github.com/sahil280114/codealpaca) | 1231 | 🔠 ⛽ 2️⃣  | This is the repo for the Code Alpaca project, which aims to build and share an instruction-following LLaMA model for code generation. This repo is fully based on Stanford Alpaca ,and only changes the data used for training. Training approach is the same. |
| 2023-03-21 | [Linly](https://github.com/CVI-SZU/Linly) | 2681 | 🔠 ⛽ 🚌 1️⃣ 2️⃣ 3️⃣ ✂️ 💡 💰 🀄  | Chinese-LLaMA 1&2、Chinese-Falcon 基础模型；ChatFlow中文对话模型；中文OpenLLaMA模型；NLP预训练/指令微调数据集 |
| 2023-03-21 | [ChatDoctor](https://github.com/Kent0n-Li/ChatDoctor) | 3080 | 🔠 ⛽ 🚕 2️⃣  |  |
| 2023-03-21 | [DreamPose](https://github.com/johannakarras/DreamPose) | 670 | 🎥 ⛽ 🚌 2️⃣  | Official implementation of "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion" |
| 2023-03-19 | [LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter) | 4729 | 🔠 🖼️ ⛽ 🚌 2️⃣  | Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters |
| 2023-03-19 | [FastChat](https://github.com/lm-sys/FastChat) | 26939 | 🔠 🚌 2️⃣ 💰  | An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena. |
| 2023-03-19 | [xTuring](https://github.com/stochasticai/xTuring) | 2244 | 🔠 2️⃣  | Easily build, customize and control your own LLMs |
| 2023-03-18 | [KoAlpaca](https://github.com/Beomi/KoAlpaca) | 1289 | 🔠 ⛽ 🚌 2️⃣  | KoAlpaca: 한국어 명령어를 이해하는 오픈소스 언어모델 |
| 2023-03-17 | [BELLE](https://github.com/LianjiaTech/BELLE) | 6638 | 🔠 ⛽ 🚌 2️⃣ ✂️ 🀄  | BELLE: Be Everyone's Large Language model Engine（开源中文对话大模型） |
| 2023-03-17 | [zero123](https://github.com/cvlab-columbia/zero123) | 1711 | 🖼️ 2️⃣ 🧊  | Zero-1-to-3: Zero-shot One Image to 3D Object (ICCV 2023) |
| 2023-03-17 | [alpaca_chinese_dataset](https://github.com/hikariming/alpaca_chinese_dataset) | 938 | 🔠 ⛽ 2️⃣ 🀄  | 人工精调的中文对话数据集和一段chatglm的微调代码 |
| 2023-03-16 | [FateZero](https://github.com/ChenyangQiQi/FateZero) | 849 | 🔠 🎥 ⛽ 🚌 2️⃣  | [ICCV 2023 Oral] "FateZero: Fusing Attentions for Zero-shot Text-based Video Editing" |
| 2023-03-16 | [ChatGLM-finetune-LoRA](https://github.com/lich99/ChatGLM-finetune-LoRA) | 601 | 🔠 2️⃣  | Code for fintune ChatGLM-6b using low-rank adaptation (LoRA) |
| 2023-03-16 | [ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) | 3281 | 🔠 2️⃣ 🀄  | 一种平价的chatgpt实现方案,  基于ChatGLM-6B + LoRA |
| 2023-03-15 | [so-vits-svc-fork](https://github.com/voicepaw/so-vits-svc-fork) | 6771 | 🎵 🚌 2️⃣  | so-vits-svc fork with realtime support, improved interface and more features. |
| 2023-03-15 | [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | 14317 | 🔠 1️⃣ 2️⃣ ✂️ 💡 🀄  | 中文LLaMA&Alpaca大语言模型+本地CPU/GPU训练部署 (Chinese LLaMA & Alpaca LLMs) |
| 2023-03-13 | [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) | 33818 | 🔠 🚌 2️⃣ ✂️ 🀄  | ChatGLM-6B: An Open Bilingual Dialogue Language Model \| 开源双语对话语言模型 |
| 2023-03-13 | [alpaca-lora](https://github.com/tloen/alpaca-lora) | 16895 | 🔠 🚌 2️⃣  | Instruct-tune LLaMA on consumer hardware |
| 2023-03-10 | [so-vits-svc](https://github.com/svc-develop-team/so-vits-svc) | 18690 | 🎵 🚌 2️⃣  | SoftVC VITS Singing Voice Conversion |
| 2023-03-10 | [stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca) | 26576 | 🔠 ⛽ 🚌 2️⃣  | Code and documentation to train Stanford's Alpaca models, and generate the data. |
| 2023-03-09 | [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) | 2970 | 🔠 🖼️ 🚌 2️⃣  | Official implementation of the paper "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection" |
| 2023-03-04 | [DDSP-SVC](https://github.com/yxlllc/DDSP-SVC) | 1079 | 🎵 ⛽ 🚌 2️⃣  | Real-time end-to-end singing voice conversion system based on DDSP (Differentiable Digital Signal Processing) |
| 2023-03-03 | [OpenChatKit](https://github.com/togethercomputer/OpenChatKit) | 8864 | 🔠 ⛽ 🚌 1️⃣ 2️⃣  |  |
| 2023-02-26 | [consistency_models](https://github.com/openai/consistency_models) | 5395 | 🖼️ 🚌 2️⃣ ✂️  | Official repo for consistency models. |
| 2023-02-11 | [VITS-fast-fine-tuning](https://github.com/Plachtaa/VITS-fast-fine-tuning) | 3019 | 🎵 2️⃣  | This repo is a pipeline of VITS finetuning for fast speaker adaptation TTS, and many-to-many voice conversion |
| 2023-02-01 | [ControlNet](https://github.com/lllyasviel/ControlNet) | 23104 | 🖼️ 🚌 2️⃣  | Let us control diffusion models! |
| 2023-01-27 | [musiclm-pytorch](https://github.com/lucidrains/musiclm-pytorch) | 2680 | 🎵 🚌 2️⃣ ✂️  | Implementation of MusicLM, Google's new SOTA model for music generation using attention networks, in Pytorch |
| 2023-01-09 | [instruct-pix2pix](https://github.com/timothybrooks/instruct-pix2pix) | 5167 | 🖼️ ⛽ 🚌 2️⃣  | PyTorch implementation of InstructPix2Pix, an instruction-based image editing model |
| 2022-12-28 | [nanoGPT](https://github.com/karpathy/nanoGPT) | 24323 | 🔠 ⛽ 🚌 1️⃣ 2️⃣  | The simplest, fastest repository for training/finetuning medium-sized GPTs. |
| 2022-12-08 | [lora](https://github.com/cloneofsimo/lora) | 5632 | 🖼️ 2️⃣  | Using Low-rank adaptation to quickly fine-tune diffusion models. |
| 2022-11-25 | [peft](https://github.com/huggingface/peft) | 9330 | 🔠 2️⃣  | 🤗 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning. |
| 2022-11-23 | [SadTalker](https://github.com/OpenTalker/SadTalker) | 6358 | 🖼️ 🎵 🎥 🚌 2️⃣  | [CVPR 2023] SadTalker：Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation |
| 2022-10-22 | [diff-svc](https://github.com/prophesier/diff-svc) | 2398 | 🎵 🚌 2️⃣  | Singing Voice Conversion via diffusion model |
| 2022-10-20 | [open_flamingo](https://github.com/mlfoundations/open_flamingo) | 2814 | 🔠 🖼️ 🎥 ⛽ 🚌 2️⃣  | An open-source framework for training large multimodal models. |
| 2022-09-29 | [motion-diffusion-model](https://github.com/GuyTevet/motion-diffusion-model) | 2374 | 🎥 ⛽ 🚌 2️⃣  | The official PyTorch implementation of the paper "Human Motion Diffusion Model" |
| 2022-09-09 | [VToonify](https://github.com/williamyang1991/VToonify) | 3283 | 🎥 🚌 2️⃣  | [SIGGRAPH Asia 2022] VToonify: Controllable High-Resolution Portrait Video Style Transfer |
| 2022-09-06 | [Dreambooth-Stable-Diffusion](https://github.com/XavierXiao/Dreambooth-Stable-Diffusion) | 6959 | 🖼️ 2️⃣  | Implementation of Dreambooth (https://arxiv.org/abs/2208.12242) with Stable Diffusion |
| 2022-08-24 | [LAVIS](https://github.com/salesforce/LAVIS) | 6512 | 🔠 🖼️ ⛽ 🚌 1️⃣ 2️⃣  | LAVIS - A One-stop Library for Language-Vision Intelligence |
| 2022-08-02 | [textual_inversion](https://github.com/rinongal/textual_inversion) | 2574 | 🖼️ ⛽ 🚌 2️⃣  |  |
| 2022-07-25 | [modelscope](https://github.com/modelscope/modelscope) | 3841 | 🔠 🖼️ 🎵 2️⃣  | ModelScope: bring the notion of Model-as-a-Service to life. |
| 2022-05-30 | [diffusers](https://github.com/huggingface/diffusers) | 17580 | 🖼️ 🎵 2️⃣ 📝  | 🤗 Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch |
