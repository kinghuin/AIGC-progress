| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-07-13 | [X-PLUG/CValues](https://github.com/X-PLUG/CValues) | 166 | 🔠⛽🀄❓ | 面向中文大模型价值观的评估与对齐研究 |
| 2023-07-06 | [InternLM/InternLM](https://github.com/InternLM/InternLM) | 2125 | 🔠🚌2️⃣🀄 | InternLM has open-sourced a 7 billion parameter base model, a chat model tailored for practical scenarios and the training system. |
| 2023-07-05 | [OpenLMLab/MOSS-RLHF](https://github.com/OpenLMLab/MOSS-RLHF) | 527 | 🔠🚌3️⃣🀄 | MOSS-RLHF |
| 2023-06-30 | [OpenBMB/VisCPM](https://github.com/OpenBMB/VisCPM) | 665 | 🔠🖼️🚌`2️⃣`🀄 | Chinese and English Multimodal Large Model Series (Chat and Paint) \| 基于CPM基础模型的中英双语多模态大模型系列 |
| 2023-06-29 | [IMOSR/MediaGPT](https://github.com/IMOSR/MediaGPT) | 403 | 🔠⛽🚕2️⃣🀄 | 中文的自媒体大语言模型MediaGPT(曾用名Media LLaMA) |
| 2023-06-24 | [THUDM/ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) | 9367 | 🔠🚌✂️🀄 | ChatGLM2-6B: An Open Bilingual Chat LLM \| 开源双语对话语言模型 |
| 2023-06-16 | [haonan-li/CMMLU](https://github.com/haonan-li/CMMLU) | 126 | 🔠⛽🀄❓ | CMMLU是一个综合性的🀄评估基准，专门用于评估语言模型在🀄语境下的知识和💡能力。 |
| 2023-06-14 | [baichuan-inc/Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B) | 4538 | 🔠⛽🚌🀄 | A large-scale 7B pretraining language model developed by BaiChuan-Inc. |
| 2023-06-12 | [lyogavin/Anima](https://github.com/lyogavin/Anima) | 1019 | 🔠⛽🚌2️⃣🀄 | 第一个开源的基于QLoRA的33B中文大语言模型First QLoRA based open source 33B Chinese LLM |
| 2023-06-02 | [shibing624/MedicalGPT](https://github.com/shibing624/MedicalGPT) | 911 | 🔠⛽🚕1️⃣2️⃣3️⃣🀄 | MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. 训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。 |
| 2023-05-28 | [THUDM/WebGLM](https://github.com/THUDM/WebGLM) | 1144 | 🔠⛽🚌2️⃣📱🀄 | WebGLM: An Efficient Web-enhanced Question Answering System (KDD 2023) |
| 2023-05-10 | [Neutralzz/BiLLa](https://github.com/Neutralzz/BiLLa) | 387 | 🔠⛽🚌1️⃣2️⃣🀄 | BiLLa: A Bilingual LLaMA with Enhanced Reasoning Ability |
| 2023-04-28 | [dandelionsllm/pandallm](https://github.com/dandelionsllm/pandallm) | 834 | 🔠⛽🚌🀄 | Panda: 海外中文开源大语言模型，基于 Llama-7B, -13B, -33B, -65B 进行中文领域上的持续预训练。 |
| 2023-04-26 | [OpenBuddy/OpenBuddy](https://github.com/OpenBuddy/OpenBuddy) | 730 | 🔠🚌🀄 | Open Multilingual Chatbot for Everyone |
| 2023-04-23 | [THUDM/VisualGLM-6B](https://github.com/THUDM/VisualGLM-6B) | 2997 | 🔠🖼️⛽🚌2️⃣✂️🀄 | Chinese and English multimodal conversational language model \| 多模态中英双语对话语言模型 |
| 2023-04-21 | [lvwzhen/law-cn-ai](https://github.com/lvwzhen/law-cn-ai) | 4323 | 🔠⛽📱🀄 | ⚖️ AI 法律助手 |
| 2023-04-20 | [pengxiao-song/LaWGPT](https://github.com/pengxiao-song/LaWGPT) | 4968 | 🔠⛽🚕2️⃣🀄 |  🎉 Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. 基于中文法律知识的大语言模型 |
| 2023-04-18 | [kaqijiang/Auto-GPT-ZH](https://github.com/kaqijiang/Auto-GPT-ZH) | 2204 | 🔠📱🀄 | Auto-GPT中文版本及爱好者组织 同步更新原项目 AI领域创业 自媒体组织 用AI工作学习创作变现 |
| 2023-04-15 | [OpenLMLab/MOSS](https://github.com/OpenLMLab/MOSS) | 11206 | 🔠⛽🚌✂️💰🀄 | An open-source tool-augmented conversational language model from Fudan University |
| 2023-04-08 | [hiyouga/ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning) | 2318 | 🔠⛽2️⃣3️⃣🀄 | Fine-tuning ChatGLM-6B with PEFT \| 基于 PEFT 的高效 ChatGLM 微调 |
| 2023-04-06 | [liucongg/ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning) | 1055 | 🔠2️⃣🀄 | 基于ChatGLM-6B模型，进行下游具体任务微调，涉及Freeze、Lora、P-tuning等 |
| 2023-04-02 | [yangjianxin1/Firefly](https://github.com/yangjianxin1/Firefly) | 1028 | 🔠⛽🚌2️⃣✂️🀄 | Firefly(流萤): 中文对话式大语言模型(全量微调+QLoRA) |
| 2023-03-31 | [LC1332/Chinese-alpaca-lora](https://github.com/LC1332/Chinese-alpaca-lora) | 625 | 🔠⛽🚌🀄 | 骆驼:A Chinese finetuned instruction LLaMA. Developed by 陈启源 @ 华中师范大学 & 李鲁鲁 @ 商汤科技 & 冷子昂 @ 商汤科技 |
| 2023-03-31 | [SCIR-HI/Huatuo-Llama-Med-Chinese](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) | 3356 | 🔠⛽🚕2️⃣🀄 | Repo for BenTsao [original name: HuaTuo (华驼)], Llama-7B tuned with Chinese medical knowledge. 本草（原名：华驼）模型仓库，基于中文医学知识的LLaMA模型指令微调 |
| 2023-03-31 | [imClumsyPanda/langchain-ChatGLM](https://github.com/imClumsyPanda/langchain-ChatGLM) | 12095 | 🔠📱🀄 | langchain-ChatGLM, local knowledge based ChatGLM with langchain ｜ 基于本地知识库的 ChatGLM 问答 |
| 2023-03-23 | [Facico/Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna) | 3837 | 🔠🚌2️⃣✂️🀄 | Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model —— 一个中文低资源的llama+lora方案，结构参考alpaca |
| 2023-03-21 | [CVI-SZU/Linly](https://github.com/CVI-SZU/Linly) | 2303 | 🔠⛽🚌1️⃣2️⃣`3️⃣`✂️💡💰🀄 | Chinese-LLaMA 、Chinese-Falcon 基础模型；ChatFlow中文对话模型；中文OpenLLaMA模型；NLP预训练/指令微调数据集 |
| 2023-03-21 | [LC1332/Luotuo-Chinese-LLM](https://github.com/LC1332/Luotuo-Chinese-LLM) | 3114 | 🔠⛽🚌🀄 | 骆驼(Luotuo): Open Sourced Chinese Language Models. Developed by 陈启源 @ 华中师范大学 & 李鲁鲁 @ 商汤科技 & 冷子昂 @ 商汤科技 |
| 2023-03-21 | [yzfly/awesome-chatgpt-zh](https://github.com/yzfly/awesome-chatgpt-zh) | 7687 | 🔠📝🀄 | ChatGPT 中文指南🔥，ChatGPT 中文调教指南，指令指南，应用开发指南，精选资源清单，更好的使用 chatGPT 让你的生产力 up up up! 🚀 |
| 2023-03-17 | [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE) | 6238 | 🔠⛽🚌2️⃣✂️🀄 | BELLE: Be Everyone's Large Language model Engine（开源中文对话大模型） |
| 2023-03-17 | [hikariming/alpaca_chinese_dataset](https://github.com/hikariming/alpaca_chinese_dataset) | 881 | 🔠⛽2️⃣🀄 | 人工精调的中文对话数据集和一段chatglm的微调代码 |
| 2023-03-16 | [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) | 3145 | 🔠2️⃣🀄 | 一种平价的chatgpt实现方案,  基于ChatGLM-6B + LoRA |
| 2023-03-15 | [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | 12696 | 🔠1️⃣2️⃣✂️💡🀄 | 中文LLaMA&Alpaca大语言模型+本地CPU/GPU训练部署 (Chinese LLaMA & Alpaca LLMs) |
| 2023-03-14 | [ssbuild/chatglm_finetuning](https://github.com/ssbuild/chatglm_finetuning) | 1377 | 🔠2️⃣🀄 | chatglm 6b finetuning and alpaca finetuning |
| 2023-03-13 | [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) | 32102 | 🔠🚌2️⃣✂️🀄 | ChatGLM-6B: An Open Bilingual Dialogue Language Model \| 开源双语对话语言模型 |
| 2023-02-11 | [AI4Finance-Foundation/FinGPT](https://github.com/AI4Finance-Foundation/FinGPT) | 7178 | 🔠⛽🚕🀄 | Data-Centric FinGPT.  Open-source for open finance!  Revolutionize 🔥    We'll soon release the trained model. |
