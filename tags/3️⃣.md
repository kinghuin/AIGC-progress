| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-07-30 | [OpenLLaMA2](https://github.com/OpenLLMAI/OpenLLaMA2) | 85 | 🔠 2️⃣ 3️⃣  | DeepSpeed+Ray based LLaMA2 RLHF/RS training framework |
| 2023-07-05 | [MOSS-RLHF](https://github.com/OpenLMLab/MOSS-RLHF) | 761 | 🔠 🚌 3️⃣ 🀄  | MOSS-RLHF |
| 2023-06-22 | [direct-preference-optimization](https://github.com/eric-mitchell/direct-preference-optimization) | 407 | 🔠 2️⃣ 3️⃣  | Reference implementation for DPO (Direct Preference Optimization) |
| 2023-06-02 | [MedicalGPT](https://github.com/shibing624/MedicalGPT) | 1361 | 🔠 ⛽ 🚕 1️⃣ 2️⃣ 3️⃣ 🀄  | MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. 训练医疗大模型，实现了包括增量预训练、有监督微调、RLHF(奖励建模、强化学习训练)和DPO(直接偏好优化)。 |
| 2023-05-28 | [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) | 3318 | 🔠 1️⃣ 2️⃣ 3️⃣  | Easy-to-use LLM fine-tuning framework (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, ChatGLM2) |
| 2023-05-15 | [safe-rlhf](https://github.com/PKU-Alignment/safe-rlhf) | 782 | 🔠 ⛽ 🚌 2️⃣ 3️⃣  | Safe-RLHF: Constrained Value Alignment via Safe Reinforcement Learning from Human Feedback |
| 2023-04-08 | [ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning) | 2939 | 🔠 ⛽ 2️⃣ 3️⃣ 🀄  | Fine-tuning ChatGLM-6B with PEFT \| 基于 PEFT 的高效 ChatGLM 微调 |
| 2023-04-01 | [ImageReward](https://github.com/THUDM/ImageReward) | 603 | 🔠 🖼️ 3️⃣ 🔌  | ImageReward: Learning and Evaluating Human Preferences for Text-to-image Generation |
| 2023-03-27 | [LMFlow](https://github.com/OptimalScale/LMFlow) | 7132 | 🔠 ⛽ 🚌 2️⃣ 3️⃣  | An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All. |
| 2023-03-21 | [Linly](https://github.com/CVI-SZU/Linly) | 2663 | 🔠 ⛽ 🚌 1️⃣ 2️⃣ 3️⃣ ✂️ 💡 💰 🀄  | Chinese-LLaMA 1&2、Chinese-Falcon 基础模型；ChatFlow中文对话模型；中文OpenLLaMA模型；NLP预训练/指令微调数据集 |
| 2023-02-27 | [chatllama](https://github.com/juncongmoo/chatllama) | 1122 | 🔠 3️⃣  | ChatLLaMA 📢 Open source implementation for LLaMA-based ChatGPT runnable in a single GPU. 15x faster training process than ChatGPT |
| 2022-12-09 | [PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch) | 7249 | 🔠 3️⃣  | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM |
| 2022-10-03 | [trlx](https://github.com/CarperAI/trlx) | 3824 | 🔠 3️⃣  | A repo for distributed training of language models with Reinforcement Learning via Human Feedback (RLHF) |
