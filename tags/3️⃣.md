| Date | Repository | Stars | tags | Language |  Description  |
|------------|---------|-------|-------------|-------------|-------------|
| 2023-07-30 | [OpenLLMAI/OpenLLaMA2](https://github.com/OpenLLMAI/OpenLLaMA2) | 63 | ğŸ” 2ï¸âƒ£3ï¸âƒ£ |   | DeepSpeed+Ray based LLaMA2 SFT/RLHF training framework |
| 2023-07-05 | [OpenLMLab/MOSS-RLHF](https://github.com/OpenLMLab/MOSS-RLHF) | 741 | ğŸ” ğŸšŒ3ï¸âƒ£ğŸ€„ |   | MOSS-RLHF |
| 2023-06-22 | [eric-mitchell/direct-preference-optimization](https://github.com/eric-mitchell/direct-preference-optimization) | 390 | ğŸ” 2ï¸âƒ£3ï¸âƒ£ |   | Reference implementation for DPO (Direct Preference Optimization) |
| 2023-06-02 | [shibing624/MedicalGPT](https://github.com/shibing624/MedicalGPT) | 1282 | ğŸ” â›½ğŸš•1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£ğŸ€„ |   | MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. è®­ç»ƒåŒ»ç–—å¤§æ¨¡å‹ï¼Œå®ç°åŒ…æ‹¬äºŒæ¬¡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚ |
| 2023-05-28 | [hiyouga/LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) | 3003 | ğŸ” 1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£ |   | Easy-to-use LLM fine-tuning framework (LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, ChatGLM2) |
| 2023-05-15 | [PKU-Alignment/safe-rlhf](https://github.com/PKU-Alignment/safe-rlhf) | 767 | ğŸ” â›½ğŸšŒ2ï¸âƒ£3ï¸âƒ£ |   | Safe-RLHF: Constrained Value Alignment via Safe Reinforcement Learning from Human Feedback |
| 2023-04-08 | [hiyouga/ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning) | 2898 | ğŸ” â›½2ï¸âƒ£3ï¸âƒ£ğŸ€„ |   | Fine-tuning ChatGLM-6B with PEFT \| åŸºäº PEFT çš„é«˜æ•ˆ ChatGLM å¾®è°ƒ |
| 2023-04-01 | [THUDM/ImageReward](https://github.com/THUDM/ImageReward) | 596 | ğŸ” ğŸ–¼ï¸3ï¸âƒ£ğŸ”Œ |   | ImageReward: Learning and Evaluating Human Preferences for Text-to-image Generation |
| 2023-03-27 | [OptimalScale/LMFlow](https://github.com/OptimalScale/LMFlow) | 7099 | ğŸ” â›½ğŸšŒ2ï¸âƒ£3ï¸âƒ£ |   | An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All. |
| 2023-03-21 | [CVI-SZU/Linly](https://github.com/CVI-SZU/Linly) | 2634 | ğŸ” â›½ğŸšŒ1ï¸âƒ£2ï¸âƒ£`3ï¸âƒ£`âœ‚ï¸ğŸ’¡ğŸ’°ğŸ€„ |   | Chinese-LLaMA 1&2ã€Chinese-Falcon åŸºç¡€æ¨¡å‹ï¼›ChatFlowä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼›ä¸­æ–‡OpenLLaMAæ¨¡å‹ï¼›NLPé¢„è®­ç»ƒ/æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† |
| 2023-02-27 | [juncongmoo/chatllama](https://github.com/juncongmoo/chatllama) | 1118 | ğŸ” 3ï¸âƒ£ |   | ChatLLaMA ğŸ“¢ Open source implementation for LLaMA-based ChatGPT runnable in a single GPU. 15x faster training process than ChatGPT |
| 2022-12-09 | [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch) | 7230 | ğŸ” 3ï¸âƒ£ |   | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM |
| 2022-10-03 | [CarperAI/trlx](https://github.com/CarperAI/trlx) | 3809 | ğŸ” 3ï¸âƒ£ |   | A repo for distributed training of language models with Reinforcement Learning via Human Feedback (RLHF) |
