| Date | Repository | Stars | Description | tags |
|------------|---------|-------|-------------|-------------|
| 2023-05-17 | [mit-han-lab/fastcomposer](https://github.com/mit-han-lab/fastcomposer) | ![mit-han-lab/fastcomposer Stars](https://img.shields.io/github/stars/mit-han-lab/fastcomposer.svg?label=&style=flat-square) |  | `图像`,`数据(TD)`,`通用模型`,`训练(TD)` |
| 2023-05-15 | [PKU-Alignment/safe-rlhf](https://github.com/PKU-Alignment/safe-rlhf) | ![PKU-Alignment/safe-rlhf Stars](https://img.shields.io/github/stars/PKU-Alignment/safe-rlhf.svg?label=&style=flat-square) | Safe-RLHF: Constrained Value Alignment via Safe Reinforcement Learning from Human Feedback | `文本`,`数据`,`通用模型`,`训练`,`强化` |
| 2023-04-29 | [replit/ReplitLM](https://github.com/replit/ReplitLM) | ![replit/ReplitLM Stars](https://img.shields.io/github/stars/replit/ReplitLM.svg?label=&style=flat-square) | Inference code and configs for the ReplitLM model family | `文本`,`数据`,`通用模型` |
| 2023-04-28 | [dandelionsllm/pandallm](https://github.com/dandelionsllm/pandallm) | ![dandelionsllm/pandallm Stars](https://img.shields.io/github/stars/dandelionsllm/pandallm.svg?label=&style=flat-square) | Panda: 海外中文开源大语言模型，基于 Llama-7B, -13B, -33B, -65B 进行中文领域上的持续预训练。 | `文本`,`数据`,`通用模型`,`中文` |
| 2023-04-28 | [mlfoundations/datacomp](https://github.com/mlfoundations/datacomp) | ![mlfoundations/datacomp Stars](https://img.shields.io/github/stars/mlfoundations/datacomp.svg?label=&style=flat-square) | DataComp: In search of the next generation of multimodal datasets | `文本`,`图像`,`数据` |
| 2023-04-26 | [open-mmlab/Multimodal-GPT](https://github.com/open-mmlab/Multimodal-GPT) | ![open-mmlab/Multimodal-GPT Stars](https://img.shields.io/github/stars/open-mmlab/Multimodal-GPT.svg?label=&style=flat-square) | Multimodal-GPT | `文本`,`图像`,`数据`,`通用模型` |
| 2023-04-25 | [X-PLUG/mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl) | ![X-PLUG/mPLUG-Owl Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?label=&style=flat-square) | mPLUG-Owl🦉: Modularization Empowers Large Language Models with Multimodality | `文本`,`图像`,`数据`,`通用模型`,`训练` |
| 2023-04-24 | [bigcode-project/starcoder](https://github.com/bigcode-project/starcoder) | ![bigcode-project/starcoder Stars](https://img.shields.io/github/stars/bigcode-project/starcoder.svg?label=&style=flat-square) | Home of StarCoder: fine-tuning & inference! | `文本`,`数据`,`定制模型`,`训练` |
| 2023-04-23 | [THUDM/VisualGLM-6B](https://github.com/THUDM/VisualGLM-6B) | ![THUDM/VisualGLM-6B Stars](https://img.shields.io/github/stars/THUDM/VisualGLM-6B.svg?label=&style=flat-square) | Chinese and English multimodal conversational language model \| 多模态中英双语对话语言模型 | `文本`,`图像`,`数据`,`通用模型`,`训练`,`压缩`,`中文` |
| 2023-04-23 | [nlpxucan/WizardLM](https://github.com/nlpxucan/WizardLM) | ![nlpxucan/WizardLM Stars](https://img.shields.io/github/stars/nlpxucan/WizardLM.svg?label=&style=flat-square) | WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions | `文本`,`数据`,`通用模型`,`训练` |
| 2023-04-23 | [mbzuai-nlp/LaMini-LM](https://github.com/mbzuai-nlp/LaMini-LM) | ![mbzuai-nlp/LaMini-LM Stars](https://img.shields.io/github/stars/mbzuai-nlp/LaMini-LM.svg?label=&style=flat-square) | LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions | `文本`,`数据`,`通用模型` |
| 2023-04-23 | [StevenGrove/GPT4Tools](https://github.com/StevenGrove/GPT4Tools) | ![StevenGrove/GPT4Tools Stars](https://img.shields.io/github/stars/StevenGrove/GPT4Tools.svg?label=&style=flat-square) | GPT4Tools is an intelligent system that can automatically decide, control, and utilize different visual foundation models, allowing the user to interact with images during a conversation. | `文本`,`图像`,`数据`,`通用模型`,`训练` |
| 2023-04-22 | [voidful/awesome-chatgpt-dataset](https://github.com/voidful/awesome-chatgpt-dataset) | ![voidful/awesome-chatgpt-dataset Stars](https://img.shields.io/github/stars/voidful/awesome-chatgpt-dataset.svg?label=&style=flat-square) | Unlock the Power of LLM: Explore These Datasets to Train Your Own ChatGPT! | `文本`,`数据`,`文档` |
| 2023-04-21 | [lvwzhen/law-cn-ai](https://github.com/lvwzhen/law-cn-ai) | ![lvwzhen/law-cn-ai Stars](https://img.shields.io/github/stars/lvwzhen/law-cn-ai.svg?label=&style=flat-square) | ⚖️ AI 法律助手 | `文本`,`数据`,`工具`,`中文` |
| 2023-04-20 | [pengxiao-song/LaWGPT](https://github.com/pengxiao-song/LaWGPT) | ![pengxiao-song/LaWGPT Stars](https://img.shields.io/github/stars/pengxiao-song/LaWGPT.svg?label=&style=flat-square) |  🎉 Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. 基于中文法律知识的大语言模型 | `文本`,`数据`,`定制模型`,`训练`,`中文` |
| 2023-04-18 | [h2oai/h2o-llmstudio](https://github.com/h2oai/h2o-llmstudio) | ![h2oai/h2o-llmstudio Stars](https://img.shields.io/github/stars/h2oai/h2o-llmstudio.svg?label=&style=flat-square) | H2O LLM Studio - a framework and no-code GUI for fine-tuning LLMs | `文本`,`数据`,`通用模型`,`训练`,`可商用` |
| 2023-04-08 | [hiyouga/ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning) | ![hiyouga/ChatGLM-Efficient-Tuning Stars](https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning.svg?label=&style=flat-square) | Fine-tuning ChatGLM-6B with PEFT \| 基于 PEFT 的高效 ChatGLM 微调 | `文本`,`数据`,`训练`,`强化`,`中文` |
| 2023-04-06 | [Instruction-Tuning-with-GPT-4/GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | ![Instruction-Tuning-with-GPT-4/GPT-4-LLM Stars](https://img.shields.io/github/stars/Instruction-Tuning-with-GPT-4/GPT-4-LLM.svg?label=&style=flat-square) | Instruction Tuning with GPT-4 | `文本`,`数据`,`训练` |
| 2023-04-01 | [project-baize/baize-chatbot](https://github.com/project-baize/baize-chatbot) | ![project-baize/baize-chatbot Stars](https://img.shields.io/github/stars/project-baize/baize-chatbot.svg?label=&style=flat-square) | Let ChatGPT teach your own chatbot in hours with a single GPU! | `文本`,`数据`,`通用模型`,`训练` |
| 2023-04-01 | [FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) | ![FreedomIntelligence/LLMZoo Stars](https://img.shields.io/github/stars/FreedomIntelligence/LLMZoo.svg?label=&style=flat-square) | ⚡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.⚡ | `文本`,`数据`,`通用模型`,`训练`,`压缩`,`文档` |
| 2023-03-31 | [SCIR-HI/Huatuo-Llama-Med-Chinese](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) | ![SCIR-HI/Huatuo-Llama-Med-Chinese Stars](https://img.shields.io/github/stars/SCIR-HI/Huatuo-Llama-Med-Chinese.svg?label=&style=flat-square) | Repo for BenTsao [original name: HuaTuo (华驼)], Llama-7B tuned with Chinese medical knowledge. 本草（原名：华驼）模型仓库，基于中文医学知识的LLaMA模型指令微调 | `文本`,`数据`,`定制模型`,`训练`,`中文` |
| 2023-03-31 | [LC1332/Chinese-alpaca-lora](https://github.com/LC1332/Chinese-alpaca-lora) | ![LC1332/Chinese-alpaca-lora Stars](https://img.shields.io/github/stars/LC1332/Chinese-alpaca-lora.svg?label=&style=flat-square) | 骆驼:A Chinese finetuned instruction LLaMA. Developed by 陈启源 @ 华中师范大学 & 李鲁鲁 @ 商汤科技 & 冷子昂 @ 商汤科技 | `文本`,`数据`,`通用模型`,`中文` |
| 2023-03-29 | [AGI-Edgerunners/LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters) | ![AGI-Edgerunners/LLM-Adapters Stars](https://img.shields.io/github/stars/AGI-Edgerunners/LLM-Adapters.svg?label=&style=flat-square) | LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models | `文本`,`数据`,`通用模型`,`训练` |
| 2023-03-27 | [OptimalScale/LMFlow](https://github.com/OptimalScale/LMFlow) | ![OptimalScale/LMFlow Stars](https://img.shields.io/github/stars/OptimalScale/LMFlow.svg?label=&style=flat-square) | An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Model for All. | `文本`,`数据`,`通用模型`,`训练`,`强化` |
| 2023-03-24 | [PhoebusSi/Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT) | ![PhoebusSi/Alpaca-CoT Stars](https://img.shields.io/github/stars/PhoebusSi/Alpaca-CoT.svg?label=&style=flat-square) | We unified the interfaces of instruction-tuning data (e.g., CoT data), multiple LLMs and parameter-efficient methods (e.g., lora, p-tuning) together for easy use. Meanwhile, we created a new branch to build a Tabular LLM.（我们分别统一了丰富的IFT数据（如CoT数据，目前仍不断扩充）、多种训练效率方法（如lora，p-tuning）以及多种LLMs，三个层面上的接口，打造方便研究人员上手的LLM-IFT研究平台。同时tabular_llm分支构建了面向表格智能任务的LLM。 | `文本`,`数据`,`训练` |
| 2023-03-22 | [Kent0n-Li/ChatDoctor](https://github.com/Kent0n-Li/ChatDoctor) | ![Kent0n-Li/ChatDoctor Stars](https://img.shields.io/github/stars/Kent0n-Li/ChatDoctor.svg?label=&style=flat-square) |  | `文本`,`数据`,`定制模型`,`训练` |
| 2023-03-21 | [LC1332/Luotuo-Chinese-LLM](https://github.com/LC1332/Luotuo-Chinese-LLM) | ![LC1332/Luotuo-Chinese-LLM Stars](https://img.shields.io/github/stars/LC1332/Luotuo-Chinese-LLM.svg?label=&style=flat-square) | 骆驼(Luotuo): Open Sourced Chinese Language Models. Developed by 陈启源 @ 华中师范大学 & 李鲁鲁 @ 商汤科技 & 冷子昂 @ 商汤科技 | `文本`,`数据`,`通用模型`,`中文` |
| 2023-03-21 | [CVI-SZU/Linly](https://github.com/CVI-SZU/Linly) | ![CVI-SZU/Linly Stars](https://img.shields.io/github/stars/CVI-SZU/Linly.svg?label=&style=flat-square) | Chinese-LLaMA基础模型；ChatFlow中文对话模型；中文OpenLLaMA模型；NLP预训练/指令微调数据集 | `文本`,`数据`,`通用模型`,`预训练`,`训练`,`强化(TD)`,`压缩`,`推理`,`可商用`,`中文` |
| 2023-03-17 | [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE) | ![LianjiaTech/BELLE Stars](https://img.shields.io/github/stars/LianjiaTech/BELLE.svg?label=&style=flat-square) | BELLE: Be Everyone's Large Language model Engine（开源中文对话大模型） | `文本`,`数据`,`通用模型`,`训练`,`压缩`,`中文` |
| 2023-03-17 | [hikariming/alpaca_chinese_dataset](https://github.com/hikariming/alpaca_chinese_dataset) | ![hikariming/alpaca_chinese_dataset Stars](https://img.shields.io/github/stars/hikariming/alpaca_chinese_dataset.svg?label=&style=flat-square) | 人工精调的中文对话数据集和一段chatglm的微调代码 | `文本`,`数据`,`训练`,`中文` |
