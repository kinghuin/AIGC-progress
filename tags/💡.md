| Date | Repository | Stars | tags |  Description  |
|------------|---------|-------|-------------|-------------|
| 2023-07-25 | [nikolaydubina/llama2.go](https://github.com/nikolaydubina/llama2.go) | 71 | 🔠💡 | LLaMA-2 in pure Go |
| 2023-07-23 | [Gadersd/whisper-burn](https://github.com/Gadersd/whisper-burn) | 60 | 🎵💡 | A Rust implementation of OpenAI's Whisper model using the burn framework |
| 2023-07-23 | [karpathy/llama2.c](https://github.com/karpathy/llama2.c) | 9659 | 🔠💡 | Inference Llama 2 in one file of pure C |
| 2023-07-15 | [Maknee/minigpt4.cpp](https://github.com/Maknee/minigpt4.cpp) | 420 | 🔠💡 | Port of MiniGPT4 in C++ (4bit, 5bit, 6bit, 8bit, 16bit CPU inference with GGML) |
| 2023-07-14 | [vitoplantamura/OnnxStream](https://github.com/vitoplantamura/OnnxStream) | 656 | 🖼️💡 | Running Stable Diffusion on a RPI Zero 2 (or in 260MB of RAM) |
| 2023-07-12 | [yangyuke001/SD-inference](https://github.com/yangyuke001/SD-inference) | 124 | 🔠🖼️💡 | Stable Diffusion inference |
| 2023-07-06 | [kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference) | 593 | 🔠💡📱 | Running Llama 2 and other Open-Source LLMs on CPU Inference Locally for Document Q&A |
| 2023-07-01 | [a1k0n/a1gpt](https://github.com/a1k0n/a1gpt) | 129 | 🔠💡 | throwaway GPT inference |
| 2023-06-15 | [InternLM/lmdeploy](https://github.com/InternLM/lmdeploy) | 391 | 🔠✂️💡 | LMDeploy is a toolkit for compressing, deploying, and serving LLM |
| 2023-06-04 | [axodox/axodox-machinelearning](https://github.com/axodox/axodox-machinelearning) | 506 | 🖼️💡 | This repository contains a C++ ONNX implementation of StableDiffusion. |
| 2023-05-23 | [li-plus/chatglm.cpp](https://github.com/li-plus/chatglm.cpp) | 786 | 🔠💡 | C++ implementation of ChatGLM-6B & ChatGLM2-6B |
| 2023-05-13 | [ztxz16/fastllm](https://github.com/ztxz16/fastllm) | 1495 | 🔠💡 | 纯c++的全平台llm加速库，支持python调用，chatglm-6B级模型单卡可达10000+token / s，支持glm, llama, moss基座，手机端流畅运行 |
| 2023-04-29 | [mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm) | 11148 | 🔠💡 | Enable everyone to develop, optimize and deploy AI models natively on everyone's devices. |
| 2023-04-20 | [0hq/WebGPT](https://github.com/0hq/WebGPT) | 3127 | 🔠💡 | Run GPT model on the browser with WebGPU. An implementation of GPT inference in less than ~1500 lines of vanilla Javascript.  |
| 2023-04-19 | [bentoml/OpenLLM](https://github.com/bentoml/OpenLLM) | 5344 | 🔠💡 | Operating LLMs in production |
| 2023-04-13 | [mlc-ai/web-llm](https://github.com/mlc-ai/web-llm) | 6736 | 🔠💡 | Bringing large-language models and chat to web browsers. Everything runs inside the browser with no server support. |
| 2023-04-02 | [Jittor/JittorLLMs](https://github.com/Jittor/JittorLLMs) | 1769 | 🔠💡 | 计图大模型推理库，具有高性能、配置要求低、中文支持好、可移植等特点 |
| 2023-03-30 | [saharNooby/rwkv.cpp](https://github.com/saharNooby/rwkv.cpp) | 825 | 🔠💡 | INT4/INT5/INT8 and FP16 inference on CPU for RWKV language model |
| 2023-03-23 | [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python) | 2318 | 🔠💡 | Python bindings for llama.cpp |
| 2023-03-21 | [CVI-SZU/Linly](https://github.com/CVI-SZU/Linly) | 2469 | 🔠⛽🚌1️⃣2️⃣`3️⃣`✂️💡💰🀄 | Chinese-LLaMA 1&2、Chinese-Falcon 基础模型；ChatFlow中文对话模型；中文OpenLLaMA模型；NLP预训练/指令微调数据集 |
| 2023-03-19 | [gotzmann/llama.go](https://github.com/gotzmann/llama.go) | 897 | 🔠💡 | llama.go is like llama.cpp in pure Golang! |
| 2023-03-18 | [go-skynet/LocalAI](https://github.com/go-skynet/LocalAI) | 9252 | 🔠🖼️💡 | :robot: Self-hosted, community-driven, local OpenAI-compatible API. Drop-in replacement for OpenAI running LLMs on consumer-grade hardware. Free Open Source OpenAI alternative. No GPU required. LocalAI is an API to run ggml compatible models: llama, gpt4all, rwkv, whisper, vicuna, koala, gpt4all-j, cerebras, falcon, dolly, starcoder, and many other |
| 2023-03-17 | [wangzhaode/ChatGLM-MNN](https://github.com/wangzhaode/ChatGLM-MNN) | 865 | 🔠💡 | Pure C++, Easy Deploy ChatGLM-6B. |
| 2023-03-15 | [kuleshov/minillm](https://github.com/kuleshov/minillm) | 589 | 🔠💡 | MiniLLM is a minimal system for running modern LLMs on consumer-grade GPUs |
| 2023-03-15 | [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | 13451 | 🔠1️⃣2️⃣✂️💡🀄 | 中文LLaMA&Alpaca大语言模型+本地CPU/GPU训练部署 (Chinese LLaMA & Alpaca LLMs) |
| 2023-03-13 | [rustformers/llm](https://github.com/rustformers/llm) | 4411 | 🔠💡 | An ecosystem of Rust libraries for working with large language models |
| 2023-03-12 | [NouamaneTazi/bloomz.cpp](https://github.com/NouamaneTazi/bloomz.cpp) | 782 | 🔠✂️💡 | C++ implementation for BLOOM |
| 2023-03-10 | [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) | 36798 | 🔠✂️💡 | Port of Facebook's LLaMA model in C/C++ |
| 2023-03-08 | [jankais3r/LLaMA_MPS](https://github.com/jankais3r/LLaMA_MPS) | 553 | 🔠💡 | Run LLaMA (and Stanford-Alpaca) inference on Apple Silicon GPUs. |
| 2023-03-06 | [mlc-ai/web-stable-diffusion](https://github.com/mlc-ai/web-stable-diffusion) | 2971 | 🖼️💡 | Bringing stable diffusion models to web browsers. Everything runs inside the browser with no server support.  |
| 2023-02-15 | [FMInference/FlexGen](https://github.com/FMInference/FlexGen) | 8379 | 🔠💡 | Running large language models on a single GPU for throughput-oriented scenarios. |
| 2023-02-11 | [guillaumekln/faster-whisper](https://github.com/guillaumekln/faster-whisper) | 3860 | 🎵💡 | Faster Whisper transcription with CTranslate2 |
| 2023-02-09 | [vllm-project/vllm](https://github.com/vllm-project/vllm) | 4360 | 🔠💡 | A high-throughput and memory-efficient inference and serving engine for LLMs |
| 2022-11-16 | [apple/ml-stable-diffusion](https://github.com/apple/ml-stable-diffusion) | 13823 | 🖼️💡 | Stable Diffusion with Core ML on Apple Silicon |
| 2022-10-08 | [huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference) | 3626 | 🔠✂️💡 | Large Language Model Text Generation Inference |
| 2022-09-18 | [ggerganov/ggml](https://github.com/ggerganov/ggml) | 5950 | 🔠💡 | Tensor library for machine learning |
| 2022-07-15 | [facebookincubator/AITemplate](https://github.com/facebookincubator/AITemplate) | 4105 | 🖼️💡 | AITemplate is a Python framework which renders neural network into high performance CUDA/HIP C++ code. Specialized for FP16 TensorCore (NVIDIA GPU) and MatrixCore (AMD GPU) inference. |
